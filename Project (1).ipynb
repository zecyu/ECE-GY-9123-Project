{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzrWq6OX23wO",
        "outputId": "a00cdd76-8386-4a43-9d1c-bd172336f80a"
      },
      "source": [
        "!pip install efficientnet_pytorch\n",
        "!pip install pydicom"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/a0/dd40b50aebf0028054b6b35062948da01123d7be38d08b6b1e5435df6363/efficientnet_pytorch-0.7.1.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-cp37-none-any.whl size=16443 sha256=fe44071807fe424a8ebc7bcd90c217ffee8ff431b86483e1bc0b02f0d15ea7da\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/27/aa/c46d23c4e8cc72d41283862b1437e0b3ad318417e8ed7d5921\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n",
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/15/df16546bc59bfca390cf072d473fb2c8acd4231636f64356593a63137e55/pydicom-2.1.2-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 11.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "NQ5A2-fS21y6"
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import time\n",
        "\n",
        "import skimage \n",
        "from skimage.transform import resize\n",
        "from skimage.exposure import rescale_intensity\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset as torchDataset\n",
        "import torchvision as tv\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "import shutil\n",
        "\n",
        "import pydicom\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.catch_warnings()\n",
        "\n",
        "gpu_available = True\n",
        "\n",
        "original_image_shape = 1024\n",
        "\n",
        "datapath_orig = '/content/drive/MyDrive/Colab_Notebooks/project/data/'\n",
        "datapath_prep = '/content/drive/MyDrive/Colab_Notebooks/project/data/prep/'\n",
        "datapath_out = '/content/drive/MyDrive/Colab_Notebooks/project/data/save/'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROjqCp1H3NXl",
        "outputId": "c87cb7b1-5a38-40be-da2a-6b7c1d9a520c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d24af5228cf386fcadd225e442bd57afcfc9590b",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "xAErfASI21y8",
        "outputId": "77295fa1-1152-4410-fbe5-8f2dcb459ed2"
      },
      "source": [
        "df_train = pd.read_csv(datapath_prep+'train.csv')\n",
        "# df_train = pd.read_csv(datapath_orig+'stage_2_train_labels')\n",
        "df_test = pd.read_csv(datapath_prep+'test.csv')\n",
        "df_train.head(3)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patientId</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>Target</th>\n",
              "      <th>class</th>\n",
              "      <th>PatientSex</th>\n",
              "      <th>PatientAge</th>\n",
              "      <th>ViewPosition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>F</td>\n",
              "      <td>51.0</td>\n",
              "      <td>PA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>F</td>\n",
              "      <td>48.0</td>\n",
              "      <td>PA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>M</td>\n",
              "      <td>19.0</td>\n",
              "      <td>AP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              patientId   x  ...  PatientAge  ViewPosition\n",
              "0  0004cfab-14fd-4e49-80ba-63a80b6bddd6 NaN  ...        51.0            PA\n",
              "1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd NaN  ...        48.0            PA\n",
              "2  00322d4d-1c29-4943-afc9-b6754be640eb NaN  ...        19.0            AP\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7a925348d5581f1ab749ba16c99e0273a9cce86f",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "f6I-P8jT21y9",
        "outputId": "9989a081-f43c-4fd3-da3a-2c7fb2def1f6"
      },
      "source": [
        "# calculate minimum box area as benchmark for CNN model\n",
        "df_train['box_area'] = df_train['width'] * df_train['height']\n",
        "df_train['box_area'].hist(bins=100)\n",
        "df_train['box_area'].describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count      9555.000000\n",
              "mean      77523.448038\n",
              "std       51807.689206\n",
              "min        2320.000000\n",
              "25%       37535.500000\n",
              "50%       64829.000000\n",
              "75%      106491.500000\n",
              "max      371184.000000\n",
              "Name: box_area, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5692b292478bd2d534a8da94595717465181ef20",
        "trusted": true,
        "id": "gURGrzjj21y-"
      },
      "source": [
        "min_box_area = 10000"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7fe5579f238cd9ddb2f51647128ff253fc5ccf8a",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EarjNGxP21y_",
        "outputId": "bf2c846d-58cf-4942-b445-0a93689bf76c"
      },
      "source": [
        "validation_frac = 0.10\n",
        "\n",
        "df_train = df_train.sample(frac=1, random_state=42)\n",
        "pIds = [pId for pId in df_train['patientId'].unique()]\n",
        "\n",
        "pIds_valid = pIds[ : int(round(validation_frac*len(pIds)))]\n",
        "pIds_train = pIds[int(round(validation_frac*len(pIds))) : ]\n",
        "\n",
        "print('{} patient IDs shuffled and {}% of them used in validation set.'.format(len(pIds), validation_frac*100))\n",
        "print('{} images went into train set and {} images went into validation set.'.format(len(pIds_train), len(pIds_valid)))\n",
        "\n",
        "# get test set patient IDs\n",
        "pIds_test = df_test['patientId'].unique()\n",
        "print('{} patient IDs in test set.'.format(len(pIds_test)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26684 patient IDs shuffled and 10.0% of them used in validation set.\n",
            "24016 images went into train set and 2668 images went into validation set.\n",
            "3000 patient IDs in test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0e44aae1c2ee31ebc8b836de4abeb5097f61ece4",
        "trusted": true,
        "id": "JiM4i6lC21y_"
      },
      "source": [
        "def get_boxes_per_patient(df, pId):\n",
        "    boxes = df.loc[df['patientId']==pId][['x', 'y', 'width', 'height']].astype('int').values.tolist()\n",
        "    return boxes"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ab8077059d5fe1142bc74171fbb2d7ba48dee932",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgMhedUu21zA",
        "outputId": "0442b4fa-b991-4d87-e61b-85344d7acbd9"
      },
      "source": [
        "pId_boxes_dict = {}\n",
        "for pId in df_train.loc[(df_train['Target']==1)]['patientId'].unique().tolist():\n",
        "    pId_boxes_dict[pId] = get_boxes_per_patient(df_train, pId)\n",
        "print('{} ({:.1f}%) images have target boxes.'.format(len(pId_boxes_dict), 100*(len(pId_boxes_dict)/len(pIds))))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6012 (22.5%) images have target boxes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1b49bb60eaa38c96f0d105bfc8d3a6aa5a698bda",
        "trusted": true,
        "id": "wfA67whg21zB"
      },
      "source": [
        "def imgMinMaxScaler(img, scale_range):\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    img = img.astype('float64')\n",
        "    img_std = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
        "    img_scaled = img_std * float(scale_range[1] - scale_range[0]) + float(scale_range[0])\n",
        "    img_scaled = np.rint(img_scaled).astype('uint8')\n",
        "\n",
        "    return img_scaled"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "930a8b2aff636355c97a8cc2c773bb2bfa691763",
        "id": "QuK8RREu21zB"
      },
      "source": [
        "def elastic_transform(image, alpha, sigma, random_state=None):\n",
        "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
        "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
        "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
        "       Proc. of the International Conference on Document Analysis and\n",
        "       Recognition, 2003.\n",
        "       Code adapted from https://gist.github.com/chsasank/4d8f68caf01f041a6453e67fb30f8f5a\n",
        "    \"\"\"\n",
        "    assert len(image.shape)==2, 'Image must have 2 dimensions.'\n",
        "\n",
        "    if random_state is None:\n",
        "        random_state = np.random.RandomState(None)\n",
        "\n",
        "    shape = image.shape\n",
        "\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "\n",
        "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
        "    indices = np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1))\n",
        "    \n",
        "    image_warped = map_coordinates(image, indices, order=1).reshape(shape)\n",
        "    \n",
        "    return image_warped"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "dc0cd3a5487265f5cd28b9abd9c0b329afd5b48b",
        "trusted": true,
        "id": "9iz9nlhk21zC"
      },
      "source": [
        "class PneumoniaDataset(torchDataset):\n",
        "\n",
        "    def __init__(self, root, subset, pIds, predict, boxes, rescale_factor=1, transform=None, rotation_angle=0, warping=False):\n",
        "\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.subset = subset\n",
        "        self.pIds = pIds\n",
        "        self.predict = predict\n",
        "        self.boxes = boxes\n",
        "        self.rescale_factor = rescale_factor\n",
        "        self.transform = transform\n",
        "        self.rotation_angle = rotation_angle\n",
        "        self.warping = warping\n",
        "\n",
        "        self.data_path = self.root + 'stage_2_'+self.subset+'_images/'\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        pId = self.pIds[index]\n",
        "        # img = pydicom.dcmread(os.path.join(self.data_path, pId+'.dcm'),force=True).pixel_array\n",
        "        img = pydicom.read_file(os.path.join(self.data_path, pId+'.dcm'),force=True).pixel_array\n",
        "        # img.file_meta.TransferSyntaxUID = pydicom.uid.ImplicitVRLittleEndian\n",
        "        # img = img.pixel_array\n",
        "        original_image_shape = img.shape[0]\n",
        "        image_shape = original_image_shape / self.rescale_factor\n",
        "        image_shape = int(image_shape)\n",
        "        # resize image \n",
        "        # IMPORTANT: skimage resize function rescales the output from 0 to 1, and pytorch doesn't like this!\n",
        "        # One solution would be using torchvision rescale function (but need to differentiate img and target transforms)\n",
        "        # Here I use skimage resize and then rescale the output again from 0 to 255\n",
        "        img = resize(img, (image_shape, image_shape), mode='reflect')\n",
        "        img = imgMinMaxScaler(img, (0,255))\n",
        "        if self.warping:\n",
        "            img = elastic_transform(img, image_shape*2., image_shape*0.1)\n",
        "        img = np.expand_dims(img, -1)\n",
        "        if self.rotation_angle>0:\n",
        "            angle = self.rotation_angle * (2 * np.random.random_sample() - 1) # generate random angle \n",
        "            img = tv.transforms.functional.to_pil_image(img)\n",
        "            img = tv.transforms.functional.rotate(img, angle, resample=PIL.Image.BILINEAR)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        if not self.predict:\n",
        "            target = np.zeros((image_shape, image_shape))\n",
        "            if pId in self.boxes:\n",
        "                for box in self.boxes[pId]:\n",
        "                    x, y, w, h = box\n",
        "                    x = int(round(x/rescale_factor))\n",
        "                    y = int(round(y/rescale_factor))\n",
        "                    w = int(round(w/rescale_factor))\n",
        "                    h = int(round(h/rescale_factor))\n",
        "                    target[y:y+h, x:x+w] = 255 #\n",
        "            target = np.expand_dims(target, -1)   \n",
        "            target = target.astype('uint8')\n",
        "            if self.rotation_angle>0:\n",
        "                target = tv.transforms.functional.to_pil_image(target)\n",
        "                target = tv.transforms.functional.rotate(target, angle, resample=PIL.Image.BILINEAR)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                target = self.transform(target)\n",
        "            return img, target, pId\n",
        "        else: \n",
        "            return img, pId\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pIds)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c6b0894fa817c5d72c1cef3af6e78658728d548c",
        "trusted": true,
        "id": "egqjkFni21zD"
      },
      "source": [
        "rescale_factor = 4 \n",
        "batch_size = 16 \n",
        "min_box_area = int(round(min_box_area / float(rescale_factor**2)))\n",
        "transform = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
        "dataset_train = PneumoniaDataset(root=datapath_orig, subset='train', pIds=pIds_train, predict=False, \n",
        "                                 boxes=pId_boxes_dict, rescale_factor=rescale_factor, transform=transform,\n",
        "                                 rotation_angle=3, warping=True)\n",
        "\n",
        "dataset_valid = PneumoniaDataset(root=datapath_orig, subset='train', pIds=pIds_valid, predict=False, \n",
        "                                 boxes=pId_boxes_dict, rescale_factor=rescale_factor, transform=transform,\n",
        "                                 rotation_angle=0, warping=False)\n",
        "\n",
        "dataset_test = PneumoniaDataset(root=datapath_orig, subset='test', pIds=pIds_test, predict=True, \n",
        "                                boxes=None, rescale_factor=rescale_factor, transform=transform,\n",
        "                                rotation_angle=0, warping=False)\n",
        "\n",
        "loader_train = DataLoader(dataset=dataset_train,\n",
        "                           batch_size=batch_size,\n",
        "                           shuffle=True) \n",
        "\n",
        "loader_valid = DataLoader(dataset=dataset_valid,\n",
        "                           batch_size=batch_size,\n",
        "                           shuffle=True) \n",
        "\n",
        "loader_test = DataLoader(dataset=dataset_test,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=False) "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFjzbqAGRA_v"
      },
      "source": [
        "# pIds_train[10006]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5-7mBlV_fBM"
      },
      "source": [
        "# ds = pydicom.read_file(os.path.join(datapath_orig+'stage_2_train_images/', pIds_train[10006]+'.dcm'))\n",
        "\n",
        "# print(ds)\n",
        "# # dcmimg = img.pixel_array\n",
        "# # print(type(dcmimg))\n",
        "# # print(dcmimg.dtype)\n",
        "# # print(dcmimg.shape)\n",
        "# # plt.figure(figsize=(20,10))\n",
        "# # plt.imshow(dcmimg)\n",
        "# # plt.axis('off')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MP7t5TMl6Y4"
      },
      "source": [
        "# for i in range(21656,len(dataset_train)):\n",
        "\n",
        "#   dataset_train[i]\n",
        "#   print(i)\n",
        "# # pIds[10006]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "18677b6eff4638b4a0186ec9e65237d48f572ddb",
        "trusted": true,
        "id": "CYOGdZRi21zF"
      },
      "source": [
        "class conv_block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True,\n",
        "                 bn_momentum=0.9, alpha_leaky=0.03):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                              stride=stride, padding=padding, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=bn_momentum)\n",
        "        self.activ = nn.LeakyReLU(negative_slope=alpha_leaky)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activ(self.bn(self.conv(x)))\n",
        "\n",
        "class conv_t_block(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, output_size=None, kernel_size=3, bias=True,\n",
        "                 bn_momentum=0.9, alpha_leaky=0.03):\n",
        "        super(conv_t_block, self).__init__()\n",
        "        self.conv_t = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=2, padding=1, \n",
        "                                         bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=bn_momentum)\n",
        "        self.activ = nn.LeakyReLU(negative_slope=alpha_leaky)\n",
        "\n",
        "    def forward(self, x, output_size):\n",
        "        return self.activ(self.bn(self.conv_t(x, output_size=output_size)))    "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "403551d53e6988d9f94534ce0461dec82ed86d1e",
        "trusted": true,
        "id": "pRU-EW2921zG"
      },
      "source": [
        "class UNET(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(UNET, self).__init__()\n",
        "        \n",
        "        self.down_1 = nn.Sequential(conv_block(in_channels=1, out_channels=64), conv_block(in_channels=64, out_channels=64))\n",
        "        self.down_2 = nn.Sequential(conv_block(in_channels=64, out_channels=128), conv_block(in_channels=128, out_channels=128))\n",
        "        self.down_3 = nn.Sequential(conv_block(in_channels=128, out_channels=256), conv_block(in_channels=256, out_channels=256))\n",
        "        self.down_4 = nn.Sequential(conv_block(in_channels=256, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "        self.down_5 = nn.Sequential(conv_block(in_channels=512, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "\n",
        "        self.middle = nn.Sequential(conv_block(in_channels=512, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "        self.middle_t = conv_t_block(in_channels=512, out_channels=256)\n",
        "\n",
        "        self.up_5 = nn.Sequential(conv_block(in_channels=768, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "        self.up_5_t = conv_t_block(in_channels=512, out_channels=256)\n",
        "        self.up_4 = nn.Sequential(conv_block(in_channels=768, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "        self.up_4_t = conv_t_block(in_channels=512, out_channels=128)\n",
        "        self.up_3 = nn.Sequential(conv_block(in_channels=384, out_channels=256), conv_block(in_channels=256, out_channels=256))\n",
        "        self.up_3_t = conv_t_block(in_channels=256, out_channels=64)\n",
        "        self.up_2 = nn.Sequential(conv_block(in_channels=192, out_channels=128), conv_block(in_channels=128, out_channels=128))\n",
        "        self.up_2_t = conv_t_block(in_channels=128, out_channels=32)\n",
        "        self.up_1 = nn.Sequential(conv_block(in_channels=96, out_channels=64), conv_block(in_channels=64, out_channels=1))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        down1 = self.down_1(x) # (1x256x256 -> 64x256x256)\n",
        "        out = F.max_pool2d(down1, kernel_size=2, stride=2) # (64x256x256 -> 64x128x128)\n",
        "\n",
        "        down2 = self.down_2(out) # (64x128x128 -> 128x128x128)\n",
        "        out = F.max_pool2d(down2, kernel_size=2, stride=2) # (128x128x128 -> 128x64x64)\n",
        "\n",
        "        down3 = self.down_3(out) # (128x64x64 -> 256x64x64)\n",
        "        out = F.max_pool2d(down3, kernel_size=2, stride=2) # (256x64x64 -> 256x32x32)\n",
        "\n",
        "        down4 = self.down_4(out) # (256x32x32 -> 512x32x32)\n",
        "        out = F.max_pool2d(down4, kernel_size=2, stride=2) # (512x32x32 -> 512x16x16)\n",
        "\n",
        "        down5 = self.down_5(out) # (512x16x16 -> 512x16x16)\n",
        "        out = F.max_pool2d(down5, kernel_size=2, stride=2) # (512x16x16 -> 512x8x8)\n",
        "\n",
        "        out = self.middle(out) # (512x8x8 -> 512x8x8)\n",
        "        out = self.middle_t(out, output_size=down5.size()) # (512x8x8 -> 256x16x16)\n",
        "\n",
        "        out = torch.cat([down5, out], 1) # (512x16x16-concat-256x16x16 -> 768x16x16)\n",
        "        out = self.up_5(out) # (768x16x16 -> 512x16x16)\n",
        "        out = self.up_5_t(out, output_size=down4.size()) # (512x16x16 -> 256x32x32)\n",
        "\n",
        "        out = torch.cat([down4, out], 1) # (512x32x32-concat-256x32x32 -> 768x32x32)\n",
        "        out = self.up_4(out) # (768x32x32 -> 512x32x32)\n",
        "        out = self.up_4_t(out, output_size=down3.size()) # (512x32x32 -> 128x64x64)\n",
        "        \n",
        "        out = torch.cat([down3, out], 1) # (256x64x64-concat-128x64x64 -> 384x64x64)\n",
        "        out = self.up_3(out) # (384x64x64 -> 256x64x64)\n",
        "        out = self.up_3_t(out, output_size=down2.size()) # (256x64x64 -> 64x128x128)\n",
        "        \n",
        "        out = torch.cat([down2, out], 1) # (128x128x128-concat-64x128x128 -> 192x128x128)\n",
        "        out = self.up_2(out) # (192x128x128 -> 128x128x128)\n",
        "        out = self.up_2_t(out, output_size=down1.size()) # (128x128x128 -> 32x256x256)\n",
        "        \n",
        "        out = torch.cat([down1, out], 1) # (64x256x256-concat-32x256x256 -> 96x256x256)\n",
        "        out = self.up_1(out) # (96x256x256 -> 1x256x256)\n",
        "        \n",
        "        return out    "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2I9UaeENBh6"
      },
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "# model_name = 'efficientnet-b1'\n",
        "# image_size = EfficientNet.get_image_size(model_name)\n",
        "# print('Image size: ', image_size)\n",
        "\n",
        "# # Load model\n",
        "# model = EfficientNet.from_pretrained(model_name,in_channels=1)\n",
        "# model.eval()\n",
        "# print('Model image size: ', model._global_params.image_size)\n",
        "\n",
        "\n",
        "class model1(nn.Module):\n",
        "    '''\n",
        "    Custom ResNet module\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(model1, self).__init__()\n",
        "        self.base = EfficientNet.from_name('efficientnet-b0',in_channels=1)#8x8\n",
        "        # self.layer1 = nn.Sequential(nn.BatchNorm2d(1280, eps=1e-05, momentum=0.9),nn.Conv2d(in_channels=1280, out_channels=1, kernel_size=1),nn.LeakyReLU(negative_slope=0.03))\n",
        "        # self.layer2 = nn.UpsamplingNearest2d(scale_factor=32)\n",
        "        self.up_layer1 = nn.Sequential(nn.ConvTranspose2d(in_channels=1280, out_channels=320, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(320, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.up_layer2 = nn.Sequential(nn.ConvTranspose2d(in_channels=320, out_channels=112, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(112, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.up_layer3 = nn.Sequential(nn.ConvTranspose2d(in_channels=112, out_channels=80, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(80, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.up_layer4 = nn.Sequential(nn.ConvTranspose2d(in_channels=80, out_channels=40, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(40, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.up_layer5 = nn.Sequential(nn.ConvTranspose2d(in_channels=40, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(16, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.out_layer = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        out = self.base.extract_features(x)\n",
        "        # out = self.layer1(out)\n",
        "        # out = self.layer2(out)\n",
        "        out = self.up_layer1(out)\n",
        "        out = self.up_layer2(out)\n",
        "        out = self.up_layer3(out)\n",
        "        out = self.up_layer4(out)\n",
        "        out = self.up_layer5(out)\n",
        "        out = self.out_layer(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo76oMCEui54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dadd54ad-8232-4634-e2e4-730201578944"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model1().cuda(),input_size=(1,256,256))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "         ZeroPad2d-1          [-1, 1, 257, 257]               0\n",
            "Conv2dStaticSamePadding-2         [-1, 32, 128, 128]             288\n",
            "       BatchNorm2d-3         [-1, 32, 128, 128]              64\n",
            "MemoryEfficientSwish-4         [-1, 32, 128, 128]               0\n",
            "         ZeroPad2d-5         [-1, 32, 130, 130]               0\n",
            "Conv2dStaticSamePadding-6         [-1, 32, 128, 128]             288\n",
            "       BatchNorm2d-7         [-1, 32, 128, 128]              64\n",
            "MemoryEfficientSwish-8         [-1, 32, 128, 128]               0\n",
            "          Identity-9             [-1, 32, 1, 1]               0\n",
            "Conv2dStaticSamePadding-10              [-1, 8, 1, 1]             264\n",
            "MemoryEfficientSwish-11              [-1, 8, 1, 1]               0\n",
            "         Identity-12              [-1, 8, 1, 1]               0\n",
            "Conv2dStaticSamePadding-13             [-1, 32, 1, 1]             288\n",
            "         Identity-14         [-1, 32, 128, 128]               0\n",
            "Conv2dStaticSamePadding-15         [-1, 16, 128, 128]             512\n",
            "      BatchNorm2d-16         [-1, 16, 128, 128]              32\n",
            "      MBConvBlock-17         [-1, 16, 128, 128]               0\n",
            "         Identity-18         [-1, 16, 128, 128]               0\n",
            "Conv2dStaticSamePadding-19         [-1, 96, 128, 128]           1,536\n",
            "      BatchNorm2d-20         [-1, 96, 128, 128]             192\n",
            "MemoryEfficientSwish-21         [-1, 96, 128, 128]               0\n",
            "        ZeroPad2d-22         [-1, 96, 129, 129]               0\n",
            "Conv2dStaticSamePadding-23           [-1, 96, 64, 64]             864\n",
            "      BatchNorm2d-24           [-1, 96, 64, 64]             192\n",
            "MemoryEfficientSwish-25           [-1, 96, 64, 64]               0\n",
            "         Identity-26             [-1, 96, 1, 1]               0\n",
            "Conv2dStaticSamePadding-27              [-1, 4, 1, 1]             388\n",
            "MemoryEfficientSwish-28              [-1, 4, 1, 1]               0\n",
            "         Identity-29              [-1, 4, 1, 1]               0\n",
            "Conv2dStaticSamePadding-30             [-1, 96, 1, 1]             480\n",
            "         Identity-31           [-1, 96, 64, 64]               0\n",
            "Conv2dStaticSamePadding-32           [-1, 24, 64, 64]           2,304\n",
            "      BatchNorm2d-33           [-1, 24, 64, 64]              48\n",
            "      MBConvBlock-34           [-1, 24, 64, 64]               0\n",
            "         Identity-35           [-1, 24, 64, 64]               0\n",
            "Conv2dStaticSamePadding-36          [-1, 144, 64, 64]           3,456\n",
            "      BatchNorm2d-37          [-1, 144, 64, 64]             288\n",
            "MemoryEfficientSwish-38          [-1, 144, 64, 64]               0\n",
            "        ZeroPad2d-39          [-1, 144, 66, 66]               0\n",
            "Conv2dStaticSamePadding-40          [-1, 144, 64, 64]           1,296\n",
            "      BatchNorm2d-41          [-1, 144, 64, 64]             288\n",
            "MemoryEfficientSwish-42          [-1, 144, 64, 64]               0\n",
            "         Identity-43            [-1, 144, 1, 1]               0\n",
            "Conv2dStaticSamePadding-44              [-1, 6, 1, 1]             870\n",
            "MemoryEfficientSwish-45              [-1, 6, 1, 1]               0\n",
            "         Identity-46              [-1, 6, 1, 1]               0\n",
            "Conv2dStaticSamePadding-47            [-1, 144, 1, 1]           1,008\n",
            "         Identity-48          [-1, 144, 64, 64]               0\n",
            "Conv2dStaticSamePadding-49           [-1, 24, 64, 64]           3,456\n",
            "      BatchNorm2d-50           [-1, 24, 64, 64]              48\n",
            "      MBConvBlock-51           [-1, 24, 64, 64]               0\n",
            "         Identity-52           [-1, 24, 64, 64]               0\n",
            "Conv2dStaticSamePadding-53          [-1, 144, 64, 64]           3,456\n",
            "      BatchNorm2d-54          [-1, 144, 64, 64]             288\n",
            "MemoryEfficientSwish-55          [-1, 144, 64, 64]               0\n",
            "        ZeroPad2d-56          [-1, 144, 67, 67]               0\n",
            "Conv2dStaticSamePadding-57          [-1, 144, 32, 32]           3,600\n",
            "      BatchNorm2d-58          [-1, 144, 32, 32]             288\n",
            "MemoryEfficientSwish-59          [-1, 144, 32, 32]               0\n",
            "         Identity-60            [-1, 144, 1, 1]               0\n",
            "Conv2dStaticSamePadding-61              [-1, 6, 1, 1]             870\n",
            "MemoryEfficientSwish-62              [-1, 6, 1, 1]               0\n",
            "         Identity-63              [-1, 6, 1, 1]               0\n",
            "Conv2dStaticSamePadding-64            [-1, 144, 1, 1]           1,008\n",
            "         Identity-65          [-1, 144, 32, 32]               0\n",
            "Conv2dStaticSamePadding-66           [-1, 40, 32, 32]           5,760\n",
            "      BatchNorm2d-67           [-1, 40, 32, 32]              80\n",
            "      MBConvBlock-68           [-1, 40, 32, 32]               0\n",
            "         Identity-69           [-1, 40, 32, 32]               0\n",
            "Conv2dStaticSamePadding-70          [-1, 240, 32, 32]           9,600\n",
            "      BatchNorm2d-71          [-1, 240, 32, 32]             480\n",
            "MemoryEfficientSwish-72          [-1, 240, 32, 32]               0\n",
            "        ZeroPad2d-73          [-1, 240, 36, 36]               0\n",
            "Conv2dStaticSamePadding-74          [-1, 240, 32, 32]           6,000\n",
            "      BatchNorm2d-75          [-1, 240, 32, 32]             480\n",
            "MemoryEfficientSwish-76          [-1, 240, 32, 32]               0\n",
            "         Identity-77            [-1, 240, 1, 1]               0\n",
            "Conv2dStaticSamePadding-78             [-1, 10, 1, 1]           2,410\n",
            "MemoryEfficientSwish-79             [-1, 10, 1, 1]               0\n",
            "         Identity-80             [-1, 10, 1, 1]               0\n",
            "Conv2dStaticSamePadding-81            [-1, 240, 1, 1]           2,640\n",
            "         Identity-82          [-1, 240, 32, 32]               0\n",
            "Conv2dStaticSamePadding-83           [-1, 40, 32, 32]           9,600\n",
            "      BatchNorm2d-84           [-1, 40, 32, 32]              80\n",
            "      MBConvBlock-85           [-1, 40, 32, 32]               0\n",
            "         Identity-86           [-1, 40, 32, 32]               0\n",
            "Conv2dStaticSamePadding-87          [-1, 240, 32, 32]           9,600\n",
            "      BatchNorm2d-88          [-1, 240, 32, 32]             480\n",
            "MemoryEfficientSwish-89          [-1, 240, 32, 32]               0\n",
            "        ZeroPad2d-90          [-1, 240, 33, 33]               0\n",
            "Conv2dStaticSamePadding-91          [-1, 240, 16, 16]           2,160\n",
            "      BatchNorm2d-92          [-1, 240, 16, 16]             480\n",
            "MemoryEfficientSwish-93          [-1, 240, 16, 16]               0\n",
            "         Identity-94            [-1, 240, 1, 1]               0\n",
            "Conv2dStaticSamePadding-95             [-1, 10, 1, 1]           2,410\n",
            "MemoryEfficientSwish-96             [-1, 10, 1, 1]               0\n",
            "         Identity-97             [-1, 10, 1, 1]               0\n",
            "Conv2dStaticSamePadding-98            [-1, 240, 1, 1]           2,640\n",
            "         Identity-99          [-1, 240, 16, 16]               0\n",
            "Conv2dStaticSamePadding-100           [-1, 80, 16, 16]          19,200\n",
            "     BatchNorm2d-101           [-1, 80, 16, 16]             160\n",
            "     MBConvBlock-102           [-1, 80, 16, 16]               0\n",
            "        Identity-103           [-1, 80, 16, 16]               0\n",
            "Conv2dStaticSamePadding-104          [-1, 480, 16, 16]          38,400\n",
            "     BatchNorm2d-105          [-1, 480, 16, 16]             960\n",
            "MemoryEfficientSwish-106          [-1, 480, 16, 16]               0\n",
            "       ZeroPad2d-107          [-1, 480, 18, 18]               0\n",
            "Conv2dStaticSamePadding-108          [-1, 480, 16, 16]           4,320\n",
            "     BatchNorm2d-109          [-1, 480, 16, 16]             960\n",
            "MemoryEfficientSwish-110          [-1, 480, 16, 16]               0\n",
            "        Identity-111            [-1, 480, 1, 1]               0\n",
            "Conv2dStaticSamePadding-112             [-1, 20, 1, 1]           9,620\n",
            "MemoryEfficientSwish-113             [-1, 20, 1, 1]               0\n",
            "        Identity-114             [-1, 20, 1, 1]               0\n",
            "Conv2dStaticSamePadding-115            [-1, 480, 1, 1]          10,080\n",
            "        Identity-116          [-1, 480, 16, 16]               0\n",
            "Conv2dStaticSamePadding-117           [-1, 80, 16, 16]          38,400\n",
            "     BatchNorm2d-118           [-1, 80, 16, 16]             160\n",
            "     MBConvBlock-119           [-1, 80, 16, 16]               0\n",
            "        Identity-120           [-1, 80, 16, 16]               0\n",
            "Conv2dStaticSamePadding-121          [-1, 480, 16, 16]          38,400\n",
            "     BatchNorm2d-122          [-1, 480, 16, 16]             960\n",
            "MemoryEfficientSwish-123          [-1, 480, 16, 16]               0\n",
            "       ZeroPad2d-124          [-1, 480, 18, 18]               0\n",
            "Conv2dStaticSamePadding-125          [-1, 480, 16, 16]           4,320\n",
            "     BatchNorm2d-126          [-1, 480, 16, 16]             960\n",
            "MemoryEfficientSwish-127          [-1, 480, 16, 16]               0\n",
            "        Identity-128            [-1, 480, 1, 1]               0\n",
            "Conv2dStaticSamePadding-129             [-1, 20, 1, 1]           9,620\n",
            "MemoryEfficientSwish-130             [-1, 20, 1, 1]               0\n",
            "        Identity-131             [-1, 20, 1, 1]               0\n",
            "Conv2dStaticSamePadding-132            [-1, 480, 1, 1]          10,080\n",
            "        Identity-133          [-1, 480, 16, 16]               0\n",
            "Conv2dStaticSamePadding-134           [-1, 80, 16, 16]          38,400\n",
            "     BatchNorm2d-135           [-1, 80, 16, 16]             160\n",
            "     MBConvBlock-136           [-1, 80, 16, 16]               0\n",
            "        Identity-137           [-1, 80, 16, 16]               0\n",
            "Conv2dStaticSamePadding-138          [-1, 480, 16, 16]          38,400\n",
            "     BatchNorm2d-139          [-1, 480, 16, 16]             960\n",
            "MemoryEfficientSwish-140          [-1, 480, 16, 16]               0\n",
            "       ZeroPad2d-141          [-1, 480, 20, 20]               0\n",
            "Conv2dStaticSamePadding-142          [-1, 480, 16, 16]          12,000\n",
            "     BatchNorm2d-143          [-1, 480, 16, 16]             960\n",
            "MemoryEfficientSwish-144          [-1, 480, 16, 16]               0\n",
            "        Identity-145            [-1, 480, 1, 1]               0\n",
            "Conv2dStaticSamePadding-146             [-1, 20, 1, 1]           9,620\n",
            "MemoryEfficientSwish-147             [-1, 20, 1, 1]               0\n",
            "        Identity-148             [-1, 20, 1, 1]               0\n",
            "Conv2dStaticSamePadding-149            [-1, 480, 1, 1]          10,080\n",
            "        Identity-150          [-1, 480, 16, 16]               0\n",
            "Conv2dStaticSamePadding-151          [-1, 112, 16, 16]          53,760\n",
            "     BatchNorm2d-152          [-1, 112, 16, 16]             224\n",
            "     MBConvBlock-153          [-1, 112, 16, 16]               0\n",
            "        Identity-154          [-1, 112, 16, 16]               0\n",
            "Conv2dStaticSamePadding-155          [-1, 672, 16, 16]          75,264\n",
            "     BatchNorm2d-156          [-1, 672, 16, 16]           1,344\n",
            "MemoryEfficientSwish-157          [-1, 672, 16, 16]               0\n",
            "       ZeroPad2d-158          [-1, 672, 20, 20]               0\n",
            "Conv2dStaticSamePadding-159          [-1, 672, 16, 16]          16,800\n",
            "     BatchNorm2d-160          [-1, 672, 16, 16]           1,344\n",
            "MemoryEfficientSwish-161          [-1, 672, 16, 16]               0\n",
            "        Identity-162            [-1, 672, 1, 1]               0\n",
            "Conv2dStaticSamePadding-163             [-1, 28, 1, 1]          18,844\n",
            "MemoryEfficientSwish-164             [-1, 28, 1, 1]               0\n",
            "        Identity-165             [-1, 28, 1, 1]               0\n",
            "Conv2dStaticSamePadding-166            [-1, 672, 1, 1]          19,488\n",
            "        Identity-167          [-1, 672, 16, 16]               0\n",
            "Conv2dStaticSamePadding-168          [-1, 112, 16, 16]          75,264\n",
            "     BatchNorm2d-169          [-1, 112, 16, 16]             224\n",
            "     MBConvBlock-170          [-1, 112, 16, 16]               0\n",
            "        Identity-171          [-1, 112, 16, 16]               0\n",
            "Conv2dStaticSamePadding-172          [-1, 672, 16, 16]          75,264\n",
            "     BatchNorm2d-173          [-1, 672, 16, 16]           1,344\n",
            "MemoryEfficientSwish-174          [-1, 672, 16, 16]               0\n",
            "       ZeroPad2d-175          [-1, 672, 20, 20]               0\n",
            "Conv2dStaticSamePadding-176          [-1, 672, 16, 16]          16,800\n",
            "     BatchNorm2d-177          [-1, 672, 16, 16]           1,344\n",
            "MemoryEfficientSwish-178          [-1, 672, 16, 16]               0\n",
            "        Identity-179            [-1, 672, 1, 1]               0\n",
            "Conv2dStaticSamePadding-180             [-1, 28, 1, 1]          18,844\n",
            "MemoryEfficientSwish-181             [-1, 28, 1, 1]               0\n",
            "        Identity-182             [-1, 28, 1, 1]               0\n",
            "Conv2dStaticSamePadding-183            [-1, 672, 1, 1]          19,488\n",
            "        Identity-184          [-1, 672, 16, 16]               0\n",
            "Conv2dStaticSamePadding-185          [-1, 112, 16, 16]          75,264\n",
            "     BatchNorm2d-186          [-1, 112, 16, 16]             224\n",
            "     MBConvBlock-187          [-1, 112, 16, 16]               0\n",
            "        Identity-188          [-1, 112, 16, 16]               0\n",
            "Conv2dStaticSamePadding-189          [-1, 672, 16, 16]          75,264\n",
            "     BatchNorm2d-190          [-1, 672, 16, 16]           1,344\n",
            "MemoryEfficientSwish-191          [-1, 672, 16, 16]               0\n",
            "       ZeroPad2d-192          [-1, 672, 19, 19]               0\n",
            "Conv2dStaticSamePadding-193            [-1, 672, 8, 8]          16,800\n",
            "     BatchNorm2d-194            [-1, 672, 8, 8]           1,344\n",
            "MemoryEfficientSwish-195            [-1, 672, 8, 8]               0\n",
            "        Identity-196            [-1, 672, 1, 1]               0\n",
            "Conv2dStaticSamePadding-197             [-1, 28, 1, 1]          18,844\n",
            "MemoryEfficientSwish-198             [-1, 28, 1, 1]               0\n",
            "        Identity-199             [-1, 28, 1, 1]               0\n",
            "Conv2dStaticSamePadding-200            [-1, 672, 1, 1]          19,488\n",
            "        Identity-201            [-1, 672, 8, 8]               0\n",
            "Conv2dStaticSamePadding-202            [-1, 192, 8, 8]         129,024\n",
            "     BatchNorm2d-203            [-1, 192, 8, 8]             384\n",
            "     MBConvBlock-204            [-1, 192, 8, 8]               0\n",
            "        Identity-205            [-1, 192, 8, 8]               0\n",
            "Conv2dStaticSamePadding-206           [-1, 1152, 8, 8]         221,184\n",
            "     BatchNorm2d-207           [-1, 1152, 8, 8]           2,304\n",
            "MemoryEfficientSwish-208           [-1, 1152, 8, 8]               0\n",
            "       ZeroPad2d-209         [-1, 1152, 12, 12]               0\n",
            "Conv2dStaticSamePadding-210           [-1, 1152, 8, 8]          28,800\n",
            "     BatchNorm2d-211           [-1, 1152, 8, 8]           2,304\n",
            "MemoryEfficientSwish-212           [-1, 1152, 8, 8]               0\n",
            "        Identity-213           [-1, 1152, 1, 1]               0\n",
            "Conv2dStaticSamePadding-214             [-1, 48, 1, 1]          55,344\n",
            "MemoryEfficientSwish-215             [-1, 48, 1, 1]               0\n",
            "        Identity-216             [-1, 48, 1, 1]               0\n",
            "Conv2dStaticSamePadding-217           [-1, 1152, 1, 1]          56,448\n",
            "        Identity-218           [-1, 1152, 8, 8]               0\n",
            "Conv2dStaticSamePadding-219            [-1, 192, 8, 8]         221,184\n",
            "     BatchNorm2d-220            [-1, 192, 8, 8]             384\n",
            "     MBConvBlock-221            [-1, 192, 8, 8]               0\n",
            "        Identity-222            [-1, 192, 8, 8]               0\n",
            "Conv2dStaticSamePadding-223           [-1, 1152, 8, 8]         221,184\n",
            "     BatchNorm2d-224           [-1, 1152, 8, 8]           2,304\n",
            "MemoryEfficientSwish-225           [-1, 1152, 8, 8]               0\n",
            "       ZeroPad2d-226         [-1, 1152, 12, 12]               0\n",
            "Conv2dStaticSamePadding-227           [-1, 1152, 8, 8]          28,800\n",
            "     BatchNorm2d-228           [-1, 1152, 8, 8]           2,304\n",
            "MemoryEfficientSwish-229           [-1, 1152, 8, 8]               0\n",
            "        Identity-230           [-1, 1152, 1, 1]               0\n",
            "Conv2dStaticSamePadding-231             [-1, 48, 1, 1]          55,344\n",
            "MemoryEfficientSwish-232             [-1, 48, 1, 1]               0\n",
            "        Identity-233             [-1, 48, 1, 1]               0\n",
            "Conv2dStaticSamePadding-234           [-1, 1152, 1, 1]          56,448\n",
            "        Identity-235           [-1, 1152, 8, 8]               0\n",
            "Conv2dStaticSamePadding-236            [-1, 192, 8, 8]         221,184\n",
            "     BatchNorm2d-237            [-1, 192, 8, 8]             384\n",
            "     MBConvBlock-238            [-1, 192, 8, 8]               0\n",
            "        Identity-239            [-1, 192, 8, 8]               0\n",
            "Conv2dStaticSamePadding-240           [-1, 1152, 8, 8]         221,184\n",
            "     BatchNorm2d-241           [-1, 1152, 8, 8]           2,304\n",
            "MemoryEfficientSwish-242           [-1, 1152, 8, 8]               0\n",
            "       ZeroPad2d-243         [-1, 1152, 12, 12]               0\n",
            "Conv2dStaticSamePadding-244           [-1, 1152, 8, 8]          28,800\n",
            "     BatchNorm2d-245           [-1, 1152, 8, 8]           2,304\n",
            "MemoryEfficientSwish-246           [-1, 1152, 8, 8]               0\n",
            "        Identity-247           [-1, 1152, 1, 1]               0\n",
            "Conv2dStaticSamePadding-248             [-1, 48, 1, 1]          55,344\n",
            "MemoryEfficientSwish-249             [-1, 48, 1, 1]               0\n",
            "        Identity-250             [-1, 48, 1, 1]               0\n",
            "Conv2dStaticSamePadding-251           [-1, 1152, 1, 1]          56,448\n",
            "        Identity-252           [-1, 1152, 8, 8]               0\n",
            "Conv2dStaticSamePadding-253            [-1, 192, 8, 8]         221,184\n",
            "     BatchNorm2d-254            [-1, 192, 8, 8]             384\n",
            "     MBConvBlock-255            [-1, 192, 8, 8]               0\n",
            "        Identity-256            [-1, 192, 8, 8]               0\n",
            "Conv2dStaticSamePadding-257           [-1, 1152, 8, 8]         221,184\n",
            "     BatchNorm2d-258           [-1, 1152, 8, 8]           2,304\n",
            "MemoryEfficientSwish-259           [-1, 1152, 8, 8]               0\n",
            "       ZeroPad2d-260         [-1, 1152, 10, 10]               0\n",
            "Conv2dStaticSamePadding-261           [-1, 1152, 8, 8]          10,368\n",
            "     BatchNorm2d-262           [-1, 1152, 8, 8]           2,304\n",
            "MemoryEfficientSwish-263           [-1, 1152, 8, 8]               0\n",
            "        Identity-264           [-1, 1152, 1, 1]               0\n",
            "Conv2dStaticSamePadding-265             [-1, 48, 1, 1]          55,344\n",
            "MemoryEfficientSwish-266             [-1, 48, 1, 1]               0\n",
            "        Identity-267             [-1, 48, 1, 1]               0\n",
            "Conv2dStaticSamePadding-268           [-1, 1152, 1, 1]          56,448\n",
            "        Identity-269           [-1, 1152, 8, 8]               0\n",
            "Conv2dStaticSamePadding-270            [-1, 320, 8, 8]         368,640\n",
            "     BatchNorm2d-271            [-1, 320, 8, 8]             640\n",
            "     MBConvBlock-272            [-1, 320, 8, 8]               0\n",
            "        Identity-273            [-1, 320, 8, 8]               0\n",
            "Conv2dStaticSamePadding-274           [-1, 1280, 8, 8]         409,600\n",
            "     BatchNorm2d-275           [-1, 1280, 8, 8]           2,560\n",
            "MemoryEfficientSwish-276           [-1, 1280, 8, 8]               0\n",
            " ConvTranspose2d-277          [-1, 320, 16, 16]       3,686,720\n",
            "     BatchNorm2d-278          [-1, 320, 16, 16]             640\n",
            "       LeakyReLU-279          [-1, 320, 16, 16]               0\n",
            " ConvTranspose2d-280          [-1, 112, 32, 32]         322,672\n",
            "     BatchNorm2d-281          [-1, 112, 32, 32]             224\n",
            "       LeakyReLU-282          [-1, 112, 32, 32]               0\n",
            " ConvTranspose2d-283           [-1, 80, 64, 64]          80,720\n",
            "     BatchNorm2d-284           [-1, 80, 64, 64]             160\n",
            "       LeakyReLU-285           [-1, 80, 64, 64]               0\n",
            " ConvTranspose2d-286         [-1, 40, 128, 128]          28,840\n",
            "     BatchNorm2d-287         [-1, 40, 128, 128]              80\n",
            "       LeakyReLU-288         [-1, 40, 128, 128]               0\n",
            " ConvTranspose2d-289         [-1, 16, 256, 256]           5,776\n",
            "     BatchNorm2d-290         [-1, 16, 256, 256]              32\n",
            "       LeakyReLU-291         [-1, 16, 256, 256]               0\n",
            "          Conv2d-292          [-1, 1, 256, 256]              17\n",
            "================================================================\n",
            "Total params: 8,132,853\n",
            "Trainable params: 8,132,853\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.25\n",
            "Forward/backward pass size (MB): 327.20\n",
            "Params size (MB): 31.02\n",
            "Estimated Total Size (MB): 358.48\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MlJBTdRsQGd",
        "outputId": "2a567d22-4a3c-4047-aba6-19bd9ea66c3e"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(UNET().cuda(),input_size=(1,256,256))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 256, 256]             640\n",
            "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
            "         LeakyReLU-3         [-1, 64, 256, 256]               0\n",
            "        conv_block-4         [-1, 64, 256, 256]               0\n",
            "            Conv2d-5         [-1, 64, 256, 256]          36,928\n",
            "       BatchNorm2d-6         [-1, 64, 256, 256]             128\n",
            "         LeakyReLU-7         [-1, 64, 256, 256]               0\n",
            "        conv_block-8         [-1, 64, 256, 256]               0\n",
            "            Conv2d-9        [-1, 128, 128, 128]          73,856\n",
            "      BatchNorm2d-10        [-1, 128, 128, 128]             256\n",
            "        LeakyReLU-11        [-1, 128, 128, 128]               0\n",
            "       conv_block-12        [-1, 128, 128, 128]               0\n",
            "           Conv2d-13        [-1, 128, 128, 128]         147,584\n",
            "      BatchNorm2d-14        [-1, 128, 128, 128]             256\n",
            "        LeakyReLU-15        [-1, 128, 128, 128]               0\n",
            "       conv_block-16        [-1, 128, 128, 128]               0\n",
            "           Conv2d-17          [-1, 256, 64, 64]         295,168\n",
            "      BatchNorm2d-18          [-1, 256, 64, 64]             512\n",
            "        LeakyReLU-19          [-1, 256, 64, 64]               0\n",
            "       conv_block-20          [-1, 256, 64, 64]               0\n",
            "           Conv2d-21          [-1, 256, 64, 64]         590,080\n",
            "      BatchNorm2d-22          [-1, 256, 64, 64]             512\n",
            "        LeakyReLU-23          [-1, 256, 64, 64]               0\n",
            "       conv_block-24          [-1, 256, 64, 64]               0\n",
            "           Conv2d-25          [-1, 512, 32, 32]       1,180,160\n",
            "      BatchNorm2d-26          [-1, 512, 32, 32]           1,024\n",
            "        LeakyReLU-27          [-1, 512, 32, 32]               0\n",
            "       conv_block-28          [-1, 512, 32, 32]               0\n",
            "           Conv2d-29          [-1, 512, 32, 32]       2,359,808\n",
            "      BatchNorm2d-30          [-1, 512, 32, 32]           1,024\n",
            "        LeakyReLU-31          [-1, 512, 32, 32]               0\n",
            "       conv_block-32          [-1, 512, 32, 32]               0\n",
            "           Conv2d-33          [-1, 512, 16, 16]       2,359,808\n",
            "      BatchNorm2d-34          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-35          [-1, 512, 16, 16]               0\n",
            "       conv_block-36          [-1, 512, 16, 16]               0\n",
            "           Conv2d-37          [-1, 512, 16, 16]       2,359,808\n",
            "      BatchNorm2d-38          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-39          [-1, 512, 16, 16]               0\n",
            "       conv_block-40          [-1, 512, 16, 16]               0\n",
            "           Conv2d-41            [-1, 512, 8, 8]       2,359,808\n",
            "      BatchNorm2d-42            [-1, 512, 8, 8]           1,024\n",
            "        LeakyReLU-43            [-1, 512, 8, 8]               0\n",
            "       conv_block-44            [-1, 512, 8, 8]               0\n",
            "           Conv2d-45            [-1, 512, 8, 8]       2,359,808\n",
            "      BatchNorm2d-46            [-1, 512, 8, 8]           1,024\n",
            "        LeakyReLU-47            [-1, 512, 8, 8]               0\n",
            "       conv_block-48            [-1, 512, 8, 8]               0\n",
            "  ConvTranspose2d-49          [-1, 256, 16, 16]       1,179,904\n",
            "      BatchNorm2d-50          [-1, 256, 16, 16]             512\n",
            "        LeakyReLU-51          [-1, 256, 16, 16]               0\n",
            "     conv_t_block-52          [-1, 256, 16, 16]               0\n",
            "           Conv2d-53          [-1, 512, 16, 16]       3,539,456\n",
            "      BatchNorm2d-54          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-55          [-1, 512, 16, 16]               0\n",
            "       conv_block-56          [-1, 512, 16, 16]               0\n",
            "           Conv2d-57          [-1, 512, 16, 16]       2,359,808\n",
            "      BatchNorm2d-58          [-1, 512, 16, 16]           1,024\n",
            "        LeakyReLU-59          [-1, 512, 16, 16]               0\n",
            "       conv_block-60          [-1, 512, 16, 16]               0\n",
            "  ConvTranspose2d-61          [-1, 256, 32, 32]       1,179,904\n",
            "      BatchNorm2d-62          [-1, 256, 32, 32]             512\n",
            "        LeakyReLU-63          [-1, 256, 32, 32]               0\n",
            "     conv_t_block-64          [-1, 256, 32, 32]               0\n",
            "           Conv2d-65          [-1, 512, 32, 32]       3,539,456\n",
            "      BatchNorm2d-66          [-1, 512, 32, 32]           1,024\n",
            "        LeakyReLU-67          [-1, 512, 32, 32]               0\n",
            "       conv_block-68          [-1, 512, 32, 32]               0\n",
            "           Conv2d-69          [-1, 512, 32, 32]       2,359,808\n",
            "      BatchNorm2d-70          [-1, 512, 32, 32]           1,024\n",
            "        LeakyReLU-71          [-1, 512, 32, 32]               0\n",
            "       conv_block-72          [-1, 512, 32, 32]               0\n",
            "  ConvTranspose2d-73          [-1, 128, 64, 64]         589,952\n",
            "      BatchNorm2d-74          [-1, 128, 64, 64]             256\n",
            "        LeakyReLU-75          [-1, 128, 64, 64]               0\n",
            "     conv_t_block-76          [-1, 128, 64, 64]               0\n",
            "           Conv2d-77          [-1, 256, 64, 64]         884,992\n",
            "      BatchNorm2d-78          [-1, 256, 64, 64]             512\n",
            "        LeakyReLU-79          [-1, 256, 64, 64]               0\n",
            "       conv_block-80          [-1, 256, 64, 64]               0\n",
            "           Conv2d-81          [-1, 256, 64, 64]         590,080\n",
            "      BatchNorm2d-82          [-1, 256, 64, 64]             512\n",
            "        LeakyReLU-83          [-1, 256, 64, 64]               0\n",
            "       conv_block-84          [-1, 256, 64, 64]               0\n",
            "  ConvTranspose2d-85         [-1, 64, 128, 128]         147,520\n",
            "      BatchNorm2d-86         [-1, 64, 128, 128]             128\n",
            "        LeakyReLU-87         [-1, 64, 128, 128]               0\n",
            "     conv_t_block-88         [-1, 64, 128, 128]               0\n",
            "           Conv2d-89        [-1, 128, 128, 128]         221,312\n",
            "      BatchNorm2d-90        [-1, 128, 128, 128]             256\n",
            "        LeakyReLU-91        [-1, 128, 128, 128]               0\n",
            "       conv_block-92        [-1, 128, 128, 128]               0\n",
            "           Conv2d-93        [-1, 128, 128, 128]         147,584\n",
            "      BatchNorm2d-94        [-1, 128, 128, 128]             256\n",
            "        LeakyReLU-95        [-1, 128, 128, 128]               0\n",
            "       conv_block-96        [-1, 128, 128, 128]               0\n",
            "  ConvTranspose2d-97         [-1, 32, 256, 256]          36,896\n",
            "      BatchNorm2d-98         [-1, 32, 256, 256]              64\n",
            "        LeakyReLU-99         [-1, 32, 256, 256]               0\n",
            "    conv_t_block-100         [-1, 32, 256, 256]               0\n",
            "          Conv2d-101         [-1, 64, 256, 256]          55,360\n",
            "     BatchNorm2d-102         [-1, 64, 256, 256]             128\n",
            "       LeakyReLU-103         [-1, 64, 256, 256]               0\n",
            "      conv_block-104         [-1, 64, 256, 256]               0\n",
            "          Conv2d-105          [-1, 1, 256, 256]             577\n",
            "     BatchNorm2d-106          [-1, 1, 256, 256]               2\n",
            "       LeakyReLU-107          [-1, 1, 256, 256]               0\n",
            "      conv_block-108          [-1, 1, 256, 256]               0\n",
            "================================================================\n",
            "Total params: 30,971,235\n",
            "Trainable params: 30,971,235\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.25\n",
            "Forward/backward pass size (MB): 974.00\n",
            "Params size (MB): 118.15\n",
            "Estimated Total Size (MB): 1092.40\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC9JBgA5k_hO"
      },
      "source": [
        "# class model2(nn.Module):\n",
        "#     '''\n",
        "#     Custom ResNet module\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(model1, self).__init__()\n",
        "#         self.base = EfficientNet.from_name('efficientnet-b0',in_channels=1)\n",
        "#         self.layer1 = nn.Sequential(nn.BatchNorm2d(1280, eps=1e-05, momentum=0.9),nn.Conv2d(in_channels=1280, out_channels=1, kernel_size=1),nn.LeakyReLU(negative_slope=0.03))\n",
        "#         self.layer2 = nn.UpsamplingNearest2d(scale_factor=32)\n",
        "#     def forward(self, x):\n",
        "#         out = self.base.extract_features(x)\n",
        "#         out = self.layer1(out)\n",
        "#         out = self.layer2(out)\n",
        "        \n",
        "#         return out"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c58d080e0979c01dd6edb4b7387e59dc04e759c4",
        "trusted": true,
        "id": "P6QjdPdK21zH"
      },
      "source": [
        "class BCEWithLogitsLoss2d(nn.Module):\n",
        "\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(BCEWithLogitsLoss2d, self).__init__()\n",
        "        self.loss = nn.BCEWithLogitsLoss(weight, size_average)\n",
        "\n",
        "    def forward(self, scores, targets):\n",
        "        scores_flat = scores.view(-1)\n",
        "        targets_flat = targets.view(-1)\n",
        "        return self.loss(scores_flat, targets_flat)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "73632678b3be947135bf03fa118643ebf8f7bf65",
        "trusted": true,
        "id": "87UCAQlV21zH"
      },
      "source": [
        "def box_mask(box, shape=1024):\n",
        "\n",
        "    x, y, w, h = box\n",
        "    mask = np.zeros((shape, shape), dtype=bool)\n",
        "    mask[y:y+h, x:x+w] = True \n",
        "    return mask\n",
        "\n",
        "def mask_boxes(msk, threshold=0.20, connectivity=None):\n",
        "\n",
        "    msk = msk[0]\n",
        "    pos = np.zeros(msk.shape)\n",
        "    pos[msk>threshold] = 1.\n",
        "    lbl = skimage.measure.label(pos, connectivity=connectivity)\n",
        "    predicted_boxes = []\n",
        "    confidences = []\n",
        "    for region in skimage.measure.regionprops(lbl):\n",
        "        y1, x1, y2, x2 = region.bbox\n",
        "        h = y2 - y1\n",
        "        w = x2 - x1\n",
        "        c = np.nanmean(msk[y1:y2, x1:x2])\n",
        "        if w*h > min_box_area: \n",
        "            predicted_boxes.append([x1, y1, w, h])\n",
        "            confidences.append(c)\n",
        "    \n",
        "    return predicted_boxes, confidences\n",
        "\n",
        "def prediction_string(predicted_boxes, confidences):\n",
        "\n",
        "    prediction_string = ''\n",
        "    for c, box in zip(confidences, predicted_boxes):\n",
        "        prediction_string += ' ' + str(c) + ' ' + ' '.join([str(b) for b in box])\n",
        "    return prediction_string[1:]   \n",
        "\n",
        "def IoU(pr, gt):\n",
        "    IoU = (pr & gt).sum() / ((pr | gt).sum() + 1.e-9)\n",
        "    return IoU\n",
        "\n",
        "def precision(tp, fp, fn):\n",
        "    return float(tp) / (tp + fp + fn + 1.e-9)\n",
        "def average_precision_image(predicted_boxes, confidences, target_boxes, shape=1024):\n",
        "\n",
        "    if predicted_boxes == [] and target_boxes == []:\n",
        "        return np.nan\n",
        "    else:\n",
        "        if len(predicted_boxes)>0 and target_boxes == []:\n",
        "            return 0.0\n",
        "        elif len(target_boxes)>0 and predicted_boxes == []:\n",
        "            return 0.0\n",
        "        else:\n",
        "            # define list of thresholds for IoU [0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75]\n",
        "            thresholds = np.arange(0.4, 0.8, 0.05) \n",
        "            # sort boxes according to their confidence (from largest to smallest)\n",
        "            predicted_boxes_sorted = list(reversed([b for _, b in sorted(zip(confidences, predicted_boxes),key=lambda pair: pair[0])]))            \n",
        "            average_precision = 0.0\n",
        "            for t in thresholds: \n",
        "                tp = 0 \n",
        "                fp = len(predicted_boxes)  \n",
        "                for box_p in predicted_boxes_sorted: \n",
        "                    box_p_msk = box_mask(box_p, shape) \n",
        "                    for box_t in target_boxes: \n",
        "                        box_t_msk = box_mask(box_t, shape) \n",
        "                        iou = IoU(box_p_msk, box_t_msk) \n",
        "                        if iou>t:\n",
        "                            tp += 1 \n",
        "                            fp -= 1 \n",
        "                            break \n",
        "                fn = len(target_boxes) \n",
        "                for box_t in target_boxes: \n",
        "                    box_t_msk = box_mask(box_t, shape) \n",
        "                    for box_p in predicted_boxes_sorted: \n",
        "                        box_p_msk = box_mask(box_p, shape) \n",
        "                        iou = IoU(box_p_msk, box_t_msk) \n",
        "                        if iou>t:\n",
        "                            fn -= 1\n",
        "                            break \n",
        "               \n",
        "                average_precision += precision(tp, fp, fn) / float(len(thresholds))\n",
        "            return average_precision\n",
        "\n",
        "def average_precision_batch(output_batch, pIds, pId_boxes_dict, rescale_factor, shape=1024, return_array=False):\n",
        "    \n",
        "    batch_precisions = []\n",
        "    for msk, pId in zip(output_batch, pIds): \n",
        "        target_boxes = pId_boxes_dict[pId] if pId in pId_boxes_dict else []\n",
        "\n",
        "        if len(target_boxes)>0:\n",
        "            target_boxes = [[int(round(c/float(rescale_factor))) for c in box_t] for box_t in target_boxes]\n",
        "\n",
        "        predicted_boxes, confidences = mask_boxes(msk) \n",
        "        batch_precisions.append(average_precision_image(predicted_boxes, confidences, target_boxes, shape=shape))\n",
        "    if return_array:\n",
        "        return np.asarray(batch_precisions)\n",
        "    else:\n",
        "        return np.nanmean(np.asarray(batch_precisions)) \n",
        "  \n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "207499baf520b5952e347ff28351d66b89f1d7ca",
        "trusted": true,
        "id": "1ZtgC1w621zL"
      },
      "source": [
        "class RunningAverage():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "    \n",
        "    def update(self, val):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "    \n",
        "    def __call__(self):\n",
        "        return self.total/float(self.steps)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4c14e5666c2ddbb03edcc139bce0a2c445ee8e21",
        "trusted": true,
        "id": "5vc7UnTO21zM"
      },
      "source": [
        "def save_checkpoint(state, is_best, metric):\n",
        "    filename = 'last.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, metric+'.best.pth.tar')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWQsQI-blxNh"
      },
      "source": [
        "def mean_iou(y_true, y_pred, device):\n",
        "    y_pred = torch.round(y_pred)\n",
        "    intersect = (y_true * y_pred).sum(axis=[1, 2, 3])\n",
        "    union = y_true.sum(axis=[1, 2, 3]) + y_pred.sum(axis=[1, 2, 3])\n",
        "    smooth = torch.ones(intersect.shape).cuda()\n",
        "    return ((intersect + smooth) / (union - intersect + smooth)).mean()"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKzuU3LDVv3O"
      },
      "source": [
        "# for i, (input_batch, labels_batch, pIds_batch) in enumerate(loader_train):\n",
        "#   print(i)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d9346c5e323d2ce3281044b8049abfb0d8234c49",
        "trusted": true,
        "id": "JrMIaQ4S21zM"
      },
      "source": [
        "def train(model, dataloader, optimizer, loss_fn, num_steps, pId_boxes_dict, rescale_factor, shape, save_summary_steps=10):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    summary = []\n",
        "    loss_avg = RunningAverage()\n",
        "\n",
        "    loss_avg_t_hist_ep, loss_t_hist_ep, prec_t_hist_ep = [], [], []\n",
        "\n",
        "\n",
        "    start = time.time()        \n",
        "    \n",
        "    for i, (input_batch, labels_batch, pIds_batch) in enumerate(dataloader):\n",
        "        # if i > num_steps:\n",
        "        #     break\n",
        "\n",
        "        input_batch = Variable(input_batch).cuda() if gpu_available else Variable(input_batch).float()\n",
        "        labels_batch = Variable(labels_batch).cuda() if gpu_available else Variable(labels_batch).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output_batch = model(input_batch)\n",
        "\n",
        "        loss = loss_fn(output_batch, labels_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_avg.update(loss.item())\n",
        "        loss_t_hist_ep.append(loss.item())\n",
        "        loss_avg_t_hist_ep.append(loss_avg())\n",
        "\n",
        "        if i % save_summary_steps == 0:\n",
        "\n",
        "            output_batch = output_batch.data.cpu().numpy()\n",
        "            prec_batch = average_precision_batch(output_batch, pIds_batch, pId_boxes_dict, rescale_factor, shape)\n",
        "            prec_t_hist_ep.append(prec_batch)\n",
        "            summary_batch_string = \"batch loss = {:05.7f} ;  \".format(loss.item())\n",
        "            summary_batch_string += \"average loss = {:05.7f} ;  \".format(loss_avg())\n",
        "            summary_batch_string += \"batch precision = {:05.7f} ;  \".format(prec_batch)\n",
        "            \n",
        "            print('--- Train batch {} / {}: '.format(i, num_steps) + summary_batch_string)\n",
        "            delta_time = time.time() - start\n",
        "            print('    {} batches processed in {:.2f} seconds'.format(save_summary_steps, delta_time))\n",
        "            start = time.time()\n",
        "\n",
        "    metrics_string = \"average loss = {:05.7f} ;  \".format(loss_avg())\n",
        "    print(\"- Train epoch metrics summary: \" + metrics_string)\n",
        "    \n",
        "    return loss_avg_t_hist_ep, loss_t_hist_ep, prec_t_hist_ep"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9de116575eb50215b6288249233d4145fa27c388",
        "trusted": true,
        "id": "WlUOlID_21zM"
      },
      "source": [
        "def evaluate(model, dataloader, loss_fn, num_steps, pId_boxes_dict, rescale_factor, shape):\n",
        "\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    precisions = []\n",
        "\n",
        "    start = time.time()\n",
        "    for i, (input_batch, labels_batch, pIds_batch) in enumerate(dataloader):\n",
        "        if i > num_steps:\n",
        "            break\n",
        "        input_batch = Variable(input_batch).cuda() if gpu_available else Variable(input_batch).float()\n",
        "        labels_batch = Variable(labels_batch).cuda() if gpu_available else Variable(labels_batch).float()\n",
        "\n",
        "        output_batch = model(input_batch)\n",
        "        \n",
        "        loss = loss_fn(output_batch, labels_batch)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        output_batch = output_batch.data.cpu()\n",
        "        prec_batch = average_precision_batch(output_batch, pIds_batch, pId_boxes_dict, rescale_factor, shape, return_array=True)\n",
        "        # iou = mean_iou(out, y, device)\n",
        "        for p in prec_batch:\n",
        "            precisions.append(p)\n",
        "        print('--- Validation batch {} / {}: '.format(i, num_steps))\n",
        "\n",
        "    metrics_mean = {'loss' : np.nanmean(losses),\n",
        "                    'precision' : np.nanmean(np.asarray(precisions)),\n",
        "                    }\n",
        "    metrics_string = \"average loss = {:05.7f} ;  \".format(metrics_mean['loss'])\n",
        "    # metrics_string += \"average precision = {:05.7f} ;  \".format(metrics_mean['precision'])\n",
        "    print(\"- Eval metrics : \" + metrics_string)\n",
        "    delta_time = time.time() - start\n",
        "    print('  Evaluation run in {:.2f} seconds.'.format(delta_time))\n",
        "    \n",
        "    return metrics_mean"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2e5cff316bb314cbb22b977864dedb664f99d829",
        "trusted": true,
        "id": "BM6dKLS121zN"
      },
      "source": [
        "def train_and_evaluate(model, train_dataloader, val_dataloader, lr_init, loss_fn, num_epochs, \n",
        "                       num_steps_train, num_steps_eval, pId_boxes_dict, rescale_factor, shape, restore_file=None):\n",
        "\n",
        "    if restore_file is not None:\n",
        "        checkpoint = torch.load(restore_file)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "            \n",
        "    best_val_loss = 1e+15\n",
        "    best_val_prec = 0.0\n",
        "    best_loss_model = None\n",
        "    best_prec_model = None\n",
        "\n",
        "    loss_t_history = []\n",
        "    loss_v_history = []\n",
        "    loss_avg_t_history = []\n",
        "    prec_t_history = []\n",
        "    prec_v_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        lr = lr_init * 0.5**float(epoch)\n",
        "        # lr = 0.001\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        print(\"Epoch {}/{}. Learning rate = {:05.3f}.\".format(epoch + 1, num_epochs, lr))\n",
        "        loss_avg_t_hist_ep, loss_t_hist_ep, prec_t_hist_ep = train(model, train_dataloader, optimizer, loss_fn, \n",
        "                                                                   num_steps_train, pId_boxes_dict, rescale_factor, shape)\n",
        "        loss_avg_t_history += loss_avg_t_hist_ep\n",
        "        loss_t_history += loss_t_hist_ep\n",
        "        prec_t_history += prec_t_hist_ep\n",
        "        \n",
        "        val_metrics = evaluate(model, val_dataloader, loss_fn, num_steps_eval, pId_boxes_dict, rescale_factor, shape)\n",
        "\n",
        "        val_loss = val_metrics['loss']\n",
        "        val_prec = val_metrics['precision']\n",
        "        \n",
        "        loss_v_history += len(loss_t_hist_ep) * [val_loss]\n",
        "        prec_v_history += len(prec_t_hist_ep) * [val_prec]\n",
        "\n",
        "        is_best_loss = val_loss<=best_val_loss\n",
        "        is_best_prec = val_prec>=best_val_prec\n",
        "        \n",
        "        if is_best_loss:\n",
        "            print(\"- Found new best loss: {:.4f}\".format(val_loss))\n",
        "            best_val_loss = val_loss\n",
        "            best_loss_model = model\n",
        "        if is_best_prec:\n",
        "            print(\"- Found new best precision: {:.4f}\".format(val_prec))\n",
        "            best_val_prec = val_prec\n",
        "            best_prec_model = model\n",
        "            \n",
        "        save_checkpoint({'epoch': epoch + 1,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'optim_dict' : optimizer.state_dict()},\n",
        "                         is_best=is_best_loss,\n",
        "                         metric='loss')\n",
        "        save_checkpoint({'epoch': epoch + 1,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'optim_dict' : optimizer.state_dict()},\n",
        "                         is_best=is_best_prec,\n",
        "                         metric='prec')\n",
        "        \n",
        "        delta_time = time.time() - start\n",
        "        print('Epoch run in {:.2f} minutes'.format(delta_time/60.))\n",
        "\n",
        "    histories = {'loss avg train' : loss_avg_t_history,\n",
        "                 'loss train' : loss_t_history,\n",
        "                 'precision train' : prec_t_history,\n",
        "                 'loss validation' : loss_v_history, \n",
        "                 'precision validation' : prec_v_history}\n",
        "    best_models = {'best loss model' : best_loss_model,\n",
        "                   'best precision model' : best_prec_model}\n",
        "    \n",
        "    return histories, best_models"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5c1e0f2d944a30cfa72d39b118caa72eba6286a5",
        "trusted": true,
        "id": "6Vw_PbUg21zN"
      },
      "source": [
        "def predict(model, dataloader): \n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = {}\n",
        "\n",
        "    for i, (test_batch, pIds) in enumerate(dataloader):\n",
        "        print('Predicting batch {} / {}.'.format(i+1, len(dataloader)))\n",
        "        test_batch = Variable(test_batch).cuda() if gpu_available else Variable(test_batch).float()\n",
        "            \n",
        "        output_batch = model(test_batch)\n",
        "        sig = nn.Sigmoid().cuda()\n",
        "        output_batch = sig(output_batch)\n",
        "        output_batch = output_batch.data.cpu().numpy()\n",
        "        for pId, output in zip(pIds, output_batch):\n",
        "            predictions[pId] = output\n",
        "        \n",
        "    return predictions"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1frcGB2MU-W",
        "outputId": "cb743a54-6a6b-4211-f418-6a7ff7510f42"
      },
      "source": [
        "debug = False\n",
        "\n",
        "model = model1().cuda() if gpu_available else model1()\n",
        "loss_fn = BCEWithLogitsLoss2d().cuda() if gpu_available else BCEWithLogitsLoss2d()\n",
        "lr_init = 0.001\n",
        "\n",
        "num_epochs = 2\n",
        "num_steps_train = len(loader_train)\n",
        "num_steps_eval = len(loader_valid)\n",
        "\n",
        "shape = int(round(original_image_shape / rescale_factor))\n",
        "\n",
        "print(\"Starting training for {} epochs\".format(num_epochs))\n",
        "histories, best_models = train_and_evaluate(model, loader_train, loader_valid, lr_init, loss_fn, \n",
        "                                            num_epochs, num_steps_train, num_steps_eval, pId_boxes_dict, rescale_factor, shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training for 2 epochs\n",
            "Epoch 1/2. Learning rate = 0.001.\n",
            "--- Train batch 0 / 1501: batch loss = 0.7246256 ;  average loss = 0.7246256 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 120.32 seconds\n",
            "--- Train batch 10 / 1501: batch loss = 0.5628086 ;  average loss = 0.6308147 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 110.01 seconds\n",
            "--- Train batch 20 / 1501: batch loss = 0.5288026 ;  average loss = 0.5860894 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 110.14 seconds\n",
            "--- Train batch 30 / 1501: batch loss = 0.4694694 ;  average loss = 0.5554578 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.74 seconds\n",
            "--- Train batch 40 / 1501: batch loss = 0.4613073 ;  average loss = 0.5320846 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 107.10 seconds\n",
            "--- Train batch 50 / 1501: batch loss = 0.4180200 ;  average loss = 0.5115052 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 108.42 seconds\n",
            "--- Train batch 60 / 1501: batch loss = 0.3938688 ;  average loss = 0.4943446 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.73 seconds\n",
            "--- Train batch 70 / 1501: batch loss = 0.3634934 ;  average loss = 0.4790938 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.66 seconds\n",
            "--- Train batch 80 / 1501: batch loss = 0.3497760 ;  average loss = 0.4658501 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.09 seconds\n",
            "--- Train batch 90 / 1501: batch loss = 0.3596833 ;  average loss = 0.4529595 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 107.59 seconds\n",
            "--- Train batch 100 / 1501: batch loss = 0.3112068 ;  average loss = 0.4399000 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 108.96 seconds\n",
            "--- Train batch 110 / 1501: batch loss = 0.3177302 ;  average loss = 0.4281543 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.17 seconds\n",
            "--- Train batch 120 / 1501: batch loss = 0.2962209 ;  average loss = 0.4172970 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.64 seconds\n",
            "--- Train batch 130 / 1501: batch loss = 0.2770234 ;  average loss = 0.4062703 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.72 seconds\n",
            "--- Train batch 140 / 1501: batch loss = 0.2552965 ;  average loss = 0.3962337 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.82 seconds\n",
            "--- Train batch 150 / 1501: batch loss = 0.2241132 ;  average loss = 0.3875027 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 107.97 seconds\n",
            "--- Train batch 160 / 1501: batch loss = 0.1992293 ;  average loss = 0.3780728 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.89 seconds\n",
            "--- Train batch 170 / 1501: batch loss = 0.1919276 ;  average loss = 0.3682547 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.71 seconds\n",
            "--- Train batch 180 / 1501: batch loss = 0.2061366 ;  average loss = 0.3601483 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 107.36 seconds\n",
            "--- Train batch 190 / 1501: batch loss = 0.1825371 ;  average loss = 0.3514443 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.24 seconds\n",
            "--- Train batch 200 / 1501: batch loss = 0.1906852 ;  average loss = 0.3435466 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.42 seconds\n",
            "--- Train batch 210 / 1501: batch loss = 0.1649481 ;  average loss = 0.3359878 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.12 seconds\n",
            "--- Train batch 220 / 1501: batch loss = 0.1766044 ;  average loss = 0.3288119 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.87 seconds\n",
            "--- Train batch 230 / 1501: batch loss = 0.1762945 ;  average loss = 0.3219030 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.26 seconds\n",
            "--- Train batch 240 / 1501: batch loss = 0.1446047 ;  average loss = 0.3155275 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.67 seconds\n",
            "--- Train batch 250 / 1501: batch loss = 0.1687488 ;  average loss = 0.3093897 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.27 seconds\n",
            "--- Train batch 260 / 1501: batch loss = 0.1340534 ;  average loss = 0.3033598 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.32 seconds\n",
            "--- Train batch 270 / 1501: batch loss = 0.1427768 ;  average loss = 0.2977049 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.98 seconds\n",
            "--- Train batch 280 / 1501: batch loss = 0.1257693 ;  average loss = 0.2918777 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.76 seconds\n",
            "--- Train batch 290 / 1501: batch loss = 0.1197181 ;  average loss = 0.2870721 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.32 seconds\n",
            "--- Train batch 300 / 1501: batch loss = 0.0970092 ;  average loss = 0.2816027 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.97 seconds\n",
            "--- Train batch 310 / 1501: batch loss = 0.1417346 ;  average loss = 0.2764032 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.62 seconds\n",
            "--- Train batch 320 / 1501: batch loss = 0.1379402 ;  average loss = 0.2711687 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.57 seconds\n",
            "--- Train batch 330 / 1501: batch loss = 0.1276226 ;  average loss = 0.2668805 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.57 seconds\n",
            "--- Train batch 340 / 1501: batch loss = 0.0783366 ;  average loss = 0.2622753 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.71 seconds\n",
            "--- Train batch 350 / 1501: batch loss = 0.1099827 ;  average loss = 0.2582568 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.91 seconds\n",
            "--- Train batch 360 / 1501: batch loss = 0.1202862 ;  average loss = 0.2543002 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.55 seconds\n",
            "--- Train batch 370 / 1501: batch loss = 0.1062215 ;  average loss = 0.2499022 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.60 seconds\n",
            "--- Train batch 380 / 1501: batch loss = 0.1854643 ;  average loss = 0.2460638 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.22 seconds\n",
            "--- Train batch 390 / 1501: batch loss = 0.1755229 ;  average loss = 0.2431959 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.38 seconds\n",
            "--- Train batch 400 / 1501: batch loss = 0.0917844 ;  average loss = 0.2402109 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.31 seconds\n",
            "--- Train batch 410 / 1501: batch loss = 0.0862266 ;  average loss = 0.2375973 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.34 seconds\n",
            "--- Train batch 420 / 1501: batch loss = 0.0949327 ;  average loss = 0.2344305 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.27 seconds\n",
            "--- Train batch 430 / 1501: batch loss = 0.1342133 ;  average loss = 0.2316933 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.14 seconds\n",
            "--- Train batch 440 / 1501: batch loss = 0.1164366 ;  average loss = 0.2286908 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.80 seconds\n",
            "--- Train batch 450 / 1501: batch loss = 0.0840276 ;  average loss = 0.2257932 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.14 seconds\n",
            "--- Train batch 460 / 1501: batch loss = 0.0914180 ;  average loss = 0.2235116 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.75 seconds\n",
            "--- Train batch 470 / 1501: batch loss = 0.1590232 ;  average loss = 0.2207458 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.35 seconds\n",
            "--- Train batch 480 / 1501: batch loss = 0.1078962 ;  average loss = 0.2184113 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.80 seconds\n",
            "--- Train batch 490 / 1501: batch loss = 0.0890721 ;  average loss = 0.2157843 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.27 seconds\n",
            "--- Train batch 500 / 1501: batch loss = 0.0576419 ;  average loss = 0.2129536 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.43 seconds\n",
            "--- Train batch 510 / 1501: batch loss = 0.0664780 ;  average loss = 0.2107763 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.87 seconds\n",
            "--- Train batch 520 / 1501: batch loss = 0.1041338 ;  average loss = 0.2083559 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.58 seconds\n",
            "--- Train batch 530 / 1501: batch loss = 0.0586188 ;  average loss = 0.2060982 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.87 seconds\n",
            "--- Train batch 540 / 1501: batch loss = 0.0816423 ;  average loss = 0.2040618 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.84 seconds\n",
            "--- Train batch 550 / 1501: batch loss = 0.0479246 ;  average loss = 0.2017929 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.06 seconds\n",
            "--- Train batch 560 / 1501: batch loss = 0.0731614 ;  average loss = 0.1994557 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.23 seconds\n",
            "--- Train batch 570 / 1501: batch loss = 0.1843508 ;  average loss = 0.1979343 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.76 seconds\n",
            "--- Train batch 580 / 1501: batch loss = 0.0725667 ;  average loss = 0.1961395 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.64 seconds\n",
            "--- Train batch 590 / 1501: batch loss = 0.1589924 ;  average loss = 0.1944087 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.31 seconds\n",
            "--- Train batch 600 / 1501: batch loss = 0.1055121 ;  average loss = 0.1927873 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.78 seconds\n",
            "--- Train batch 610 / 1501: batch loss = 0.0443789 ;  average loss = 0.1911702 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.73 seconds\n",
            "--- Train batch 620 / 1501: batch loss = 0.0590818 ;  average loss = 0.1895882 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.45 seconds\n",
            "--- Train batch 630 / 1501: batch loss = 0.1725758 ;  average loss = 0.1881623 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.12 seconds\n",
            "--- Train batch 640 / 1501: batch loss = 0.0777387 ;  average loss = 0.1863943 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.35 seconds\n",
            "--- Train batch 650 / 1501: batch loss = 0.0798560 ;  average loss = 0.1849298 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.89 seconds\n",
            "--- Train batch 660 / 1501: batch loss = 0.1389851 ;  average loss = 0.1833736 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.29 seconds\n",
            "--- Train batch 670 / 1501: batch loss = 0.0700106 ;  average loss = 0.1822158 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.82 seconds\n",
            "--- Train batch 680 / 1501: batch loss = 0.0359039 ;  average loss = 0.1806661 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.83 seconds\n",
            "--- Train batch 690 / 1501: batch loss = 0.0700797 ;  average loss = 0.1791466 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.16 seconds\n",
            "--- Train batch 700 / 1501: batch loss = 0.0577349 ;  average loss = 0.1776792 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.48 seconds\n",
            "--- Train batch 710 / 1501: batch loss = 0.0826059 ;  average loss = 0.1761933 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.19 seconds\n",
            "--- Train batch 720 / 1501: batch loss = 0.2624828 ;  average loss = 0.1750584 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.17 seconds\n",
            "--- Train batch 730 / 1501: batch loss = 0.1672686 ;  average loss = 0.1737592 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 101.58 seconds\n",
            "--- Train batch 740 / 1501: batch loss = 0.0953045 ;  average loss = 0.1727589 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.90 seconds\n",
            "--- Train batch 750 / 1501: batch loss = 0.0580056 ;  average loss = 0.1715314 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 101.88 seconds\n",
            "--- Train batch 760 / 1501: batch loss = 0.0902981 ;  average loss = 0.1703310 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 100.63 seconds\n",
            "--- Train batch 770 / 1501: batch loss = 0.1114806 ;  average loss = 0.1690739 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.05 seconds\n",
            "--- Train batch 780 / 1501: batch loss = 0.0707071 ;  average loss = 0.1676863 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.38 seconds\n",
            "--- Train batch 790 / 1501: batch loss = 0.0498338 ;  average loss = 0.1665905 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.97 seconds\n",
            "--- Train batch 800 / 1501: batch loss = 0.0833736 ;  average loss = 0.1654451 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 101.60 seconds\n",
            "--- Train batch 810 / 1501: batch loss = 0.0976088 ;  average loss = 0.1644594 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.29 seconds\n",
            "--- Train batch 820 / 1501: batch loss = 0.0779491 ;  average loss = 0.1634552 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 101.78 seconds\n",
            "--- Train batch 830 / 1501: batch loss = 0.0835253 ;  average loss = 0.1625188 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.57 seconds\n",
            "--- Train batch 840 / 1501: batch loss = 0.0325088 ;  average loss = 0.1616761 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.74 seconds\n",
            "--- Train batch 850 / 1501: batch loss = 0.0761267 ;  average loss = 0.1604940 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.56 seconds\n",
            "--- Train batch 860 / 1501: batch loss = 0.0674077 ;  average loss = 0.1596016 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.05 seconds\n",
            "--- Train batch 870 / 1501: batch loss = 0.0412429 ;  average loss = 0.1586752 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.67 seconds\n",
            "--- Train batch 880 / 1501: batch loss = 0.1088368 ;  average loss = 0.1577802 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 101.61 seconds\n",
            "--- Train batch 890 / 1501: batch loss = 0.0400674 ;  average loss = 0.1567432 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 101.75 seconds\n",
            "--- Train batch 900 / 1501: batch loss = 0.0573828 ;  average loss = 0.1557091 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.70 seconds\n",
            "--- Train batch 910 / 1501: batch loss = 0.0531312 ;  average loss = 0.1546232 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.44 seconds\n",
            "--- Train batch 920 / 1501: batch loss = 0.0377210 ;  average loss = 0.1536970 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.37 seconds\n",
            "--- Train batch 930 / 1501: batch loss = 0.1102674 ;  average loss = 0.1529784 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 100.76 seconds\n",
            "--- Train batch 940 / 1501: batch loss = 0.0435051 ;  average loss = 0.1520497 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.23 seconds\n",
            "--- Train batch 950 / 1501: batch loss = 0.1048581 ;  average loss = 0.1513992 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.30 seconds\n",
            "--- Train batch 960 / 1501: batch loss = 0.0443554 ;  average loss = 0.1507190 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.81 seconds\n",
            "--- Train batch 970 / 1501: batch loss = 0.0562968 ;  average loss = 0.1500210 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.24 seconds\n",
            "--- Train batch 980 / 1501: batch loss = 0.0576130 ;  average loss = 0.1492900 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.41 seconds\n",
            "--- Train batch 990 / 1501: batch loss = 0.0280483 ;  average loss = 0.1485572 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.92 seconds\n",
            "--- Train batch 1000 / 1501: batch loss = 0.0257827 ;  average loss = 0.1476099 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.16 seconds\n",
            "--- Train batch 1010 / 1501: batch loss = 0.0583171 ;  average loss = 0.1468318 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.27 seconds\n",
            "--- Train batch 1020 / 1501: batch loss = 0.0742628 ;  average loss = 0.1460788 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.30 seconds\n",
            "--- Train batch 1030 / 1501: batch loss = 0.0394092 ;  average loss = 0.1452300 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.01 seconds\n",
            "--- Train batch 1040 / 1501: batch loss = 0.0801841 ;  average loss = 0.1446689 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.44 seconds\n",
            "--- Train batch 1050 / 1501: batch loss = 0.0870499 ;  average loss = 0.1440136 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.44 seconds\n",
            "--- Train batch 1060 / 1501: batch loss = 0.0839285 ;  average loss = 0.1433539 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.43 seconds\n",
            "--- Train batch 1070 / 1501: batch loss = 0.0813141 ;  average loss = 0.1426523 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.00 seconds\n",
            "--- Train batch 1080 / 1501: batch loss = 0.1157645 ;  average loss = 0.1419813 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.89 seconds\n",
            "--- Train batch 1090 / 1501: batch loss = 0.0485826 ;  average loss = 0.1413813 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.38 seconds\n",
            "--- Train batch 1100 / 1501: batch loss = 0.0982894 ;  average loss = 0.1408487 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.02 seconds\n",
            "--- Train batch 1110 / 1501: batch loss = 0.1433300 ;  average loss = 0.1402261 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.99 seconds\n",
            "--- Train batch 1120 / 1501: batch loss = 0.0542140 ;  average loss = 0.1395064 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.75 seconds\n",
            "--- Train batch 1130 / 1501: batch loss = 0.1066983 ;  average loss = 0.1391116 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.85 seconds\n",
            "--- Train batch 1140 / 1501: batch loss = 0.0428036 ;  average loss = 0.1385348 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.90 seconds\n",
            "--- Train batch 1150 / 1501: batch loss = 0.0608927 ;  average loss = 0.1379149 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.97 seconds\n",
            "--- Train batch 1160 / 1501: batch loss = 0.1297266 ;  average loss = 0.1376485 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.85 seconds\n",
            "--- Train batch 1170 / 1501: batch loss = 0.0399399 ;  average loss = 0.1370609 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 101.42 seconds\n",
            "--- Train batch 1180 / 1501: batch loss = 0.1030689 ;  average loss = 0.1366032 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 101.61 seconds\n",
            "--- Train batch 1190 / 1501: batch loss = 0.0746870 ;  average loss = 0.1360257 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.31 seconds\n",
            "--- Train batch 1200 / 1501: batch loss = 0.0864249 ;  average loss = 0.1354414 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.27 seconds\n",
            "--- Train batch 1210 / 1501: batch loss = 0.0505295 ;  average loss = 0.1349261 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.85 seconds\n",
            "--- Train batch 1220 / 1501: batch loss = 0.1049663 ;  average loss = 0.1344337 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.64 seconds\n",
            "--- Train batch 1230 / 1501: batch loss = 0.0384098 ;  average loss = 0.1341371 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.56 seconds\n",
            "--- Train batch 1240 / 1501: batch loss = 0.0402288 ;  average loss = 0.1337674 ;  batch precision = 00nan ;  \n",
            "    10 batches processed in 105.04 seconds\n",
            "--- Train batch 1250 / 1501: batch loss = 0.0748969 ;  average loss = 0.1332396 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.12 seconds\n",
            "--- Train batch 1260 / 1501: batch loss = 0.1166757 ;  average loss = 0.1327053 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.05 seconds\n",
            "--- Train batch 1270 / 1501: batch loss = 0.0278173 ;  average loss = 0.1321720 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.02 seconds\n",
            "--- Train batch 1280 / 1501: batch loss = 0.0506570 ;  average loss = 0.1318113 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 105.83 seconds\n",
            "--- Train batch 1290 / 1501: batch loss = 0.0430870 ;  average loss = 0.1314028 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 106.15 seconds\n",
            "--- Train batch 1300 / 1501: batch loss = 0.0390493 ;  average loss = 0.1310231 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.11 seconds\n",
            "--- Train batch 1310 / 1501: batch loss = 0.0354578 ;  average loss = 0.1305787 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.13 seconds\n",
            "--- Train batch 1320 / 1501: batch loss = 0.0376083 ;  average loss = 0.1300624 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.39 seconds\n",
            "--- Train batch 1330 / 1501: batch loss = 0.0452877 ;  average loss = 0.1295980 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.67 seconds\n",
            "--- Train batch 1340 / 1501: batch loss = 0.0801925 ;  average loss = 0.1291677 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.57 seconds\n",
            "--- Train batch 1350 / 1501: batch loss = 0.0347963 ;  average loss = 0.1287223 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 101.92 seconds\n",
            "--- Train batch 1360 / 1501: batch loss = 0.1622923 ;  average loss = 0.1283802 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.12 seconds\n",
            "--- Train batch 1370 / 1501: batch loss = 0.0848475 ;  average loss = 0.1280036 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.40 seconds\n",
            "--- Train batch 1380 / 1501: batch loss = 0.0530239 ;  average loss = 0.1276818 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.99 seconds\n",
            "--- Train batch 1390 / 1501: batch loss = 0.0699763 ;  average loss = 0.1272777 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.64 seconds\n",
            "--- Train batch 1400 / 1501: batch loss = 0.0600764 ;  average loss = 0.1268625 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.29 seconds\n",
            "--- Train batch 1410 / 1501: batch loss = 0.0861170 ;  average loss = 0.1265319 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.62 seconds\n",
            "--- Train batch 1420 / 1501: batch loss = 0.0587583 ;  average loss = 0.1260819 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.01 seconds\n",
            "--- Train batch 1430 / 1501: batch loss = 0.0650486 ;  average loss = 0.1256858 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.26 seconds\n",
            "--- Train batch 1440 / 1501: batch loss = 0.0293351 ;  average loss = 0.1253106 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.66 seconds\n",
            "--- Train batch 1450 / 1501: batch loss = 0.0680906 ;  average loss = 0.1249594 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.16 seconds\n",
            "--- Train batch 1460 / 1501: batch loss = 0.0680962 ;  average loss = 0.1245042 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.12 seconds\n",
            "--- Train batch 1470 / 1501: batch loss = 0.0807408 ;  average loss = 0.1241827 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.82 seconds\n",
            "--- Train batch 1480 / 1501: batch loss = 0.0482726 ;  average loss = 0.1237243 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 102.79 seconds\n",
            "--- Train batch 1490 / 1501: batch loss = 0.0987181 ;  average loss = 0.1235330 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 103.69 seconds\n",
            "--- Train batch 1500 / 1501: batch loss = 0.0392337 ;  average loss = 0.1232934 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 104.31 seconds\n",
            "- Train epoch metrics summary: average loss = 0.1232934 ;  \n",
            "--- Validation batch 0 / 167: \n",
            "--- Validation batch 1 / 167: \n",
            "--- Validation batch 2 / 167: \n",
            "--- Validation batch 3 / 167: \n",
            "--- Validation batch 4 / 167: \n",
            "--- Validation batch 5 / 167: \n",
            "--- Validation batch 6 / 167: \n",
            "--- Validation batch 7 / 167: \n",
            "--- Validation batch 8 / 167: \n",
            "--- Validation batch 9 / 167: \n",
            "--- Validation batch 10 / 167: \n",
            "--- Validation batch 11 / 167: \n",
            "--- Validation batch 12 / 167: \n",
            "--- Validation batch 13 / 167: \n",
            "--- Validation batch 14 / 167: \n",
            "--- Validation batch 15 / 167: \n",
            "--- Validation batch 16 / 167: \n",
            "--- Validation batch 17 / 167: \n",
            "--- Validation batch 18 / 167: \n",
            "--- Validation batch 19 / 167: \n",
            "--- Validation batch 20 / 167: \n",
            "--- Validation batch 21 / 167: \n",
            "--- Validation batch 22 / 167: \n",
            "--- Validation batch 23 / 167: \n",
            "--- Validation batch 24 / 167: \n",
            "--- Validation batch 25 / 167: \n",
            "--- Validation batch 26 / 167: \n",
            "--- Validation batch 27 / 167: \n",
            "--- Validation batch 28 / 167: \n",
            "--- Validation batch 29 / 167: \n",
            "--- Validation batch 30 / 167: \n",
            "--- Validation batch 31 / 167: \n",
            "--- Validation batch 32 / 167: \n",
            "--- Validation batch 33 / 167: \n",
            "--- Validation batch 34 / 167: \n",
            "--- Validation batch 35 / 167: \n",
            "--- Validation batch 36 / 167: \n",
            "--- Validation batch 37 / 167: \n",
            "--- Validation batch 38 / 167: \n",
            "--- Validation batch 39 / 167: \n",
            "--- Validation batch 40 / 167: \n",
            "--- Validation batch 41 / 167: \n",
            "--- Validation batch 42 / 167: \n",
            "--- Validation batch 43 / 167: \n",
            "--- Validation batch 44 / 167: \n",
            "--- Validation batch 45 / 167: \n",
            "--- Validation batch 46 / 167: \n",
            "--- Validation batch 47 / 167: \n",
            "--- Validation batch 48 / 167: \n",
            "--- Validation batch 49 / 167: \n",
            "--- Validation batch 50 / 167: \n",
            "--- Validation batch 51 / 167: \n",
            "--- Validation batch 52 / 167: \n",
            "--- Validation batch 53 / 167: \n",
            "--- Validation batch 54 / 167: \n",
            "--- Validation batch 55 / 167: \n",
            "--- Validation batch 56 / 167: \n",
            "--- Validation batch 57 / 167: \n",
            "--- Validation batch 58 / 167: \n",
            "--- Validation batch 59 / 167: \n",
            "--- Validation batch 60 / 167: \n",
            "--- Validation batch 61 / 167: \n",
            "--- Validation batch 62 / 167: \n",
            "--- Validation batch 63 / 167: \n",
            "--- Validation batch 64 / 167: \n",
            "--- Validation batch 65 / 167: \n",
            "--- Validation batch 66 / 167: \n",
            "--- Validation batch 67 / 167: \n",
            "--- Validation batch 68 / 167: \n",
            "--- Validation batch 69 / 167: \n",
            "--- Validation batch 70 / 167: \n",
            "--- Validation batch 71 / 167: \n",
            "--- Validation batch 72 / 167: \n",
            "--- Validation batch 73 / 167: \n",
            "--- Validation batch 74 / 167: \n",
            "--- Validation batch 75 / 167: \n",
            "--- Validation batch 76 / 167: \n",
            "--- Validation batch 77 / 167: \n",
            "--- Validation batch 78 / 167: \n",
            "--- Validation batch 79 / 167: \n",
            "--- Validation batch 80 / 167: \n",
            "--- Validation batch 81 / 167: \n",
            "--- Validation batch 82 / 167: \n",
            "--- Validation batch 83 / 167: \n",
            "--- Validation batch 84 / 167: \n",
            "--- Validation batch 85 / 167: \n",
            "--- Validation batch 86 / 167: \n",
            "--- Validation batch 87 / 167: \n",
            "--- Validation batch 88 / 167: \n",
            "--- Validation batch 89 / 167: \n",
            "--- Validation batch 90 / 167: \n",
            "--- Validation batch 91 / 167: \n",
            "--- Validation batch 92 / 167: \n",
            "--- Validation batch 93 / 167: \n",
            "--- Validation batch 94 / 167: \n",
            "--- Validation batch 95 / 167: \n",
            "--- Validation batch 96 / 167: \n",
            "--- Validation batch 97 / 167: \n",
            "--- Validation batch 98 / 167: \n",
            "--- Validation batch 99 / 167: \n",
            "--- Validation batch 100 / 167: \n",
            "--- Validation batch 101 / 167: \n",
            "--- Validation batch 102 / 167: \n",
            "--- Validation batch 103 / 167: \n",
            "--- Validation batch 104 / 167: \n",
            "--- Validation batch 105 / 167: \n",
            "--- Validation batch 106 / 167: \n",
            "--- Validation batch 107 / 167: \n",
            "--- Validation batch 108 / 167: \n",
            "--- Validation batch 109 / 167: \n",
            "--- Validation batch 110 / 167: \n",
            "--- Validation batch 111 / 167: \n",
            "--- Validation batch 112 / 167: \n",
            "--- Validation batch 113 / 167: \n",
            "--- Validation batch 114 / 167: \n",
            "--- Validation batch 115 / 167: \n",
            "--- Validation batch 116 / 167: \n",
            "--- Validation batch 117 / 167: \n",
            "--- Validation batch 118 / 167: \n",
            "--- Validation batch 119 / 167: \n",
            "--- Validation batch 120 / 167: \n",
            "--- Validation batch 121 / 167: \n",
            "--- Validation batch 122 / 167: \n",
            "--- Validation batch 123 / 167: \n",
            "--- Validation batch 124 / 167: \n",
            "--- Validation batch 125 / 167: \n",
            "--- Validation batch 126 / 167: \n",
            "--- Validation batch 127 / 167: \n",
            "--- Validation batch 128 / 167: \n",
            "--- Validation batch 129 / 167: \n",
            "--- Validation batch 130 / 167: \n",
            "--- Validation batch 131 / 167: \n",
            "--- Validation batch 132 / 167: \n",
            "--- Validation batch 133 / 167: \n",
            "--- Validation batch 134 / 167: \n",
            "--- Validation batch 135 / 167: \n",
            "--- Validation batch 136 / 167: \n",
            "--- Validation batch 137 / 167: \n",
            "--- Validation batch 138 / 167: \n",
            "--- Validation batch 139 / 167: \n",
            "--- Validation batch 140 / 167: \n",
            "--- Validation batch 141 / 167: \n",
            "--- Validation batch 142 / 167: \n",
            "--- Validation batch 143 / 167: \n",
            "--- Validation batch 144 / 167: \n",
            "--- Validation batch 145 / 167: \n",
            "--- Validation batch 146 / 167: \n",
            "--- Validation batch 147 / 167: \n",
            "--- Validation batch 148 / 167: \n",
            "--- Validation batch 149 / 167: \n",
            "--- Validation batch 150 / 167: \n",
            "--- Validation batch 151 / 167: \n",
            "--- Validation batch 152 / 167: \n",
            "--- Validation batch 153 / 167: \n",
            "--- Validation batch 154 / 167: \n",
            "--- Validation batch 155 / 167: \n",
            "--- Validation batch 156 / 167: \n",
            "--- Validation batch 157 / 167: \n",
            "--- Validation batch 158 / 167: \n",
            "--- Validation batch 159 / 167: \n",
            "--- Validation batch 160 / 167: \n",
            "--- Validation batch 161 / 167: \n",
            "--- Validation batch 162 / 167: \n",
            "--- Validation batch 163 / 167: \n",
            "--- Validation batch 164 / 167: \n",
            "--- Validation batch 165 / 167: \n",
            "--- Validation batch 166 / 167: \n",
            "- Eval metrics : average loss = 0.0974635 ;  \n",
            "  Evaluation run in 1532.87 seconds.\n",
            "- Found new best loss: 0.0975\n",
            "- Found new best precision: 0.0000\n",
            "Epoch run in 287.99 minutes\n",
            "Epoch 2/2. Learning rate = 0.001.\n",
            "--- Train batch 0 / 1501: batch loss = 0.0458316 ;  average loss = 0.0458316 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 2.57 seconds\n",
            "--- Train batch 10 / 1501: batch loss = 0.0634337 ;  average loss = 0.0808625 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.64 seconds\n",
            "--- Train batch 20 / 1501: batch loss = 0.0677293 ;  average loss = 0.0686538 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.69 seconds\n",
            "--- Train batch 30 / 1501: batch loss = 0.0274404 ;  average loss = 0.0653012 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.79 seconds\n",
            "--- Train batch 40 / 1501: batch loss = 0.0995603 ;  average loss = 0.0674673 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.79 seconds\n",
            "--- Train batch 50 / 1501: batch loss = 0.0979668 ;  average loss = 0.0679526 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.68 seconds\n",
            "--- Train batch 60 / 1501: batch loss = 0.1066712 ;  average loss = 0.0718210 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.71 seconds\n",
            "--- Train batch 70 / 1501: batch loss = 0.0787226 ;  average loss = 0.0726788 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.69 seconds\n",
            "--- Train batch 80 / 1501: batch loss = 0.0871947 ;  average loss = 0.0718840 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.76 seconds\n",
            "--- Train batch 90 / 1501: batch loss = 0.0367293 ;  average loss = 0.0703743 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.78 seconds\n",
            "--- Train batch 100 / 1501: batch loss = 0.0809293 ;  average loss = 0.0705121 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.75 seconds\n",
            "--- Train batch 110 / 1501: batch loss = 0.0822098 ;  average loss = 0.0703680 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.69 seconds\n",
            "--- Train batch 120 / 1501: batch loss = 0.0418937 ;  average loss = 0.0696616 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.85 seconds\n",
            "--- Train batch 130 / 1501: batch loss = 0.1943678 ;  average loss = 0.0701232 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.69 seconds\n",
            "--- Train batch 140 / 1501: batch loss = 0.0863812 ;  average loss = 0.0706610 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.74 seconds\n",
            "--- Train batch 150 / 1501: batch loss = 0.0989693 ;  average loss = 0.0714758 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.82 seconds\n",
            "--- Train batch 160 / 1501: batch loss = 0.0706206 ;  average loss = 0.0714154 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.73 seconds\n",
            "--- Train batch 170 / 1501: batch loss = 0.0295343 ;  average loss = 0.0717049 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.73 seconds\n",
            "--- Train batch 180 / 1501: batch loss = 0.0680947 ;  average loss = 0.0716279 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.72 seconds\n",
            "--- Train batch 190 / 1501: batch loss = 0.0730695 ;  average loss = 0.0717532 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.83 seconds\n",
            "--- Train batch 200 / 1501: batch loss = 0.0806623 ;  average loss = 0.0718139 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.72 seconds\n",
            "--- Train batch 210 / 1501: batch loss = 0.1235286 ;  average loss = 0.0717938 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.70 seconds\n",
            "--- Train batch 220 / 1501: batch loss = 0.0548031 ;  average loss = 0.0715267 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.83 seconds\n",
            "--- Train batch 230 / 1501: batch loss = 0.1121384 ;  average loss = 0.0708291 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.81 seconds\n",
            "--- Train batch 240 / 1501: batch loss = 0.0839781 ;  average loss = 0.0706913 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.83 seconds\n",
            "--- Train batch 250 / 1501: batch loss = 0.0820416 ;  average loss = 0.0707593 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.83 seconds\n",
            "--- Train batch 260 / 1501: batch loss = 0.0391830 ;  average loss = 0.0700257 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.78 seconds\n",
            "--- Train batch 270 / 1501: batch loss = 0.0939185 ;  average loss = 0.0696211 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.77 seconds\n",
            "--- Train batch 280 / 1501: batch loss = 0.0652360 ;  average loss = 0.0692024 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.90 seconds\n",
            "--- Train batch 290 / 1501: batch loss = 0.0777965 ;  average loss = 0.0691066 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.91 seconds\n",
            "--- Train batch 300 / 1501: batch loss = 0.0457661 ;  average loss = 0.0686726 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.86 seconds\n",
            "--- Train batch 310 / 1501: batch loss = 0.0796529 ;  average loss = 0.0689081 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.84 seconds\n",
            "--- Train batch 320 / 1501: batch loss = 0.0707409 ;  average loss = 0.0687095 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.90 seconds\n",
            "--- Train batch 330 / 1501: batch loss = 0.0398512 ;  average loss = 0.0690443 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.92 seconds\n",
            "--- Train batch 340 / 1501: batch loss = 0.0364780 ;  average loss = 0.0687015 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.96 seconds\n",
            "--- Train batch 350 / 1501: batch loss = 0.0827413 ;  average loss = 0.0691439 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.87 seconds\n",
            "--- Train batch 360 / 1501: batch loss = 0.1348553 ;  average loss = 0.0693452 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.90 seconds\n",
            "--- Train batch 370 / 1501: batch loss = 0.0990009 ;  average loss = 0.0696712 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.91 seconds\n",
            "--- Train batch 380 / 1501: batch loss = 0.0544812 ;  average loss = 0.0699517 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.97 seconds\n",
            "--- Train batch 390 / 1501: batch loss = 0.0603733 ;  average loss = 0.0695134 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.00 seconds\n",
            "--- Train batch 400 / 1501: batch loss = 0.0254216 ;  average loss = 0.0695851 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.93 seconds\n",
            "--- Train batch 410 / 1501: batch loss = 0.0521339 ;  average loss = 0.0693464 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.85 seconds\n",
            "--- Train batch 420 / 1501: batch loss = 0.0659818 ;  average loss = 0.0691041 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.91 seconds\n",
            "--- Train batch 430 / 1501: batch loss = 0.0682368 ;  average loss = 0.0692428 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.93 seconds\n",
            "--- Train batch 440 / 1501: batch loss = 0.0329227 ;  average loss = 0.0691227 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.88 seconds\n",
            "--- Train batch 450 / 1501: batch loss = 0.0463724 ;  average loss = 0.0689569 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.95 seconds\n",
            "--- Train batch 460 / 1501: batch loss = 0.0844752 ;  average loss = 0.0687820 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.01 seconds\n",
            "--- Train batch 470 / 1501: batch loss = 0.1001498 ;  average loss = 0.0691353 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.92 seconds\n",
            "--- Train batch 480 / 1501: batch loss = 0.0556515 ;  average loss = 0.0689927 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.01 seconds\n",
            "--- Train batch 490 / 1501: batch loss = 0.1179097 ;  average loss = 0.0688797 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.91 seconds\n",
            "--- Train batch 500 / 1501: batch loss = 0.0923554 ;  average loss = 0.0689925 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.95 seconds\n",
            "--- Train batch 510 / 1501: batch loss = 0.1139518 ;  average loss = 0.0691775 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.98 seconds\n",
            "--- Train batch 520 / 1501: batch loss = 0.0736988 ;  average loss = 0.0692166 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.02 seconds\n",
            "--- Train batch 530 / 1501: batch loss = 0.0598885 ;  average loss = 0.0692209 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.03 seconds\n",
            "--- Train batch 540 / 1501: batch loss = 0.0311339 ;  average loss = 0.0691883 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.02 seconds\n",
            "--- Train batch 550 / 1501: batch loss = 0.0686390 ;  average loss = 0.0692829 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.01 seconds\n",
            "--- Train batch 560 / 1501: batch loss = 0.0639362 ;  average loss = 0.0694660 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.10 seconds\n",
            "--- Train batch 570 / 1501: batch loss = 0.0803153 ;  average loss = 0.0694941 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.01 seconds\n",
            "--- Train batch 580 / 1501: batch loss = 0.1446251 ;  average loss = 0.0696315 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.87 seconds\n",
            "--- Train batch 590 / 1501: batch loss = 0.1186055 ;  average loss = 0.0695171 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.79 seconds\n",
            "--- Train batch 600 / 1501: batch loss = 0.0783500 ;  average loss = 0.0696422 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.83 seconds\n",
            "--- Train batch 610 / 1501: batch loss = 0.0758097 ;  average loss = 0.0696558 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.95 seconds\n",
            "--- Train batch 620 / 1501: batch loss = 0.0495281 ;  average loss = 0.0696397 ;  batch precision = 0.0937500 ;  \n",
            "    10 batches processed in 23.97 seconds\n",
            "--- Train batch 630 / 1501: batch loss = 0.0415748 ;  average loss = 0.0694784 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.99 seconds\n",
            "--- Train batch 640 / 1501: batch loss = 0.1761893 ;  average loss = 0.0696140 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.02 seconds\n",
            "--- Train batch 650 / 1501: batch loss = 0.0365137 ;  average loss = 0.0694249 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.97 seconds\n",
            "--- Train batch 660 / 1501: batch loss = 0.0485517 ;  average loss = 0.0693840 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.91 seconds\n",
            "--- Train batch 670 / 1501: batch loss = 0.0526473 ;  average loss = 0.0696717 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.98 seconds\n",
            "--- Train batch 680 / 1501: batch loss = 0.0331554 ;  average loss = 0.0694714 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.06 seconds\n",
            "--- Train batch 690 / 1501: batch loss = 0.0477398 ;  average loss = 0.0694281 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.98 seconds\n",
            "--- Train batch 700 / 1501: batch loss = 0.1323387 ;  average loss = 0.0695141 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.99 seconds\n",
            "--- Train batch 710 / 1501: batch loss = 0.1281901 ;  average loss = 0.0694321 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.94 seconds\n",
            "--- Train batch 720 / 1501: batch loss = 0.0703121 ;  average loss = 0.0694942 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.02 seconds\n",
            "--- Train batch 730 / 1501: batch loss = 0.0618816 ;  average loss = 0.0692166 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.96 seconds\n",
            "--- Train batch 740 / 1501: batch loss = 0.0931915 ;  average loss = 0.0692885 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.97 seconds\n",
            "--- Train batch 750 / 1501: batch loss = 0.0471957 ;  average loss = 0.0697904 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.09 seconds\n",
            "--- Train batch 760 / 1501: batch loss = 0.0451351 ;  average loss = 0.0697571 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.96 seconds\n",
            "--- Train batch 770 / 1501: batch loss = 0.1693482 ;  average loss = 0.0698987 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.12 seconds\n",
            "--- Train batch 780 / 1501: batch loss = 0.0622406 ;  average loss = 0.0698006 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.04 seconds\n",
            "--- Train batch 790 / 1501: batch loss = 0.0798722 ;  average loss = 0.0697312 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.07 seconds\n",
            "--- Train batch 800 / 1501: batch loss = 0.0600791 ;  average loss = 0.0695718 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.08 seconds\n",
            "--- Train batch 810 / 1501: batch loss = 0.0419708 ;  average loss = 0.0695381 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.14 seconds\n",
            "--- Train batch 820 / 1501: batch loss = 0.0873527 ;  average loss = 0.0695302 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.03 seconds\n",
            "--- Train batch 830 / 1501: batch loss = 0.0406073 ;  average loss = 0.0693361 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.04 seconds\n",
            "--- Train batch 840 / 1501: batch loss = 0.1413487 ;  average loss = 0.0694531 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.09 seconds\n",
            "--- Train batch 850 / 1501: batch loss = 0.0865422 ;  average loss = 0.0694831 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.15 seconds\n",
            "--- Train batch 860 / 1501: batch loss = 0.0508126 ;  average loss = 0.0693960 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.18 seconds\n",
            "--- Train batch 870 / 1501: batch loss = 0.0615734 ;  average loss = 0.0695237 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.15 seconds\n",
            "--- Train batch 880 / 1501: batch loss = 0.0572786 ;  average loss = 0.0694801 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.07 seconds\n",
            "--- Train batch 890 / 1501: batch loss = 0.0779064 ;  average loss = 0.0693775 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.10 seconds\n",
            "--- Train batch 900 / 1501: batch loss = 0.0472590 ;  average loss = 0.0693226 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.07 seconds\n",
            "--- Train batch 910 / 1501: batch loss = 0.0782533 ;  average loss = 0.0693612 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.11 seconds\n",
            "--- Train batch 920 / 1501: batch loss = 0.0417975 ;  average loss = 0.0691782 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.08 seconds\n",
            "--- Train batch 930 / 1501: batch loss = 0.0859996 ;  average loss = 0.0691627 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.01 seconds\n",
            "--- Train batch 940 / 1501: batch loss = 0.0833589 ;  average loss = 0.0690710 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.16 seconds\n",
            "--- Train batch 950 / 1501: batch loss = 0.1123523 ;  average loss = 0.0689494 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.16 seconds\n",
            "--- Train batch 960 / 1501: batch loss = 0.0709721 ;  average loss = 0.0689113 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.09 seconds\n",
            "--- Train batch 970 / 1501: batch loss = 0.0381659 ;  average loss = 0.0687521 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.02 seconds\n",
            "--- Train batch 980 / 1501: batch loss = 0.0721321 ;  average loss = 0.0685584 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.04 seconds\n",
            "--- Train batch 990 / 1501: batch loss = 0.1834983 ;  average loss = 0.0686010 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.01 seconds\n",
            "--- Train batch 1000 / 1501: batch loss = 0.0444966 ;  average loss = 0.0685739 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.02 seconds\n",
            "--- Train batch 1010 / 1501: batch loss = 0.0491939 ;  average loss = 0.0684864 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.00 seconds\n",
            "--- Train batch 1020 / 1501: batch loss = 0.1064851 ;  average loss = 0.0684868 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.95 seconds\n",
            "--- Train batch 1030 / 1501: batch loss = 0.0358321 ;  average loss = 0.0685052 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.95 seconds\n",
            "--- Train batch 1040 / 1501: batch loss = 0.1017029 ;  average loss = 0.0684394 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.92 seconds\n",
            "--- Train batch 1050 / 1501: batch loss = 0.1141423 ;  average loss = 0.0682950 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.80 seconds\n",
            "--- Train batch 1060 / 1501: batch loss = 0.0628984 ;  average loss = 0.0680903 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.88 seconds\n",
            "--- Train batch 1070 / 1501: batch loss = 0.0305193 ;  average loss = 0.0681544 ;  batch precision = 00nan ;  \n",
            "    10 batches processed in 23.81 seconds\n",
            "--- Train batch 1080 / 1501: batch loss = 0.1534319 ;  average loss = 0.0681129 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.88 seconds\n",
            "--- Train batch 1090 / 1501: batch loss = 0.1660762 ;  average loss = 0.0682206 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.92 seconds\n",
            "--- Train batch 1100 / 1501: batch loss = 0.0373988 ;  average loss = 0.0681027 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.05 seconds\n",
            "--- Train batch 1110 / 1501: batch loss = 0.0631795 ;  average loss = 0.0679685 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.86 seconds\n",
            "--- Train batch 1120 / 1501: batch loss = 0.0501983 ;  average loss = 0.0679767 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.89 seconds\n",
            "--- Train batch 1130 / 1501: batch loss = 0.0755357 ;  average loss = 0.0679665 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.09 seconds\n",
            "--- Train batch 1140 / 1501: batch loss = 0.0296499 ;  average loss = 0.0679923 ;  batch precision = 00nan ;  \n",
            "    10 batches processed in 24.11 seconds\n",
            "--- Train batch 1150 / 1501: batch loss = 0.1446820 ;  average loss = 0.0681543 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.00 seconds\n",
            "--- Train batch 1160 / 1501: batch loss = 0.0548943 ;  average loss = 0.0681419 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 24.02 seconds\n",
            "--- Train batch 1170 / 1501: batch loss = 0.0964296 ;  average loss = 0.0680424 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.95 seconds\n",
            "--- Train batch 1180 / 1501: batch loss = 0.0485682 ;  average loss = 0.0680499 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.93 seconds\n",
            "--- Train batch 1190 / 1501: batch loss = 0.1106526 ;  average loss = 0.0680684 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.90 seconds\n",
            "--- Train batch 1200 / 1501: batch loss = 0.0393889 ;  average loss = 0.0680699 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.87 seconds\n",
            "--- Train batch 1210 / 1501: batch loss = 0.0290919 ;  average loss = 0.0680228 ;  batch precision = 0.6250000 ;  \n",
            "    10 batches processed in 23.94 seconds\n",
            "--- Train batch 1220 / 1501: batch loss = 0.1772813 ;  average loss = 0.0679636 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.89 seconds\n",
            "--- Train batch 1230 / 1501: batch loss = 0.0362763 ;  average loss = 0.0679302 ;  batch precision = 0.0208333 ;  \n",
            "    10 batches processed in 24.00 seconds\n",
            "--- Train batch 1240 / 1501: batch loss = 0.1012455 ;  average loss = 0.0679779 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.98 seconds\n",
            "--- Train batch 1250 / 1501: batch loss = 0.0410631 ;  average loss = 0.0680667 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.99 seconds\n",
            "--- Train batch 1260 / 1501: batch loss = 0.0424358 ;  average loss = 0.0680358 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.94 seconds\n",
            "--- Train batch 1270 / 1501: batch loss = 0.0547818 ;  average loss = 0.0680865 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.97 seconds\n",
            "--- Train batch 1280 / 1501: batch loss = 0.0587414 ;  average loss = 0.0680986 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.98 seconds\n",
            "--- Train batch 1290 / 1501: batch loss = 0.0357146 ;  average loss = 0.0680037 ;  batch precision = 0.0555556 ;  \n",
            "    10 batches processed in 23.94 seconds\n",
            "--- Train batch 1300 / 1501: batch loss = 0.0423626 ;  average loss = 0.0680777 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.92 seconds\n",
            "--- Train batch 1310 / 1501: batch loss = 0.0591113 ;  average loss = 0.0680721 ;  batch precision = 0.0416667 ;  \n",
            "    10 batches processed in 23.89 seconds\n",
            "--- Train batch 1320 / 1501: batch loss = 0.0485297 ;  average loss = 0.0680426 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.94 seconds\n",
            "--- Train batch 1330 / 1501: batch loss = 0.0327461 ;  average loss = 0.0680699 ;  batch precision = 0.1041667 ;  \n",
            "    10 batches processed in 23.93 seconds\n",
            "--- Train batch 1340 / 1501: batch loss = 0.0877929 ;  average loss = 0.0680852 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.99 seconds\n",
            "--- Train batch 1350 / 1501: batch loss = 0.0468139 ;  average loss = 0.0681852 ;  batch precision = 0.0500000 ;  \n",
            "    10 batches processed in 23.94 seconds\n",
            "--- Train batch 1360 / 1501: batch loss = 0.0774027 ;  average loss = 0.0681087 ;  batch precision = 0.0416667 ;  \n",
            "    10 batches processed in 23.91 seconds\n",
            "--- Train batch 1370 / 1501: batch loss = 0.0593153 ;  average loss = 0.0680483 ;  batch precision = 0.0937500 ;  \n",
            "    10 batches processed in 23.94 seconds\n",
            "--- Train batch 1380 / 1501: batch loss = 0.0548200 ;  average loss = 0.0679579 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.96 seconds\n",
            "--- Train batch 1390 / 1501: batch loss = 0.0723752 ;  average loss = 0.0679005 ;  batch precision = 0.0375000 ;  \n",
            "    10 batches processed in 23.88 seconds\n",
            "--- Train batch 1400 / 1501: batch loss = 0.0476932 ;  average loss = 0.0678631 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.96 seconds\n",
            "--- Train batch 1410 / 1501: batch loss = 0.0523581 ;  average loss = 0.0677967 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.79 seconds\n",
            "--- Train batch 1420 / 1501: batch loss = 0.0517435 ;  average loss = 0.0677708 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.81 seconds\n",
            "--- Train batch 1430 / 1501: batch loss = 0.0490195 ;  average loss = 0.0677101 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.85 seconds\n",
            "--- Train batch 1440 / 1501: batch loss = 0.1420924 ;  average loss = 0.0677816 ;  batch precision = 0.0125000 ;  \n",
            "    10 batches processed in 23.82 seconds\n",
            "--- Train batch 1450 / 1501: batch loss = 0.1167204 ;  average loss = 0.0677863 ;  batch precision = 0.0138889 ;  \n",
            "    10 batches processed in 23.81 seconds\n",
            "--- Train batch 1460 / 1501: batch loss = 0.0621358 ;  average loss = 0.0676883 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.71 seconds\n",
            "--- Train batch 1470 / 1501: batch loss = 0.0294750 ;  average loss = 0.0676082 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.73 seconds\n",
            "--- Train batch 1480 / 1501: batch loss = 0.0693148 ;  average loss = 0.0675365 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.76 seconds\n",
            "--- Train batch 1490 / 1501: batch loss = 0.1421209 ;  average loss = 0.0675921 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.74 seconds\n",
            "--- Train batch 1500 / 1501: batch loss = 0.0390967 ;  average loss = 0.0675791 ;  batch precision = 0.0000000 ;  \n",
            "    10 batches processed in 23.73 seconds\n",
            "- Train epoch metrics summary: average loss = 0.0675791 ;  \n",
            "--- Validation batch 0 / 167: \n",
            "--- Validation batch 1 / 167: \n",
            "--- Validation batch 2 / 167: \n",
            "--- Validation batch 3 / 167: \n",
            "--- Validation batch 4 / 167: \n",
            "--- Validation batch 5 / 167: \n",
            "--- Validation batch 6 / 167: \n",
            "--- Validation batch 7 / 167: \n",
            "--- Validation batch 8 / 167: \n",
            "--- Validation batch 9 / 167: \n",
            "--- Validation batch 10 / 167: \n",
            "--- Validation batch 11 / 167: \n",
            "--- Validation batch 12 / 167: \n",
            "--- Validation batch 13 / 167: \n",
            "--- Validation batch 14 / 167: \n",
            "--- Validation batch 15 / 167: \n",
            "--- Validation batch 16 / 167: \n",
            "--- Validation batch 17 / 167: \n",
            "--- Validation batch 18 / 167: \n",
            "--- Validation batch 19 / 167: \n",
            "--- Validation batch 20 / 167: \n",
            "--- Validation batch 21 / 167: \n",
            "--- Validation batch 22 / 167: \n",
            "--- Validation batch 23 / 167: \n",
            "--- Validation batch 24 / 167: \n",
            "--- Validation batch 25 / 167: \n",
            "--- Validation batch 26 / 167: \n",
            "--- Validation batch 27 / 167: \n",
            "--- Validation batch 28 / 167: \n",
            "--- Validation batch 29 / 167: \n",
            "--- Validation batch 30 / 167: \n",
            "--- Validation batch 31 / 167: \n",
            "--- Validation batch 32 / 167: \n",
            "--- Validation batch 33 / 167: \n",
            "--- Validation batch 34 / 167: \n",
            "--- Validation batch 35 / 167: \n",
            "--- Validation batch 36 / 167: \n",
            "--- Validation batch 37 / 167: \n",
            "--- Validation batch 38 / 167: \n",
            "--- Validation batch 39 / 167: \n",
            "--- Validation batch 40 / 167: \n",
            "--- Validation batch 41 / 167: \n",
            "--- Validation batch 42 / 167: \n",
            "--- Validation batch 43 / 167: \n",
            "--- Validation batch 44 / 167: \n",
            "--- Validation batch 45 / 167: \n",
            "--- Validation batch 46 / 167: \n",
            "--- Validation batch 47 / 167: \n",
            "--- Validation batch 48 / 167: \n",
            "--- Validation batch 49 / 167: \n",
            "--- Validation batch 50 / 167: \n",
            "--- Validation batch 51 / 167: \n",
            "--- Validation batch 52 / 167: \n",
            "--- Validation batch 53 / 167: \n",
            "--- Validation batch 54 / 167: \n",
            "--- Validation batch 55 / 167: \n",
            "--- Validation batch 56 / 167: \n",
            "--- Validation batch 57 / 167: \n",
            "--- Validation batch 58 / 167: \n",
            "--- Validation batch 59 / 167: \n",
            "--- Validation batch 60 / 167: \n",
            "--- Validation batch 61 / 167: \n",
            "--- Validation batch 62 / 167: \n",
            "--- Validation batch 63 / 167: \n",
            "--- Validation batch 64 / 167: \n",
            "--- Validation batch 65 / 167: \n",
            "--- Validation batch 66 / 167: \n",
            "--- Validation batch 67 / 167: \n",
            "--- Validation batch 68 / 167: \n",
            "--- Validation batch 69 / 167: \n",
            "--- Validation batch 70 / 167: \n",
            "--- Validation batch 71 / 167: \n",
            "--- Validation batch 72 / 167: \n",
            "--- Validation batch 73 / 167: \n",
            "--- Validation batch 74 / 167: \n",
            "--- Validation batch 75 / 167: \n",
            "--- Validation batch 76 / 167: \n",
            "--- Validation batch 77 / 167: \n",
            "--- Validation batch 78 / 167: \n",
            "--- Validation batch 79 / 167: \n",
            "--- Validation batch 80 / 167: \n",
            "--- Validation batch 81 / 167: \n",
            "--- Validation batch 82 / 167: \n",
            "--- Validation batch 83 / 167: \n",
            "--- Validation batch 84 / 167: \n",
            "--- Validation batch 85 / 167: \n",
            "--- Validation batch 86 / 167: \n",
            "--- Validation batch 87 / 167: \n",
            "--- Validation batch 88 / 167: \n",
            "--- Validation batch 89 / 167: \n",
            "--- Validation batch 90 / 167: \n",
            "--- Validation batch 91 / 167: \n",
            "--- Validation batch 92 / 167: \n",
            "--- Validation batch 93 / 167: \n",
            "--- Validation batch 94 / 167: \n",
            "--- Validation batch 95 / 167: \n",
            "--- Validation batch 96 / 167: \n",
            "--- Validation batch 97 / 167: \n",
            "--- Validation batch 98 / 167: \n",
            "--- Validation batch 99 / 167: \n",
            "--- Validation batch 100 / 167: \n",
            "--- Validation batch 101 / 167: \n",
            "--- Validation batch 102 / 167: \n",
            "--- Validation batch 103 / 167: \n",
            "--- Validation batch 104 / 167: \n",
            "--- Validation batch 105 / 167: \n",
            "--- Validation batch 106 / 167: \n",
            "--- Validation batch 107 / 167: \n",
            "--- Validation batch 108 / 167: \n",
            "--- Validation batch 109 / 167: \n",
            "--- Validation batch 110 / 167: \n",
            "--- Validation batch 111 / 167: \n",
            "--- Validation batch 112 / 167: \n",
            "--- Validation batch 113 / 167: \n",
            "--- Validation batch 114 / 167: \n",
            "--- Validation batch 115 / 167: \n",
            "--- Validation batch 116 / 167: \n",
            "--- Validation batch 117 / 167: \n",
            "--- Validation batch 118 / 167: \n",
            "--- Validation batch 119 / 167: \n",
            "--- Validation batch 120 / 167: \n",
            "--- Validation batch 121 / 167: \n",
            "--- Validation batch 122 / 167: \n",
            "--- Validation batch 123 / 167: \n",
            "--- Validation batch 124 / 167: \n",
            "--- Validation batch 125 / 167: \n",
            "--- Validation batch 126 / 167: \n",
            "--- Validation batch 127 / 167: \n",
            "--- Validation batch 128 / 167: \n",
            "--- Validation batch 129 / 167: \n",
            "--- Validation batch 130 / 167: \n",
            "--- Validation batch 131 / 167: \n",
            "--- Validation batch 132 / 167: \n",
            "--- Validation batch 133 / 167: \n",
            "--- Validation batch 134 / 167: \n",
            "--- Validation batch 135 / 167: \n",
            "--- Validation batch 136 / 167: \n",
            "--- Validation batch 137 / 167: \n",
            "--- Validation batch 138 / 167: \n",
            "--- Validation batch 139 / 167: \n",
            "--- Validation batch 140 / 167: \n",
            "--- Validation batch 141 / 167: \n",
            "--- Validation batch 142 / 167: \n",
            "--- Validation batch 143 / 167: \n",
            "--- Validation batch 144 / 167: \n",
            "--- Validation batch 145 / 167: \n",
            "--- Validation batch 146 / 167: \n",
            "--- Validation batch 147 / 167: \n",
            "--- Validation batch 148 / 167: \n",
            "--- Validation batch 149 / 167: \n",
            "--- Validation batch 150 / 167: \n",
            "--- Validation batch 151 / 167: \n",
            "--- Validation batch 152 / 167: \n",
            "--- Validation batch 153 / 167: \n",
            "--- Validation batch 154 / 167: \n",
            "--- Validation batch 155 / 167: \n",
            "--- Validation batch 156 / 167: \n",
            "--- Validation batch 157 / 167: \n",
            "--- Validation batch 158 / 167: \n",
            "--- Validation batch 159 / 167: \n",
            "--- Validation batch 160 / 167: \n",
            "--- Validation batch 161 / 167: \n",
            "--- Validation batch 162 / 167: \n",
            "--- Validation batch 163 / 167: \n",
            "--- Validation batch 164 / 167: \n",
            "--- Validation batch 165 / 167: \n",
            "--- Validation batch 166 / 167: \n",
            "- Eval metrics : average loss = 0.0878704 ;  \n",
            "  Evaluation run in 192.90 seconds.\n",
            "- Found new best loss: 0.0879\n",
            "- Found new best precision: 0.0526\n",
            "Epoch run in 63.14 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e0a7bcf8f26561d276caa4ee7687e04c4ec0461a",
        "trusted": true,
        "id": "EtJWDgig21zN"
      },
      "source": [
        "debug = False\n",
        "\n",
        "model = UNET().cuda() if gpu_available else model1()\n",
        "loss_fn = BCEWithLogitsLoss2d().cuda() if gpu_available else BCEWithLogitsLoss2d()\n",
        "lr_init = 0.5\n",
        "num_epochs = 2\n",
        "num_steps_train = len(loader_train)\n",
        "num_steps_eval = len(loader_valid)\n",
        "shape = int(round(original_image_shape / rescale_factor))\n",
        "\n",
        "print(\"Starting training for {} epochs\".format(num_epochs))\n",
        "histories, best_models = train_and_evaluate(model, loader_train, loader_valid, lr_init, loss_fn, \n",
        "                                            num_epochs, num_steps_train, num_steps_eval, pId_boxes_dict, rescale_factor, shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "27b584a443adff0f3c8a47792bba5c5756cd3b4b",
        "trusted": true,
        "id": "OXSY7Wpy21zO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e9f79502-6d26-4137-eda1-3147dd366f85"
      },
      "source": [
        "\n",
        "plt.plot(range(len(histories['loss train'])), histories['loss train'], color='k', label='loss train')\n",
        "plt.plot(range(len(histories['loss avg train'])), histories['loss avg train'], color='g', ls='dashed', label='loss avg train')\n",
        "plt.plot(range(len(histories['loss validation'])), histories['loss validation'], color='r', label='loss validation')\n",
        "plt.legend()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fed59fe1090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "be45f54706df3e60ee3e652f73758c214bf48e4a",
        "trusted": true,
        "id": "XK-uxTwh21zO"
      },
      "source": [
        "# plt.plot(range(len(histories['precision train'])), histories['precision train'], color='k', label='precision train')\n",
        "# plt.plot(range(len(histories['precision validation'])), histories['precision validation'], color='r', label='precision validation')\n",
        "# plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7800bc360cd836bd95464d45214ddfc245c37f38",
        "trusted": true,
        "id": "e-OM0LB721zO"
      },
      "source": [
        "# pick model with best precision\n",
        "best_model = best_models['best precision model']"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTbb-eK-MMOT"
      },
      "source": [
        "# checkpoint = torch.load('/content/drive/MyDrive/Colab_Notebooks/project/data/save/loss.best.pth.tar')\n",
        "# best_model = model1().cuda() if gpu_available else model1()\n",
        "# loss_fn = BCEWithLogitsLoss2d().cuda() if gpu_available else BCEWithLogitsLoss2d()\n",
        "# lr_init = 0.002\n",
        "# best_model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e43d8eaf21f7ff546e2102e111f9d4586a1464a2",
        "trusted": true,
        "id": "Y4RI2p4W21zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60994905-d4ad-46a8-8493-183bc49ec2f2"
      },
      "source": [
        "dataset_valid = PneumoniaDataset(root=datapath_orig, subset='train', pIds=pIds_valid, predict=True, \n",
        "                                 boxes=None, rescale_factor=rescale_factor, transform=transform)\n",
        "loader_valid = DataLoader(dataset=dataset_valid,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False) \n",
        "\n",
        "predictions_valid = predict(best_model, loader_valid)\n",
        "print('Predicted {} validation images.'.format(len(predictions_valid)))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting batch 1 / 167.\n",
            "Predicting batch 2 / 167.\n",
            "Predicting batch 3 / 167.\n",
            "Predicting batch 4 / 167.\n",
            "Predicting batch 5 / 167.\n",
            "Predicting batch 6 / 167.\n",
            "Predicting batch 7 / 167.\n",
            "Predicting batch 8 / 167.\n",
            "Predicting batch 9 / 167.\n",
            "Predicting batch 10 / 167.\n",
            "Predicting batch 11 / 167.\n",
            "Predicting batch 12 / 167.\n",
            "Predicting batch 13 / 167.\n",
            "Predicting batch 14 / 167.\n",
            "Predicting batch 15 / 167.\n",
            "Predicting batch 16 / 167.\n",
            "Predicting batch 17 / 167.\n",
            "Predicting batch 18 / 167.\n",
            "Predicting batch 19 / 167.\n",
            "Predicting batch 20 / 167.\n",
            "Predicting batch 21 / 167.\n",
            "Predicting batch 22 / 167.\n",
            "Predicting batch 23 / 167.\n",
            "Predicting batch 24 / 167.\n",
            "Predicting batch 25 / 167.\n",
            "Predicting batch 26 / 167.\n",
            "Predicting batch 27 / 167.\n",
            "Predicting batch 28 / 167.\n",
            "Predicting batch 29 / 167.\n",
            "Predicting batch 30 / 167.\n",
            "Predicting batch 31 / 167.\n",
            "Predicting batch 32 / 167.\n",
            "Predicting batch 33 / 167.\n",
            "Predicting batch 34 / 167.\n",
            "Predicting batch 35 / 167.\n",
            "Predicting batch 36 / 167.\n",
            "Predicting batch 37 / 167.\n",
            "Predicting batch 38 / 167.\n",
            "Predicting batch 39 / 167.\n",
            "Predicting batch 40 / 167.\n",
            "Predicting batch 41 / 167.\n",
            "Predicting batch 42 / 167.\n",
            "Predicting batch 43 / 167.\n",
            "Predicting batch 44 / 167.\n",
            "Predicting batch 45 / 167.\n",
            "Predicting batch 46 / 167.\n",
            "Predicting batch 47 / 167.\n",
            "Predicting batch 48 / 167.\n",
            "Predicting batch 49 / 167.\n",
            "Predicting batch 50 / 167.\n",
            "Predicting batch 51 / 167.\n",
            "Predicting batch 52 / 167.\n",
            "Predicting batch 53 / 167.\n",
            "Predicting batch 54 / 167.\n",
            "Predicting batch 55 / 167.\n",
            "Predicting batch 56 / 167.\n",
            "Predicting batch 57 / 167.\n",
            "Predicting batch 58 / 167.\n",
            "Predicting batch 59 / 167.\n",
            "Predicting batch 60 / 167.\n",
            "Predicting batch 61 / 167.\n",
            "Predicting batch 62 / 167.\n",
            "Predicting batch 63 / 167.\n",
            "Predicting batch 64 / 167.\n",
            "Predicting batch 65 / 167.\n",
            "Predicting batch 66 / 167.\n",
            "Predicting batch 67 / 167.\n",
            "Predicting batch 68 / 167.\n",
            "Predicting batch 69 / 167.\n",
            "Predicting batch 70 / 167.\n",
            "Predicting batch 71 / 167.\n",
            "Predicting batch 72 / 167.\n",
            "Predicting batch 73 / 167.\n",
            "Predicting batch 74 / 167.\n",
            "Predicting batch 75 / 167.\n",
            "Predicting batch 76 / 167.\n",
            "Predicting batch 77 / 167.\n",
            "Predicting batch 78 / 167.\n",
            "Predicting batch 79 / 167.\n",
            "Predicting batch 80 / 167.\n",
            "Predicting batch 81 / 167.\n",
            "Predicting batch 82 / 167.\n",
            "Predicting batch 83 / 167.\n",
            "Predicting batch 84 / 167.\n",
            "Predicting batch 85 / 167.\n",
            "Predicting batch 86 / 167.\n",
            "Predicting batch 87 / 167.\n",
            "Predicting batch 88 / 167.\n",
            "Predicting batch 89 / 167.\n",
            "Predicting batch 90 / 167.\n",
            "Predicting batch 91 / 167.\n",
            "Predicting batch 92 / 167.\n",
            "Predicting batch 93 / 167.\n",
            "Predicting batch 94 / 167.\n",
            "Predicting batch 95 / 167.\n",
            "Predicting batch 96 / 167.\n",
            "Predicting batch 97 / 167.\n",
            "Predicting batch 98 / 167.\n",
            "Predicting batch 99 / 167.\n",
            "Predicting batch 100 / 167.\n",
            "Predicting batch 101 / 167.\n",
            "Predicting batch 102 / 167.\n",
            "Predicting batch 103 / 167.\n",
            "Predicting batch 104 / 167.\n",
            "Predicting batch 105 / 167.\n",
            "Predicting batch 106 / 167.\n",
            "Predicting batch 107 / 167.\n",
            "Predicting batch 108 / 167.\n",
            "Predicting batch 109 / 167.\n",
            "Predicting batch 110 / 167.\n",
            "Predicting batch 111 / 167.\n",
            "Predicting batch 112 / 167.\n",
            "Predicting batch 113 / 167.\n",
            "Predicting batch 114 / 167.\n",
            "Predicting batch 115 / 167.\n",
            "Predicting batch 116 / 167.\n",
            "Predicting batch 117 / 167.\n",
            "Predicting batch 118 / 167.\n",
            "Predicting batch 119 / 167.\n",
            "Predicting batch 120 / 167.\n",
            "Predicting batch 121 / 167.\n",
            "Predicting batch 122 / 167.\n",
            "Predicting batch 123 / 167.\n",
            "Predicting batch 124 / 167.\n",
            "Predicting batch 125 / 167.\n",
            "Predicting batch 126 / 167.\n",
            "Predicting batch 127 / 167.\n",
            "Predicting batch 128 / 167.\n",
            "Predicting batch 129 / 167.\n",
            "Predicting batch 130 / 167.\n",
            "Predicting batch 131 / 167.\n",
            "Predicting batch 132 / 167.\n",
            "Predicting batch 133 / 167.\n",
            "Predicting batch 134 / 167.\n",
            "Predicting batch 135 / 167.\n",
            "Predicting batch 136 / 167.\n",
            "Predicting batch 137 / 167.\n",
            "Predicting batch 138 / 167.\n",
            "Predicting batch 139 / 167.\n",
            "Predicting batch 140 / 167.\n",
            "Predicting batch 141 / 167.\n",
            "Predicting batch 142 / 167.\n",
            "Predicting batch 143 / 167.\n",
            "Predicting batch 144 / 167.\n",
            "Predicting batch 145 / 167.\n",
            "Predicting batch 146 / 167.\n",
            "Predicting batch 147 / 167.\n",
            "Predicting batch 148 / 167.\n",
            "Predicting batch 149 / 167.\n",
            "Predicting batch 150 / 167.\n",
            "Predicting batch 151 / 167.\n",
            "Predicting batch 152 / 167.\n",
            "Predicting batch 153 / 167.\n",
            "Predicting batch 154 / 167.\n",
            "Predicting batch 155 / 167.\n",
            "Predicting batch 156 / 167.\n",
            "Predicting batch 157 / 167.\n",
            "Predicting batch 158 / 167.\n",
            "Predicting batch 159 / 167.\n",
            "Predicting batch 160 / 167.\n",
            "Predicting batch 161 / 167.\n",
            "Predicting batch 162 / 167.\n",
            "Predicting batch 163 / 167.\n",
            "Predicting batch 164 / 167.\n",
            "Predicting batch 165 / 167.\n",
            "Predicting batch 166 / 167.\n",
            "Predicting batch 167 / 167.\n",
            "Predicted 2668 validation images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "71801746920ea5effb8d7d94f5921b97569b45f7",
        "trusted": true,
        "id": "wZo3RK6C21zP"
      },
      "source": [
        "def rescale_box_coordinates(box, rescale_factor):\n",
        "    x, y, w, h = box\n",
        "    x = int(round(x/rescale_factor))\n",
        "    y = int(round(y/rescale_factor))\n",
        "    w = int(round(w/rescale_factor))\n",
        "    h = int(round(h/rescale_factor))\n",
        "    return [x, y, w, h]    "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "54b741a09c36add0d71e73481fcca7ca0c06b820",
        "trusted": true,
        "id": "KA1-5YY021zP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "ea8a6d95-4582-41b4-e527-c4c66657300f"
      },
      "source": [
        "def draw_boxes(predicted_boxes, confidences, target_boxes, ax, angle=0):\n",
        "    if len(predicted_boxes)>0 and len(target_boxes)>0:\n",
        "      for box_p, box_t, c in zip(predicted_boxes,target_boxes, confidences):\n",
        "\n",
        "            xp, yp, wp, hp = box_p \n",
        "            patch_p = Rectangle((xp,yp), wp, hp, color='red', ls='dashed',\n",
        "                              angle=angle, fill=False, lw=4, joinstyle='round', alpha=0.6)\n",
        "            ax.add_patch(patch_p)\n",
        "            ax.text(xp+wp/2., yp-5, 'Confidence：{:.2}'.format(c), color='red', size=8, va='center', ha='center')\n",
        "\n",
        "            xt, yt, wt, ht = box_t\n",
        "            patch_t = Rectangle((xt,yt), wt, ht, color='red',  \n",
        "                              angle=angle, fill=False, lw=4, joinstyle='round', alpha=0.6)\n",
        "            ax.add_patch(patch_t)\n",
        "            box_p_msk = box_mask(box_p, shape=shape)\n",
        "            box_t_msk = box_mask(box_t, shape=shape)\n",
        "            iou = IoU(box_p_msk, box_t_msk)\n",
        "\n",
        "            ax.text(xp+wp/2., yp-20, 'IoU：{:.2}'.format(iou), color='red', size=8, va='center', ha='center')\n",
        "      return ax\n",
        "    if len(predicted_boxes)>0:\n",
        "        for box, c in zip(predicted_boxes, confidences):\n",
        "\n",
        "            x, y, w, h = box \n",
        "            patch = Rectangle((x,y), w, h, color='red', ls='dashed',\n",
        "                              angle=angle, fill=False, lw=4, joinstyle='round', alpha=0.6)\n",
        "            ax.add_patch(patch)\n",
        "\n",
        "            ax.text(x+w/2., y-5, 'Confidence：{:.2}'.format(c), color='red', size=20, va='center', ha='center')\n",
        "            ax.text(x+w/2., y-5, 'Confidence：{:.2}'.format(c), color='red', size=20, va='center', ha='center')\n",
        "    if len(target_boxes)>0:\n",
        "        for box in target_boxes:\n",
        "            x, y, w, h = box\n",
        "            patch = Rectangle((x,y), w, h, color='red',  \n",
        "                              angle=angle, fill=False, lw=4, joinstyle='round', alpha=0.6)\n",
        "            ax.add_patch(patch)\n",
        "    \n",
        "    return ax"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-6e9cc0f88a94>\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    return ax\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b1df63cc6f3fc4d62db21006d7e50693dfe0d67c",
        "trusted": false,
        "id": "JhcxiFRB21zP"
      },
      "source": [
        "best_threshold = None\n",
        "best_avg_precision_valid = 0.0\n",
        "thresholds = np.arange(0, 0.6, 0.01)\n",
        "avg_precision_valids = []\n",
        "for threshold in thresholds:\n",
        "    precision_valid = []\n",
        "    for i in range(len(dataset_valid)):\n",
        "        img, pId = dataset_valid[i]\n",
        "        target_boxes = [rescale_box_coordinates(box, rescale_factor) for box in pId_boxes_dict[pId]] if pId in pId_boxes_dict else []\n",
        "        prediction = predictions_valid[pId]\n",
        "        predicted_boxes, confidences = mask_boxes(prediction, threshold=threshold, connectivity=None)\n",
        "        avg_precision_img = average_precision_image(predicted_boxes, confidences, target_boxes, shape=img[0].shape[0])\n",
        "        precision_valid.append(avg_precision_img)\n",
        "    avg_precision_valid = np.nanmean(precision_valid)\n",
        "    avg_precision_valids.append(avg_precision_valid)\n",
        "    print('Threshold: {}, average precision validation: {:03.5f}'.format(threshold, avg_precision_valid))\n",
        "    if avg_precision_valid>best_avg_precision_valid:\n",
        "        print('Found new best average precision validation!')\n",
        "        best_avg_precision_valid = avg_precision_valid\n",
        "        best_threshold = threshold\n",
        "plt.plot(thresholds, avg_precision_valids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "6e06f1b38cc25b40f3b628fd770e182dd31097e2",
        "id": "IbkQzRPX21zP"
      },
      "source": [
        "for i in range(len(dataset_valid)):\n",
        "    img, pId = dataset_valid[i]\n",
        "    target_boxes = [rescale_box_coordinates(box, rescale_factor) for box in pId_boxes_dict[pId]] if pId in pId_boxes_dict else []\n",
        "    prediction = predictions_valid[pId]\n",
        "    predicted_boxes, confidences = mask_boxes(prediction, threshold=best_threshold, connectivity=None)\n",
        "    avg_precision_img = average_precision_image(predicted_boxes, confidences, target_boxes, shape=img[0].shape[0])\n",
        "    if i%100==0: # print every 100\n",
        "        plt.imshow(img[0], cmap=mpl.cm.gist_gray) # [0] is the channel index (here there's just one channel)\n",
        "        plt.imshow(prediction[0], cmap=mpl.cm.jet, alpha=0.5)\n",
        "        draw_boxes(predicted_boxes, confidences, target_boxes, plt.gca())\n",
        "        print('Prediction mask scale:', prediction[0].min(), '-', prediction[0].max())\n",
        "        print('Prediction string:', prediction_string(predicted_boxes, confidences))\n",
        "        print('Ground truth boxes:', target_boxes)\n",
        "        print('Average precision image: {:05.5f}'.format(avg_precision_img))\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSuC6dMex54z"
      },
      "source": [
        "# def mean_iou(y_true, y_pred):\n",
        "#     '''\n",
        "#     Mean-Intersection-Over-Union\n",
        "#     '''\n",
        "#     y_pred = torch.round(y_pred)\n",
        "#     intersect = (y_true * y_pred).sum(axis=[1, 2, 3])\n",
        "#     union = y_true.sum(axis=[1, 2, 3]) + y_pred.sum(axis=[1, 2, 3])\n",
        "#     smooth = torch.ones(intersect.shape).cuda()\n",
        "#     return ((intersect + smooth) / (union - intersect + smooth)).mean()\n",
        "\n",
        "\n",
        "# for i,(input_batch, labels_batch, pIds_batch) in enumerate(loader_train):\n",
        "#   input_batch = Variable(input_batch).cuda() if gpu_available else Variable(input_batch).float()\n",
        "#   labels_batch = Variable(labels_batch).cuda() if gpu_available else Variable(labels_batch).float()\n",
        "            \n",
        "#   # compute output\n",
        "\n",
        "#   output_batch = model(input_batch)\n",
        "#   iou = mean_iou(labels_batch,output_batch)\n",
        "#   print(iou)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "116af95be7c76068865aebcb1833fc65682ea583",
        "trusted": true,
        "id": "FNIIZIg-21zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a6630b06e78a0646ca232c6b096a416f4a8f1e27",
        "trusted": true,
        "id": "Of5R7LkY21zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7f6e6cffb7aefc82af074d0706d5a4b3b6c92bf8",
        "trusted": true,
        "id": "IC0tL_Ce21zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c83de59cf3d57ab80fb5a8d23f884bf9706cf961",
        "trusted": true,
        "id": "rQf5mfN121zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "865f866c851c9a0917d86813eddbe9aafcbd23e5",
        "trusted": false,
        "id": "n_MFv-fq21zR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}