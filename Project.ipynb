{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzrWq6OX23wO",
        "outputId": "a7402421-5526-4f6c-c19f-7b3350accb89"
      },
      "source": [
        "!pip install efficientnet_pytorch\n",
        "!pip install pydicom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting efficientnet_pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/2e/a0/dd40b50aebf0028054b6b35062948da01123d7be38d08b6b1e5435df6363/efficientnet_pytorch-0.7.1.tar.gz\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-cp37-none-any.whl size=16443 sha256=f1913d8f3f874b796b1657bef3b3372a41dc44ed327faa1c149ab7be6d24f8f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/27/aa/c46d23c4e8cc72d41283862b1437e0b3ad318417e8ed7d5921\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1\n",
            "Collecting pydicom\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/15/df16546bc59bfca390cf072d473fb2c8acd4231636f64356593a63137e55/pydicom-2.1.2-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 4.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-2.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": true,
        "id": "NQ5A2-fS21y6"
      },
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import time\n",
        "\n",
        "import skimage \n",
        "from skimage.transform import resize\n",
        "from skimage.exposure import rescale_intensity\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "from scipy.ndimage.filters import gaussian_filter\n",
        "import PIL\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset as torchDataset\n",
        "import torchvision as tv\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "import shutil\n",
        "\n",
        "import pydicom\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.catch_warnings()\n",
        "\n",
        "gpu_available = True\n",
        "\n",
        "original_image_shape = 1024\n",
        "\n",
        "datapath_orig = '/content/drive/MyDrive/Colab_Notebooks/project/data/'\n",
        "datapath_prep = '/content/drive/MyDrive/Colab_Notebooks/project/data/prep/'\n",
        "datapath_out = '/content/drive/MyDrive/Colab_Notebooks/project/data/save/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROjqCp1H3NXl",
        "outputId": "fb6ec7be-4126-4a8f-982d-70f58072b429"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d24af5228cf386fcadd225e442bd57afcfc9590b",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "xAErfASI21y8",
        "outputId": "19de8246-79f3-4b7e-b053-511b5c9579cd"
      },
      "source": [
        "df_train = pd.read_csv(datapath_prep+'train.csv')\n",
        "# df_train = pd.read_csv(datapath_orig+'stage_2_train_labels')\n",
        "df_test = pd.read_csv(datapath_prep+'test.csv')\n",
        "df_train.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patientId</th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>Target</th>\n",
              "      <th>class</th>\n",
              "      <th>PatientSex</th>\n",
              "      <th>PatientAge</th>\n",
              "      <th>ViewPosition</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0004cfab-14fd-4e49-80ba-63a80b6bddd6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>F</td>\n",
              "      <td>51.0</td>\n",
              "      <td>PA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00313ee0-9eaa-42f4-b0ab-c148ed3241cd</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>F</td>\n",
              "      <td>48.0</td>\n",
              "      <td>PA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00322d4d-1c29-4943-afc9-b6754be640eb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>No Lung Opacity / Not Normal</td>\n",
              "      <td>M</td>\n",
              "      <td>19.0</td>\n",
              "      <td>AP</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                              patientId   x  ...  PatientAge  ViewPosition\n",
              "0  0004cfab-14fd-4e49-80ba-63a80b6bddd6 NaN  ...        51.0            PA\n",
              "1  00313ee0-9eaa-42f4-b0ab-c148ed3241cd NaN  ...        48.0            PA\n",
              "2  00322d4d-1c29-4943-afc9-b6754be640eb NaN  ...        19.0            AP\n",
              "\n",
              "[3 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7a925348d5581f1ab749ba16c99e0273a9cce86f",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "f6I-P8jT21y9",
        "outputId": "5e97fbca-528a-4b32-e11e-ecab6ae5b500"
      },
      "source": [
        "# calculate minimum box area as benchmark for CNN model\n",
        "df_train['box_area'] = df_train['width'] * df_train['height']\n",
        "df_train['box_area'].hist(bins=100)\n",
        "df_train['box_area'].describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count      9555.000000\n",
              "mean      77523.448038\n",
              "std       51807.689206\n",
              "min        2320.000000\n",
              "25%       37535.500000\n",
              "50%       64829.000000\n",
              "75%      106491.500000\n",
              "max      371184.000000\n",
              "Name: box_area, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5692b292478bd2d534a8da94595717465181ef20",
        "trusted": true,
        "id": "gURGrzjj21y-"
      },
      "source": [
        "# arbitrary value for minimum box area in the CNN model\n",
        "min_box_area = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c7687d275d2641b47e05a373326e012b6892e92d",
        "id": "7BfPXWn821y-"
      },
      "source": [
        "The following code prepares the training data in a useful format for the unet model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7fe5579f238cd9ddb2f51647128ff253fc5ccf8a",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EarjNGxP21y_",
        "outputId": "bfe5a9a7-e2ed-41b0-f244-e66be276cec2"
      },
      "source": [
        "# shuffle and create patient ID list, then split into train and validation sets\n",
        "validation_frac = 0.10\n",
        "\n",
        "df_train = df_train.sample(frac=1, random_state=42) # .sample(frac=1) does the shuffling\n",
        "pIds = [pId for pId in df_train['patientId'].unique()]\n",
        "\n",
        "pIds_valid = pIds[ : int(round(validation_frac*len(pIds)))]\n",
        "pIds_train = pIds[int(round(validation_frac*len(pIds))) : ]\n",
        "\n",
        "print('{} patient IDs shuffled and {}% of them used in validation set.'.format(len(pIds), validation_frac*100))\n",
        "print('{} images went into train set and {} images went into validation set.'.format(len(pIds_train), len(pIds_valid)))\n",
        "\n",
        "# get test set patient IDs\n",
        "pIds_test = df_test['patientId'].unique()\n",
        "print('{} patient IDs in test set.'.format(len(pIds_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26684 patient IDs shuffled and 10.0% of them used in validation set.\n",
            "24016 images went into train set and 2668 images went into validation set.\n",
            "3000 patient IDs in test set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0e44aae1c2ee31ebc8b836de4abeb5097f61ece4",
        "trusted": true,
        "id": "JiM4i6lC21y_"
      },
      "source": [
        "def get_boxes_per_patient(df, pId):\n",
        "    boxes = df.loc[df['patientId']==pId][['x', 'y', 'width', 'height']].astype('int').values.tolist()\n",
        "    return boxes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ab8077059d5fe1142bc74171fbb2d7ba48dee932",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgMhedUu21zA",
        "outputId": "73f92d0c-b3ce-4341-e383-ba264b83beb7"
      },
      "source": [
        "# create dictionary of {patientId : list of boxes}\n",
        "pId_boxes_dict = {}\n",
        "for pId in df_train.loc[(df_train['Target']==1)]['patientId'].unique().tolist():\n",
        "    pId_boxes_dict[pId] = get_boxes_per_patient(df_train, pId)\n",
        "print('{} ({:.1f}%) images have target boxes.'.format(len(pId_boxes_dict), 100*(len(pId_boxes_dict)/len(pIds))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6012 (22.5%) images have target boxes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1b49bb60eaa38c96f0d105bfc8d3a6aa5a698bda",
        "trusted": true,
        "id": "wfA67whg21zB"
      },
      "source": [
        "# define a MinMaxScaler function for the images\n",
        "def imgMinMaxScaler(img, scale_range):\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    img = img.astype('float64')\n",
        "    img_std = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
        "    img_scaled = img_std * float(scale_range[1] - scale_range[0]) + float(scale_range[0])\n",
        "    # round at closest integer and transform to integer \n",
        "    img_scaled = np.rint(img_scaled).astype('uint8')\n",
        "\n",
        "    return img_scaled"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "930a8b2aff636355c97a8cc2c773bb2bfa691763",
        "id": "QuK8RREu21zB"
      },
      "source": [
        "# define a \"warping\" image/mask function \n",
        "def elastic_transform(image, alpha, sigma, random_state=None):\n",
        "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
        "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
        "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
        "       Proc. of the International Conference on Document Analysis and\n",
        "       Recognition, 2003.\n",
        "       Code adapted from https://gist.github.com/chsasank/4d8f68caf01f041a6453e67fb30f8f5a\n",
        "    \"\"\"\n",
        "    assert len(image.shape)==2, 'Image must have 2 dimensions.'\n",
        "\n",
        "    if random_state is None:\n",
        "        random_state = np.random.RandomState(None)\n",
        "\n",
        "    shape = image.shape\n",
        "\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
        "\n",
        "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
        "    indices = np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1))\n",
        "    \n",
        "    image_warped = map_coordinates(image, indices, order=1).reshape(shape)\n",
        "    \n",
        "    return image_warped"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "dc0cd3a5487265f5cd28b9abd9c0b329afd5b48b",
        "trusted": true,
        "id": "9iz9nlhk21zC"
      },
      "source": [
        "# define the data generator class\n",
        "class PneumoniaDataset(torchDataset):\n",
        "\n",
        "    def __init__(self, root, subset, pIds, predict, boxes, rescale_factor=1, transform=None, rotation_angle=0, warping=False):\n",
        "\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.subset = subset\n",
        "        self.pIds = pIds\n",
        "        self.predict = predict\n",
        "        self.boxes = boxes\n",
        "        self.rescale_factor = rescale_factor\n",
        "        self.transform = transform\n",
        "        self.rotation_angle = rotation_angle\n",
        "        self.warping = warping\n",
        "\n",
        "        self.data_path = self.root + 'stage_2_'+self.subset+'_images/'\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        pId = self.pIds[index]\n",
        "        # img = pydicom.dcmread(os.path.join(self.data_path, pId+'.dcm'),force=True).pixel_array\n",
        "        img = pydicom.read_file(os.path.join(self.data_path, pId+'.dcm'),force=True).pixel_array\n",
        "        # img.file_meta.TransferSyntaxUID = pydicom.uid.ImplicitVRLittleEndian\n",
        "        # img = img.pixel_array\n",
        "        original_image_shape = img.shape[0]\n",
        "        image_shape = original_image_shape / self.rescale_factor\n",
        "        image_shape = int(image_shape)\n",
        "        # resize image \n",
        "        # IMPORTANT: skimage resize function rescales the output from 0 to 1, and pytorch doesn't like this!\n",
        "        # One solution would be using torchvision rescale function (but need to differentiate img and target transforms)\n",
        "        # Here I use skimage resize and then rescale the output again from 0 to 255\n",
        "        img = resize(img, (image_shape, image_shape), mode='reflect')\n",
        "        img = imgMinMaxScaler(img, (0,255))\n",
        "        if self.warping:\n",
        "            img = elastic_transform(img, image_shape*2., image_shape*0.1)\n",
        "        img = np.expand_dims(img, -1)\n",
        "        if self.rotation_angle>0:\n",
        "            angle = self.rotation_angle * (2 * np.random.random_sample() - 1) # generate random angle \n",
        "            img = tv.transforms.functional.to_pil_image(img)\n",
        "            img = tv.transforms.functional.rotate(img, angle, resample=PIL.Image.BILINEAR)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        \n",
        "        if not self.predict:\n",
        "            target = np.zeros((image_shape, image_shape))\n",
        "            if pId in self.boxes:\n",
        "                for box in self.boxes[pId]:\n",
        "                    x, y, w, h = box\n",
        "                    x = int(round(x/rescale_factor))\n",
        "                    y = int(round(y/rescale_factor))\n",
        "                    w = int(round(w/rescale_factor))\n",
        "                    h = int(round(h/rescale_factor))\n",
        "                    target[y:y+h, x:x+w] = 255 #\n",
        "            target = np.expand_dims(target, -1)   \n",
        "            target = target.astype('uint8')\n",
        "            if self.rotation_angle>0:\n",
        "                target = tv.transforms.functional.to_pil_image(target)\n",
        "                target = tv.transforms.functional.rotate(target, angle, resample=PIL.Image.BILINEAR)\n",
        "\n",
        "            if self.transform is not None:\n",
        "                target = self.transform(target)\n",
        "            return img, target, pId\n",
        "        else: \n",
        "            return img, pId\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pIds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c6b0894fa817c5d72c1cef3af6e78658728d548c",
        "trusted": true,
        "id": "egqjkFni21zD"
      },
      "source": [
        "# manual model parameters\n",
        "rescale_factor = 4 # resize factor to reduce image size (new_image_shape = original_image_shape / rescale_factor)\n",
        "batch_size = 16 # I used 25 on GCP\n",
        "\n",
        "# recalculate minimum box area\n",
        "min_box_area = int(round(min_box_area / float(rescale_factor**2)))\n",
        "\n",
        "# TBD add normalization of images into transforms\n",
        "# define transformation \n",
        "transform = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
        "\n",
        "# create datasets\n",
        "dataset_train = PneumoniaDataset(root=datapath_orig, subset='train', pIds=pIds_train, predict=False, \n",
        "                                 boxes=pId_boxes_dict, rescale_factor=rescale_factor, transform=transform,\n",
        "                                 rotation_angle=3, warping=True)\n",
        "\n",
        "dataset_valid = PneumoniaDataset(root=datapath_orig, subset='train', pIds=pIds_valid, predict=False, \n",
        "                                 boxes=pId_boxes_dict, rescale_factor=rescale_factor, transform=transform,\n",
        "                                 rotation_angle=0, warping=False)\n",
        "\n",
        "dataset_test = PneumoniaDataset(root=datapath_orig, subset='test', pIds=pIds_test, predict=True, \n",
        "                                boxes=None, rescale_factor=rescale_factor, transform=transform,\n",
        "                                rotation_angle=0, warping=False)\n",
        "\n",
        "# define the dataloaders with the previous dataset\n",
        "loader_train = DataLoader(dataset=dataset_train,\n",
        "                           batch_size=batch_size,\n",
        "                           shuffle=True) \n",
        "\n",
        "loader_valid = DataLoader(dataset=dataset_valid,\n",
        "                           batch_size=batch_size,\n",
        "                           shuffle=True) \n",
        "\n",
        "loader_test = DataLoader(dataset=dataset_test,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFjzbqAGRA_v"
      },
      "source": [
        "# pIds_train[10006]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5-7mBlV_fBM"
      },
      "source": [
        "# ds = pydicom.read_file(os.path.join(datapath_orig+'stage_2_train_images/', pIds_train[10006]+'.dcm'))\n",
        "\n",
        "# print(ds)\n",
        "# # dcmimg = img.pixel_array\n",
        "# # print(type(dcmimg))\n",
        "# # print(dcmimg.dtype)\n",
        "# # print(dcmimg.shape)\n",
        "# # plt.figure(figsize=(20,10))\n",
        "# # plt.imshow(dcmimg)\n",
        "# # plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MP7t5TMl6Y4"
      },
      "source": [
        "# for i in range(21656,len(dataset_train)):\n",
        "\n",
        "#   dataset_train[i]\n",
        "#   print(i)\n",
        "# # pIds[10006]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "18677b6eff4638b4a0186ec9e65237d48f572ddb",
        "trusted": true,
        "id": "CYOGdZRi21zF"
      },
      "source": [
        "\n",
        "class conv_block(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True,\n",
        "                 bn_momentum=0.9, alpha_leaky=0.03):\n",
        "        super(conv_block, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
        "                              stride=stride, padding=padding, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=bn_momentum)\n",
        "        self.activ = nn.LeakyReLU(negative_slope=alpha_leaky)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activ(self.bn(self.conv(x)))\n",
        "\n",
        "class conv_t_block(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, output_size=None, kernel_size=3, bias=True,\n",
        "                 bn_momentum=0.9, alpha_leaky=0.03):\n",
        "        super(conv_t_block, self).__init__()\n",
        "        self.conv_t = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=2, padding=1, \n",
        "                                         bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=bn_momentum)\n",
        "        self.activ = nn.LeakyReLU(negative_slope=alpha_leaky)\n",
        "\n",
        "    def forward(self, x, output_size):\n",
        "        return self.activ(self.bn(self.conv_t(x, output_size=output_size)))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "403551d53e6988d9f94534ce0461dec82ed86d1e",
        "trusted": true,
        "id": "pRU-EW2921zG"
      },
      "source": [
        "class UNET(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(UNET, self).__init__()\n",
        "        \n",
        "        self.down_1 = nn.Sequential(conv_block(in_channels=1, out_channels=64), conv_block(in_channels=64, out_channels=64))\n",
        "        self.down_2 = nn.Sequential(conv_block(in_channels=64, out_channels=128), conv_block(in_channels=128, out_channels=128))\n",
        "        self.down_3 = nn.Sequential(conv_block(in_channels=128, out_channels=256), conv_block(in_channels=256, out_channels=256))\n",
        "        self.down_4 = nn.Sequential(conv_block(in_channels=256, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "        self.down_5 = nn.Sequential(conv_block(in_channels=512, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "\n",
        "        self.middle = nn.Sequential(conv_block(in_channels=512, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "        self.middle_t = conv_t_block(in_channels=512, out_channels=256)\n",
        "\n",
        "        self.up_5 = nn.Sequential(conv_block(in_channels=768, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "        self.up_5_t = conv_t_block(in_channels=512, out_channels=256)\n",
        "        self.up_4 = nn.Sequential(conv_block(in_channels=768, out_channels=512), conv_block(in_channels=512, out_channels=512))\n",
        "        self.up_4_t = conv_t_block(in_channels=512, out_channels=128)\n",
        "        self.up_3 = nn.Sequential(conv_block(in_channels=384, out_channels=256), conv_block(in_channels=256, out_channels=256))\n",
        "        self.up_3_t = conv_t_block(in_channels=256, out_channels=64)\n",
        "        self.up_2 = nn.Sequential(conv_block(in_channels=192, out_channels=128), conv_block(in_channels=128, out_channels=128))\n",
        "        self.up_2_t = conv_t_block(in_channels=128, out_channels=32)\n",
        "        self.up_1 = nn.Sequential(conv_block(in_channels=96, out_channels=64), conv_block(in_channels=64, out_channels=1))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        down1 = self.down_1(x) # (1x256x256 -> 64x256x256)\n",
        "        out = F.max_pool2d(down1, kernel_size=2, stride=2) # (64x256x256 -> 64x128x128)\n",
        "\n",
        "        down2 = self.down_2(out) # (64x128x128 -> 128x128x128)\n",
        "        out = F.max_pool2d(down2, kernel_size=2, stride=2) # (128x128x128 -> 128x64x64)\n",
        "\n",
        "        down3 = self.down_3(out) # (128x64x64 -> 256x64x64)\n",
        "        out = F.max_pool2d(down3, kernel_size=2, stride=2) # (256x64x64 -> 256x32x32)\n",
        "\n",
        "        down4 = self.down_4(out) # (256x32x32 -> 512x32x32)\n",
        "        out = F.max_pool2d(down4, kernel_size=2, stride=2) # (512x32x32 -> 512x16x16)\n",
        "\n",
        "        down5 = self.down_5(out) # (512x16x16 -> 512x16x16)\n",
        "        out = F.max_pool2d(down5, kernel_size=2, stride=2) # (512x16x16 -> 512x8x8)\n",
        "\n",
        "        out = self.middle(out) # (512x8x8 -> 512x8x8)\n",
        "        out = self.middle_t(out, output_size=down5.size()) # (512x8x8 -> 256x16x16)\n",
        "\n",
        "        out = torch.cat([down5, out], 1) # (512x16x16-concat-256x16x16 -> 768x16x16)\n",
        "        out = self.up_5(out) # (768x16x16 -> 512x16x16)\n",
        "        out = self.up_5_t(out, output_size=down4.size()) # (512x16x16 -> 256x32x32)\n",
        "\n",
        "        out = torch.cat([down4, out], 1) # (512x32x32-concat-256x32x32 -> 768x32x32)\n",
        "        out = self.up_4(out) # (768x32x32 -> 512x32x32)\n",
        "        out = self.up_4_t(out, output_size=down3.size()) # (512x32x32 -> 128x64x64)\n",
        "        \n",
        "        out = torch.cat([down3, out], 1) # (256x64x64-concat-128x64x64 -> 384x64x64)\n",
        "        out = self.up_3(out) # (384x64x64 -> 256x64x64)\n",
        "        out = self.up_3_t(out, output_size=down2.size()) # (256x64x64 -> 64x128x128)\n",
        "        \n",
        "        out = torch.cat([down2, out], 1) # (128x128x128-concat-64x128x128 -> 192x128x128)\n",
        "        out = self.up_2(out) # (192x128x128 -> 128x128x128)\n",
        "        out = self.up_2_t(out, output_size=down1.size()) # (128x128x128 -> 32x256x256)\n",
        "        \n",
        "        out = torch.cat([down1, out], 1) # (64x256x256-concat-32x256x256 -> 96x256x256)\n",
        "        out = self.up_1(out) # (96x256x256 -> 1x256x256)\n",
        "        \n",
        "        return out    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2I9UaeENBh6"
      },
      "source": [
        "from efficientnet_pytorch import EfficientNet\n",
        "# model_name = 'efficientnet-b1'\n",
        "# image_size = EfficientNet.get_image_size(model_name)\n",
        "# print('Image size: ', image_size)\n",
        "\n",
        "# # Load model\n",
        "# model = EfficientNet.from_pretrained(model_name,in_channels=1)\n",
        "# model.eval()\n",
        "# print('Model image size: ', model._global_params.image_size)\n",
        "\n",
        "\n",
        "class model1(nn.Module):\n",
        "    '''\n",
        "    Custom ResNet module\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(model1, self).__init__()\n",
        "        self.base = EfficientNet.from_name('efficientnet-b0',in_channels=1)#8x8\n",
        "        # self.layer1 = nn.Sequential(nn.BatchNorm2d(1280, eps=1e-05, momentum=0.9),nn.Conv2d(in_channels=1280, out_channels=1, kernel_size=1),nn.LeakyReLU(negative_slope=0.03))\n",
        "        # self.layer2 = nn.UpsamplingNearest2d(scale_factor=32)\n",
        "        self.up_layer1 = nn.Sequential(nn.ConvTranspose2d(in_channels=1280, out_channels=320, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(320, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.up_layer2 = nn.Sequential(nn.ConvTranspose2d(in_channels=320, out_channels=112, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(112, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.up_layer3 = nn.Sequential(nn.ConvTranspose2d(in_channels=112, out_channels=80, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(80, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.up_layer4 = nn.Sequential(nn.ConvTranspose2d(in_channels=80, out_channels=40, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(40, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.up_layer5 = nn.Sequential(nn.ConvTranspose2d(in_channels=40, out_channels=16, kernel_size=3, stride=2, padding=1, output_padding=1), \n",
        "                               nn.BatchNorm2d(16, eps=1e-05, momentum=0.9), nn.LeakyReLU(negative_slope=0.03))\n",
        "        self.out_layer = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        out = self.base.extract_features(x)\n",
        "        # out = self.layer1(out)\n",
        "        # out = self.layer2(out)\n",
        "        out = self.up_layer1(out)\n",
        "        out = self.up_layer2(out)\n",
        "        out = self.up_layer3(out)\n",
        "        out = self.up_layer4(out)\n",
        "        out = self.up_layer5(out)\n",
        "        out = self.out_layer(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oo76oMCEui54"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model1().cuda(),input_size=(1,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MlJBTdRsQGd"
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(UNET().cuda(),input_size=(1,256,256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC9JBgA5k_hO"
      },
      "source": [
        "# class model2(nn.Module):\n",
        "#     '''\n",
        "#     Custom ResNet module\n",
        "#     '''\n",
        "\n",
        "#     def __init__(self):\n",
        "#         super(model1, self).__init__()\n",
        "#         self.base = EfficientNet.from_name('efficientnet-b0',in_channels=1)\n",
        "#         self.layer1 = nn.Sequential(nn.BatchNorm2d(1280, eps=1e-05, momentum=0.9),nn.Conv2d(in_channels=1280, out_channels=1, kernel_size=1),nn.LeakyReLU(negative_slope=0.03))\n",
        "#         self.layer2 = nn.UpsamplingNearest2d(scale_factor=32)\n",
        "#     def forward(self, x):\n",
        "#         out = self.base.extract_features(x)\n",
        "#         out = self.layer1(out)\n",
        "#         out = self.layer2(out)\n",
        "        \n",
        "#         return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c58d080e0979c01dd6edb4b7387e59dc04e759c4",
        "trusted": true,
        "id": "P6QjdPdK21zH"
      },
      "source": [
        "\n",
        "class BCEWithLogitsLoss2d(nn.Module):\n",
        "\n",
        "    def __init__(self, weight=None, size_average=True):\n",
        "        super(BCEWithLogitsLoss2d, self).__init__()\n",
        "        self.loss = nn.BCEWithLogitsLoss(weight, size_average)\n",
        "\n",
        "    def forward(self, scores, targets):\n",
        "        scores_flat = scores.view(-1)\n",
        "        targets_flat = targets.view(-1)\n",
        "        return self.loss(scores_flat, targets_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "73632678b3be947135bf03fa118643ebf8f7bf65",
        "trusted": true,
        "id": "87UCAQlV21zH"
      },
      "source": [
        "\n",
        "def box_mask(box, shape=1024):\n",
        "\n",
        "    x, y, w, h = box\n",
        "    mask = np.zeros((shape, shape), dtype=bool)\n",
        "    mask[y:y+h, x:x+w] = True \n",
        "    return mask\n",
        "\n",
        "def mask_boxes(msk, threshold=0.20, connectivity=None):\n",
        "\n",
        "    msk = msk[0]\n",
        "    pos = np.zeros(msk.shape)\n",
        "    pos[msk>threshold] = 1.\n",
        "    lbl = skimage.measure.label(pos, connectivity=connectivity)\n",
        "    predicted_boxes = []\n",
        "    confidences = []\n",
        "    for region in skimage.measure.regionprops(lbl):\n",
        "        y1, x1, y2, x2 = region.bbox\n",
        "        h = y2 - y1\n",
        "        w = x2 - x1\n",
        "        c = np.nanmean(msk[y1:y2, x1:x2])\n",
        "        if w*h > min_box_area: \n",
        "            predicted_boxes.append([x1, y1, w, h])\n",
        "            confidences.append(c)\n",
        "    \n",
        "    return predicted_boxes, confidences\n",
        "\n",
        "def prediction_string(predicted_boxes, confidences):\n",
        "\n",
        "    prediction_string = ''\n",
        "    for c, box in zip(confidences, predicted_boxes):\n",
        "        prediction_string += ' ' + str(c) + ' ' + ' '.join([str(b) for b in box])\n",
        "    return prediction_string[1:]   \n",
        "\n",
        "def IoU(pr, gt):\n",
        "    IoU = (pr & gt).sum() / ((pr | gt).sum() + 1.e-9)\n",
        "    return IoU\n",
        "\n",
        "def precision(tp, fp, fn):\n",
        "    return float(tp) / (tp + fp + fn + 1.e-9)\n",
        "def average_precision_image(predicted_boxes, confidences, target_boxes, shape=1024):\n",
        "\n",
        "    if predicted_boxes == [] and target_boxes == []:\n",
        "        return np.nan\n",
        "    else:\n",
        "        if len(predicted_boxes)>0 and target_boxes == []:\n",
        "            return 0.0\n",
        "        elif len(target_boxes)>0 and predicted_boxes == []:\n",
        "            return 0.0\n",
        "        else:\n",
        "            # define list of thresholds for IoU [0.4 , 0.45, 0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75]\n",
        "            thresholds = np.arange(0.4, 0.8, 0.05) \n",
        "            # sort boxes according to their confidence (from largest to smallest)\n",
        "            predicted_boxes_sorted = list(reversed([b for _, b in sorted(zip(confidences, predicted_boxes),key=lambda pair: pair[0])]))            \n",
        "            average_precision = 0.0\n",
        "            for t in thresholds: \n",
        "                tp = 0 \n",
        "                fp = len(predicted_boxes)  \n",
        "                for box_p in predicted_boxes_sorted: \n",
        "                    box_p_msk = box_mask(box_p, shape) \n",
        "                    for box_t in target_boxes: \n",
        "                        box_t_msk = box_mask(box_t, shape) \n",
        "                        iou = IoU(box_p_msk, box_t_msk) \n",
        "                        if iou>t:\n",
        "                            tp += 1 \n",
        "                            fp -= 1 \n",
        "                            break \n",
        "                fn = len(target_boxes) \n",
        "                for box_t in target_boxes: \n",
        "                    box_t_msk = box_mask(box_t, shape) \n",
        "                    for box_p in predicted_boxes_sorted: \n",
        "                        box_p_msk = box_mask(box_p, shape) \n",
        "                        iou = IoU(box_p_msk, box_t_msk) \n",
        "                        if iou>t:\n",
        "                            fn -= 1\n",
        "                            break \n",
        "               \n",
        "                average_precision += precision(tp, fp, fn) / float(len(thresholds))\n",
        "            return average_precision\n",
        "\n",
        "def average_precision_batch(output_batch, pIds, pId_boxes_dict, rescale_factor, shape=1024, return_array=False):\n",
        "    \n",
        "    batch_precisions = []\n",
        "    for msk, pId in zip(output_batch, pIds): \n",
        "        target_boxes = pId_boxes_dict[pId] if pId in pId_boxes_dict else []\n",
        "\n",
        "        if len(target_boxes)>0:\n",
        "            target_boxes = [[int(round(c/float(rescale_factor))) for c in box_t] for box_t in target_boxes]\n",
        "\n",
        "        predicted_boxes, confidences = mask_boxes(msk) \n",
        "        batch_precisions.append(average_precision_image(predicted_boxes, confidences, target_boxes, shape=shape))\n",
        "    if return_array:\n",
        "        return np.asarray(batch_precisions)\n",
        "    else:\n",
        "        return np.nanmean(np.asarray(batch_precisions)) \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "207499baf520b5952e347ff28351d66b89f1d7ca",
        "trusted": true,
        "id": "1ZtgC1w621zL"
      },
      "source": [
        "class RunningAverage():\n",
        "\n",
        "    def __init__(self):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "    \n",
        "    def update(self, val):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "    \n",
        "    def __call__(self):\n",
        "        return self.total/float(self.steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "4c14e5666c2ddbb03edcc139bce0a2c445ee8e21",
        "trusted": true,
        "id": "5vc7UnTO21zM"
      },
      "source": [
        "def save_checkpoint(state, is_best, metric):\n",
        "    filename = 'last.pth.tar'\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, metric+'.best.pth.tar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWQsQI-blxNh"
      },
      "source": [
        "def mean_iou(y_true, y_pred, device):\n",
        "    y_pred = torch.round(y_pred)\n",
        "    intersect = (y_true * y_pred).sum(axis=[1, 2, 3])\n",
        "    union = y_true.sum(axis=[1, 2, 3]) + y_pred.sum(axis=[1, 2, 3])\n",
        "    smooth = torch.ones(intersect.shape).cuda()\n",
        "    return ((intersect + smooth) / (union - intersect + smooth)).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKzuU3LDVv3O"
      },
      "source": [
        "# for i, (input_batch, labels_batch, pIds_batch) in enumerate(loader_train):\n",
        "#   print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d9346c5e323d2ce3281044b8049abfb0d8234c49",
        "trusted": true,
        "id": "JrMIaQ4S21zM"
      },
      "source": [
        "\n",
        "def train(model, dataloader, optimizer, loss_fn, num_steps, pId_boxes_dict, rescale_factor, shape, save_summary_steps=10):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    summary = []\n",
        "    loss_avg = RunningAverage()\n",
        "\n",
        "    loss_avg_t_hist_ep, loss_t_hist_ep, prec_t_hist_ep = [], [], []\n",
        "\n",
        "\n",
        "    start = time.time()        \n",
        "    \n",
        "    for i, (input_batch, labels_batch, pIds_batch) in enumerate(dataloader):\n",
        "        # if i > num_steps:\n",
        "        #     break\n",
        "\n",
        "        input_batch = Variable(input_batch).cuda() if gpu_available else Variable(input_batch).float()\n",
        "        labels_batch = Variable(labels_batch).cuda() if gpu_available else Variable(labels_batch).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output_batch = model(input_batch)\n",
        "\n",
        "        loss = loss_fn(output_batch, labels_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_avg.update(loss.item())\n",
        "        loss_t_hist_ep.append(loss.item())\n",
        "        loss_avg_t_hist_ep.append(loss_avg())\n",
        "\n",
        "        if i % save_summary_steps == 0:\n",
        "\n",
        "            output_batch = output_batch.data.cpu().numpy()\n",
        "            prec_batch = average_precision_batch(output_batch, pIds_batch, pId_boxes_dict, rescale_factor, shape)\n",
        "            prec_t_hist_ep.append(prec_batch)\n",
        "            summary_batch_string = \"batch loss = {:05.7f} ;  \".format(loss.item())\n",
        "            summary_batch_string += \"average loss = {:05.7f} ;  \".format(loss_avg())\n",
        "            summary_batch_string += \"batch precision = {:05.7f} ;  \".format(prec_batch)\n",
        "            \n",
        "            print('--- Train batch {} / {}: '.format(i, num_steps) + summary_batch_string)\n",
        "            delta_time = time.time() - start\n",
        "            print('    {} batches processed in {:.2f} seconds'.format(save_summary_steps, delta_time))\n",
        "            start = time.time()\n",
        "\n",
        "    metrics_string = \"average loss = {:05.7f} ;  \".format(loss_avg())\n",
        "    print(\"- Train epoch metrics summary: \" + metrics_string)\n",
        "    \n",
        "    return loss_avg_t_hist_ep, loss_t_hist_ep, prec_t_hist_ep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "9de116575eb50215b6288249233d4145fa27c388",
        "trusted": true,
        "id": "WlUOlID_21zM"
      },
      "source": [
        "def evaluate(model, dataloader, loss_fn, num_steps, pId_boxes_dict, rescale_factor, shape):\n",
        "\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    precisions = []\n",
        "\n",
        "    start = time.time()\n",
        "    for i, (input_batch, labels_batch, pIds_batch) in enumerate(dataloader):\n",
        "        if i > num_steps:\n",
        "            break\n",
        "        input_batch = Variable(input_batch).cuda() if gpu_available else Variable(input_batch).float()\n",
        "        labels_batch = Variable(labels_batch).cuda() if gpu_available else Variable(labels_batch).float()\n",
        "\n",
        "        output_batch = model(input_batch)\n",
        "        \n",
        "        loss = loss_fn(output_batch, labels_batch)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        output_batch = output_batch.data.cpu()\n",
        "        prec_batch = average_precision_batch(output_batch, pIds_batch, pId_boxes_dict, rescale_factor, shape, return_array=True)\n",
        "        # iou = mean_iou(out, y, device)\n",
        "        for p in prec_batch:\n",
        "            precisions.append(p)\n",
        "        print('--- Validation batch {} / {}: '.format(i, num_steps))\n",
        "\n",
        "    metrics_mean = {'loss' : np.nanmean(losses),\n",
        "                    'precision' : np.nanmean(np.asarray(precisions)),\n",
        "                    }\n",
        "    metrics_string = \"average loss = {:05.7f} ;  \".format(metrics_mean['loss'])\n",
        "    # metrics_string += \"average precision = {:05.7f} ;  \".format(metrics_mean['precision'])\n",
        "    print(\"- Eval metrics : \" + metrics_string)\n",
        "    delta_time = time.time() - start\n",
        "    print('  Evaluation run in {:.2f} seconds.'.format(delta_time))\n",
        "    \n",
        "    return metrics_mean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2e5cff316bb314cbb22b977864dedb664f99d829",
        "trusted": true,
        "id": "BM6dKLS121zN"
      },
      "source": [
        "def train_and_evaluate(model, train_dataloader, val_dataloader, lr_init, loss_fn, num_epochs, \n",
        "                       num_steps_train, num_steps_eval, pId_boxes_dict, rescale_factor, shape, restore_file=None):\n",
        "\n",
        "    if restore_file is not None:\n",
        "        checkpoint = torch.load(restore_file)\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "            \n",
        "    best_val_loss = 1e+15\n",
        "    best_val_prec = 0.0\n",
        "    best_loss_model = None\n",
        "    best_prec_model = None\n",
        "\n",
        "    loss_t_history = []\n",
        "    loss_v_history = []\n",
        "    loss_avg_t_history = []\n",
        "    prec_t_history = []\n",
        "    prec_v_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "        lr = lr_init * 0.5**float(epoch) # reduce the learning rate at each epoch\n",
        "        # lr = 0.001\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "        print(\"Epoch {}/{}. Learning rate = {:05.3f}.\".format(epoch + 1, num_epochs, lr))\n",
        "        loss_avg_t_hist_ep, loss_t_hist_ep, prec_t_hist_ep = train(model, train_dataloader, optimizer, loss_fn, \n",
        "                                                                   num_steps_train, pId_boxes_dict, rescale_factor, shape)\n",
        "        loss_avg_t_history += loss_avg_t_hist_ep\n",
        "        loss_t_history += loss_t_hist_ep\n",
        "        prec_t_history += prec_t_hist_ep\n",
        "        \n",
        "        val_metrics = evaluate(model, val_dataloader, loss_fn, num_steps_eval, pId_boxes_dict, rescale_factor, shape)\n",
        "\n",
        "        val_loss = val_metrics['loss']\n",
        "        val_prec = val_metrics['precision']\n",
        "        \n",
        "        loss_v_history += len(loss_t_hist_ep) * [val_loss]\n",
        "        prec_v_history += len(prec_t_hist_ep) * [val_prec]\n",
        "\n",
        "        is_best_loss = val_loss<=best_val_loss\n",
        "        is_best_prec = val_prec>=best_val_prec\n",
        "        \n",
        "        if is_best_loss:\n",
        "            print(\"- Found new best loss: {:.4f}\".format(val_loss))\n",
        "            best_val_loss = val_loss\n",
        "            best_loss_model = model\n",
        "        if is_best_prec:\n",
        "            print(\"- Found new best precision: {:.4f}\".format(val_prec))\n",
        "            best_val_prec = val_prec\n",
        "            best_prec_model = model\n",
        "            \n",
        "        save_checkpoint({'epoch': epoch + 1,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'optim_dict' : optimizer.state_dict()},\n",
        "                         is_best=is_best_loss,\n",
        "                         metric='loss')\n",
        "        save_checkpoint({'epoch': epoch + 1,\n",
        "                         'state_dict': model.state_dict(),\n",
        "                         'optim_dict' : optimizer.state_dict()},\n",
        "                         is_best=is_best_prec,\n",
        "                         metric='prec')\n",
        "        \n",
        "        delta_time = time.time() - start\n",
        "        print('Epoch run in {:.2f} minutes'.format(delta_time/60.))\n",
        "\n",
        "    histories = {'loss avg train' : loss_avg_t_history,\n",
        "                 'loss train' : loss_t_history,\n",
        "                 'precision train' : prec_t_history,\n",
        "                 'loss validation' : loss_v_history, \n",
        "                 'precision validation' : prec_v_history}\n",
        "    best_models = {'best loss model' : best_loss_model,\n",
        "                   'best precision model' : best_prec_model}\n",
        "    \n",
        "    return histories, best_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "5c1e0f2d944a30cfa72d39b118caa72eba6286a5",
        "trusted": true,
        "id": "6Vw_PbUg21zN"
      },
      "source": [
        "def predict(model, dataloader): \n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    predictions = {}\n",
        "\n",
        "    for i, (test_batch, pIds) in enumerate(dataloader):\n",
        "        print('Predicting batch {} / {}.'.format(i+1, len(dataloader)))\n",
        "        test_batch = Variable(test_batch).cuda() if gpu_available else Variable(test_batch).float()\n",
        "            \n",
        "        output_batch = model(test_batch)\n",
        "        sig = nn.Sigmoid().cuda()\n",
        "        output_batch = sig(output_batch)\n",
        "        output_batch = output_batch.data.cpu().numpy()\n",
        "        for pId, output in zip(pIds, output_batch):\n",
        "            predictions[pId] = output\n",
        "        \n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1frcGB2MU-W"
      },
      "source": [
        "\n",
        "debug = False\n",
        "\n",
        "model = model1().cuda() if gpu_available else model1()\n",
        "loss_fn = BCEWithLogitsLoss2d().cuda() if gpu_available else BCEWithLogitsLoss2d()\n",
        "lr_init = 0.001\n",
        "\n",
        "num_epochs = 2\n",
        "num_steps_train = len(loader_train)\n",
        "num_steps_eval = len(loader_valid)\n",
        "\n",
        "shape = int(round(original_image_shape / rescale_factor))\n",
        "\n",
        "print(\"Starting training for {} epochs\".format(num_epochs))\n",
        "histories, best_models = train_and_evaluate(model, loader_train, loader_valid, lr_init, loss_fn, \n",
        "                                            num_epochs, num_steps_train, num_steps_eval, pId_boxes_dict, rescale_factor, shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e0a7bcf8f26561d276caa4ee7687e04c4ec0461a",
        "trusted": true,
        "id": "EtJWDgig21zN"
      },
      "source": [
        "debug = False\n",
        "\n",
        "model = UNET().cuda() if gpu_available else model1()\n",
        "loss_fn = BCEWithLogitsLoss2d().cuda() if gpu_available else BCEWithLogitsLoss2d()\n",
        "lr_init = 0.5\n",
        "num_epochs = 2\n",
        "num_steps_train = len(loader_train)\n",
        "num_steps_eval = len(loader_valid)\n",
        "shape = int(round(original_image_shape / rescale_factor))\n",
        "\n",
        "print(\"Starting training for {} epochs\".format(num_epochs))\n",
        "histories, best_models = train_and_evaluate(model, loader_train, loader_valid, lr_init, loss_fn, \n",
        "                                            num_epochs, num_steps_train, num_steps_eval, pId_boxes_dict, rescale_factor, shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "27b584a443adff0f3c8a47792bba5c5756cd3b4b",
        "trusted": true,
        "id": "OXSY7Wpy21zO"
      },
      "source": [
        "\n",
        "plt.plot(range(len(histories['loss train'])), histories['loss train'], color='k', label='loss train')\n",
        "plt.plot(range(len(histories['loss avg train'])), histories['loss avg train'], color='g', ls='dashed', label='loss avg train')\n",
        "plt.plot(range(len(histories['loss validation'])), histories['loss validation'], color='r', label='loss validation')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "be45f54706df3e60ee3e652f73758c214bf48e4a",
        "trusted": true,
        "id": "XK-uxTwh21zO"
      },
      "source": [
        "# plt.plot(range(len(histories['precision train'])), histories['precision train'], color='k', label='precision train')\n",
        "# plt.plot(range(len(histories['precision validation'])), histories['precision validation'], color='r', label='precision validation')\n",
        "# plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7800bc360cd836bd95464d45214ddfc245c37f38",
        "trusted": true,
        "id": "e-OM0LB721zO"
      },
      "source": [
        "best_model = best_models['best precision model']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTbb-eK-MMOT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42802c39-1caa-4a39-feee-5238f3f0bd2d"
      },
      "source": [
        "checkpoint = torch.load('/content/drive/MyDrive/Colab_Notebooks/project/data/save/loss.best.pth.tar')\n",
        "best_model = model1().cuda() if gpu_available else model1()\n",
        "loss_fn = BCEWithLogitsLoss2d().cuda() if gpu_available else BCEWithLogitsLoss2d()\n",
        "lr_init = 0.002\n",
        "best_model.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "e43d8eaf21f7ff546e2102e111f9d4586a1464a2",
        "trusted": true,
        "id": "Y4RI2p4W21zO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2796240-741f-420b-bd33-70c2466ffed5"
      },
      "source": [
        "\n",
        "dataset_valid = PneumoniaDataset(root=datapath_orig, subset='train', pIds=pIds_valid, predict=True, \n",
        "                                 boxes=None, rescale_factor=rescale_factor, transform=transform)\n",
        "loader_valid = DataLoader(dataset=dataset_valid,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=False) \n",
        "\n",
        "predictions_valid = predict(best_model, loader_valid)\n",
        "print('Predicted {} validation images.'.format(len(predictions_valid)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting batch 1 / 167.\n",
            "Predicting batch 2 / 167.\n",
            "Predicting batch 3 / 167.\n",
            "Predicting batch 4 / 167.\n",
            "Predicting batch 5 / 167.\n",
            "Predicting batch 6 / 167.\n",
            "Predicting batch 7 / 167.\n",
            "Predicting batch 8 / 167.\n",
            "Predicting batch 9 / 167.\n",
            "Predicting batch 10 / 167.\n",
            "Predicting batch 11 / 167.\n",
            "Predicting batch 12 / 167.\n",
            "Predicting batch 13 / 167.\n",
            "Predicting batch 14 / 167.\n",
            "Predicting batch 15 / 167.\n",
            "Predicting batch 16 / 167.\n",
            "Predicting batch 17 / 167.\n",
            "Predicting batch 18 / 167.\n",
            "Predicting batch 19 / 167.\n",
            "Predicting batch 20 / 167.\n",
            "Predicting batch 21 / 167.\n",
            "Predicting batch 22 / 167.\n",
            "Predicting batch 23 / 167.\n",
            "Predicting batch 24 / 167.\n",
            "Predicting batch 25 / 167.\n",
            "Predicting batch 26 / 167.\n",
            "Predicting batch 27 / 167.\n",
            "Predicting batch 28 / 167.\n",
            "Predicting batch 29 / 167.\n",
            "Predicting batch 30 / 167.\n",
            "Predicting batch 31 / 167.\n",
            "Predicting batch 32 / 167.\n",
            "Predicting batch 33 / 167.\n",
            "Predicting batch 34 / 167.\n",
            "Predicting batch 35 / 167.\n",
            "Predicting batch 36 / 167.\n",
            "Predicting batch 37 / 167.\n",
            "Predicting batch 38 / 167.\n",
            "Predicting batch 39 / 167.\n",
            "Predicting batch 40 / 167.\n",
            "Predicting batch 41 / 167.\n",
            "Predicting batch 42 / 167.\n",
            "Predicting batch 43 / 167.\n",
            "Predicting batch 44 / 167.\n",
            "Predicting batch 45 / 167.\n",
            "Predicting batch 46 / 167.\n",
            "Predicting batch 47 / 167.\n",
            "Predicting batch 48 / 167.\n",
            "Predicting batch 49 / 167.\n",
            "Predicting batch 50 / 167.\n",
            "Predicting batch 51 / 167.\n",
            "Predicting batch 52 / 167.\n",
            "Predicting batch 53 / 167.\n",
            "Predicting batch 54 / 167.\n",
            "Predicting batch 55 / 167.\n",
            "Predicting batch 56 / 167.\n",
            "Predicting batch 57 / 167.\n",
            "Predicting batch 58 / 167.\n",
            "Predicting batch 59 / 167.\n",
            "Predicting batch 60 / 167.\n",
            "Predicting batch 61 / 167.\n",
            "Predicting batch 62 / 167.\n",
            "Predicting batch 63 / 167.\n",
            "Predicting batch 64 / 167.\n",
            "Predicting batch 65 / 167.\n",
            "Predicting batch 66 / 167.\n",
            "Predicting batch 67 / 167.\n",
            "Predicting batch 68 / 167.\n",
            "Predicting batch 69 / 167.\n",
            "Predicting batch 70 / 167.\n",
            "Predicting batch 71 / 167.\n",
            "Predicting batch 72 / 167.\n",
            "Predicting batch 73 / 167.\n",
            "Predicting batch 74 / 167.\n",
            "Predicting batch 75 / 167.\n",
            "Predicting batch 76 / 167.\n",
            "Predicting batch 77 / 167.\n",
            "Predicting batch 78 / 167.\n",
            "Predicting batch 79 / 167.\n",
            "Predicting batch 80 / 167.\n",
            "Predicting batch 81 / 167.\n",
            "Predicting batch 82 / 167.\n",
            "Predicting batch 83 / 167.\n",
            "Predicting batch 84 / 167.\n",
            "Predicting batch 85 / 167.\n",
            "Predicting batch 86 / 167.\n",
            "Predicting batch 87 / 167.\n",
            "Predicting batch 88 / 167.\n",
            "Predicting batch 89 / 167.\n",
            "Predicting batch 90 / 167.\n",
            "Predicting batch 91 / 167.\n",
            "Predicting batch 92 / 167.\n",
            "Predicting batch 93 / 167.\n",
            "Predicting batch 94 / 167.\n",
            "Predicting batch 95 / 167.\n",
            "Predicting batch 96 / 167.\n",
            "Predicting batch 97 / 167.\n",
            "Predicting batch 98 / 167.\n",
            "Predicting batch 99 / 167.\n",
            "Predicting batch 100 / 167.\n",
            "Predicting batch 101 / 167.\n",
            "Predicting batch 102 / 167.\n",
            "Predicting batch 103 / 167.\n",
            "Predicting batch 104 / 167.\n",
            "Predicting batch 105 / 167.\n",
            "Predicting batch 106 / 167.\n",
            "Predicting batch 107 / 167.\n",
            "Predicting batch 108 / 167.\n",
            "Predicting batch 109 / 167.\n",
            "Predicting batch 110 / 167.\n",
            "Predicting batch 111 / 167.\n",
            "Predicting batch 112 / 167.\n",
            "Predicting batch 113 / 167.\n",
            "Predicting batch 114 / 167.\n",
            "Predicting batch 115 / 167.\n",
            "Predicting batch 116 / 167.\n",
            "Predicting batch 117 / 167.\n",
            "Predicting batch 118 / 167.\n",
            "Predicting batch 119 / 167.\n",
            "Predicting batch 120 / 167.\n",
            "Predicting batch 121 / 167.\n",
            "Predicting batch 122 / 167.\n",
            "Predicting batch 123 / 167.\n",
            "Predicting batch 124 / 167.\n",
            "Predicting batch 125 / 167.\n",
            "Predicting batch 126 / 167.\n",
            "Predicting batch 127 / 167.\n",
            "Predicting batch 128 / 167.\n",
            "Predicting batch 129 / 167.\n",
            "Predicting batch 130 / 167.\n",
            "Predicting batch 131 / 167.\n",
            "Predicting batch 132 / 167.\n",
            "Predicting batch 133 / 167.\n",
            "Predicting batch 134 / 167.\n",
            "Predicting batch 135 / 167.\n",
            "Predicting batch 136 / 167.\n",
            "Predicting batch 137 / 167.\n",
            "Predicting batch 138 / 167.\n",
            "Predicting batch 139 / 167.\n",
            "Predicting batch 140 / 167.\n",
            "Predicting batch 141 / 167.\n",
            "Predicting batch 142 / 167.\n",
            "Predicting batch 143 / 167.\n",
            "Predicting batch 144 / 167.\n",
            "Predicting batch 145 / 167.\n",
            "Predicting batch 146 / 167.\n",
            "Predicting batch 147 / 167.\n",
            "Predicting batch 148 / 167.\n",
            "Predicting batch 149 / 167.\n",
            "Predicting batch 150 / 167.\n",
            "Predicting batch 151 / 167.\n",
            "Predicting batch 152 / 167.\n",
            "Predicting batch 153 / 167.\n",
            "Predicting batch 154 / 167.\n",
            "Predicting batch 155 / 167.\n",
            "Predicting batch 156 / 167.\n",
            "Predicting batch 157 / 167.\n",
            "Predicting batch 158 / 167.\n",
            "Predicting batch 159 / 167.\n",
            "Predicting batch 160 / 167.\n",
            "Predicting batch 161 / 167.\n",
            "Predicting batch 162 / 167.\n",
            "Predicting batch 163 / 167.\n",
            "Predicting batch 164 / 167.\n",
            "Predicting batch 165 / 167.\n",
            "Predicting batch 166 / 167.\n",
            "Predicting batch 167 / 167.\n",
            "Predicted 2668 validation images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "71801746920ea5effb8d7d94f5921b97569b45f7",
        "trusted": true,
        "id": "wZo3RK6C21zP"
      },
      "source": [
        "def rescale_box_coordinates(box, rescale_factor):\n",
        "    x, y, w, h = box\n",
        "    x = int(round(x/rescale_factor))\n",
        "    y = int(round(y/rescale_factor))\n",
        "    w = int(round(w/rescale_factor))\n",
        "    h = int(round(h/rescale_factor))\n",
        "    return [x, y, w, h]    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "54b741a09c36add0d71e73481fcca7ca0c06b820",
        "trusted": true,
        "id": "KA1-5YY021zP"
      },
      "source": [
        "def draw_boxes(predicted_boxes, confidences, target_boxes, ax, angle=0):\n",
        "    if len(predicted_boxes)>0 and len(target_boxes)>0:\n",
        "      for box_p, box_t, c in zip(predicted_boxes,target_boxes, confidences):\n",
        "            xp, yp, wp, hp = box_p \n",
        "            patch_p = Rectangle((xp,yp), wp, hp, color='red', ls='dashed',\n",
        "                              angle=angle, fill=False, lw=4, joinstyle='round', alpha=0.6)\n",
        "\n",
        "            ax.add_patch(patch_p)\n",
        "            ax.text(xp+wp/2., y-5, 'Confidence：{:.2}'.format(c), color='red', size=20, va='center', ha='center')\n",
        "\n",
        "            xt, yt, wt, ht = box_t\n",
        "            patch_t = Rectangle((xt,yt), wt, ht, color='red',  \n",
        "                              angle=angle, fill=False, lw=4, joinstyle='round', alpha=0.6)\n",
        "            ax.add_patch(patch_t)\n",
        "            box_p_msk = box_mask(box_p, shape=shape)\n",
        "            box_t_msk = box_mask(box_t, shape=shape)\n",
        "            iou = IoU(box_p_msk, box_t_msk)\n",
        "            \n",
        "            ax.text(x+w/2., y-5, 'Confidence：{:.2}'.format(c), color='red', size=20, va='center', ha='center')\n",
        "        return ax\n",
        "    if len(predicted_boxes)>0:\n",
        "        for box, c in zip(predicted_boxes, confidences):\n",
        "\n",
        "            x, y, w, h = box \n",
        "            patch = Rectangle((x,y), w, h, color='red', ls='dashed',\n",
        "                              angle=angle, fill=False, lw=4, joinstyle='round', alpha=0.6)\n",
        "            ax.add_patch(patch)\n",
        "\n",
        "            ax.text(x+w/2., y-5, 'Confidence：{:.2}'.format(c), color='red', size=20, va='center', ha='center')\n",
        "            ax.text(x+w/2., y-5, 'Confidence：{:.2}'.format(c), color='red', size=20, va='center', ha='center')\n",
        "    if len(target_boxes)>0:\n",
        "        for box in target_boxes:\n",
        "            x, y, w, h = box\n",
        "            patch = Rectangle((x,y), w, h, color='red',  \n",
        "                              angle=angle, fill=False, lw=4, joinstyle='round', alpha=0.6)\n",
        "            ax.add_patch(patch)\n",
        "    \n",
        "    return ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "b1df63cc6f3fc4d62db21006d7e50693dfe0d67c",
        "trusted": false,
        "id": "JhcxiFRB21zP"
      },
      "source": [
        "best_threshold = None\n",
        "best_avg_precision_valid = 0.0\n",
        "thresholds = np.arange(0.15, 0.25, 0.01)\n",
        "avg_precision_valids = []\n",
        "for threshold in thresholds:\n",
        "    precision_valid = []\n",
        "    for i in range(len(dataset_valid)):\n",
        "        img, pId = dataset_valid[i]\n",
        "        target_boxes = [rescale_box_coordinates(box, rescale_factor) for box in pId_boxes_dict[pId]] if pId in pId_boxes_dict else []\n",
        "        prediction = predictions_valid[pId]\n",
        "        predicted_boxes, confidences = mask_boxes(prediction, threshold=threshold, connectivity=None)\n",
        "        avg_precision_img = average_precision_image(predicted_boxes, confidences, target_boxes, shape=img[0].shape[0])\n",
        "        precision_valid.append(avg_precision_img)\n",
        "    avg_precision_valid = np.nanmean(precision_valid)\n",
        "    avg_precision_valids.append(avg_precision_valid)\n",
        "    print('Threshold: {}, average precision validation: {:03.5f}'.format(threshold, avg_precision_valid))\n",
        "    if avg_precision_valid>best_avg_precision_valid:\n",
        "        print('Found new best average precision validation!')\n",
        "        best_avg_precision_valid = avg_precision_valid\n",
        "        best_threshold = threshold\n",
        "plt.plot(thresholds, avg_precision_valids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "6e06f1b38cc25b40f3b628fd770e182dd31097e2",
        "id": "IbkQzRPX21zP"
      },
      "source": [
        "for i in range(len(dataset_valid)):\n",
        "    img, pId = dataset_valid[i]\n",
        "    target_boxes = [rescale_box_coordinates(box, rescale_factor) for box in pId_boxes_dict[pId]] if pId in pId_boxes_dict else []\n",
        "    prediction = predictions_valid[pId]\n",
        "    predicted_boxes, confidences = mask_boxes(prediction, threshold=best_threshold, connectivity=None)\n",
        "    avg_precision_img = average_precision_image(predicted_boxes, confidences, target_boxes, shape=img[0].shape[0])\n",
        "    if i%100==0: # print every 100\n",
        "        plt.imshow(img[0], cmap=mpl.cm.gist_gray) # [0] is the channel index (here there's just one channel)\n",
        "        plt.imshow(prediction[0], cmap=mpl.cm.jet, alpha=0.5)\n",
        "        draw_boxes(predicted_boxes, confidences, target_boxes, plt.gca())\n",
        "        print('Prediction mask scale:', prediction[0].min(), '-', prediction[0].max())\n",
        "        print('Prediction string:', prediction_string(predicted_boxes, confidences))\n",
        "        print('Ground truth boxes:', target_boxes)\n",
        "        print('Average precision image: {:05.5f}'.format(avg_precision_img))\n",
        "        plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSuC6dMex54z"
      },
      "source": [
        "# def mean_iou(y_true, y_pred):\n",
        "#     '''\n",
        "#     Mean-Intersection-Over-Union\n",
        "#     '''\n",
        "#     y_pred = torch.round(y_pred)\n",
        "#     intersect = (y_true * y_pred).sum(axis=[1, 2, 3])\n",
        "#     union = y_true.sum(axis=[1, 2, 3]) + y_pred.sum(axis=[1, 2, 3])\n",
        "#     smooth = torch.ones(intersect.shape).cuda()\n",
        "#     return ((intersect + smooth) / (union - intersect + smooth)).mean()\n",
        "\n",
        "\n",
        "# for i,(input_batch, labels_batch, pIds_batch) in enumerate(loader_train):\n",
        "#   input_batch = Variable(input_batch).cuda() if gpu_available else Variable(input_batch).float()\n",
        "#   labels_batch = Variable(labels_batch).cuda() if gpu_available else Variable(labels_batch).float()\n",
        "            \n",
        "#   # compute output\n",
        "\n",
        "#   output_batch = model(input_batch)\n",
        "#   iou = mean_iou(labels_batch,output_batch)\n",
        "#   print(iou)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "116af95be7c76068865aebcb1833fc65682ea583",
        "trusted": true,
        "id": "FNIIZIg-21zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "a6630b06e78a0646ca232c6b096a416f4a8f1e27",
        "trusted": true,
        "id": "Of5R7LkY21zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "7f6e6cffb7aefc82af074d0706d5a4b3b6c92bf8",
        "trusted": true,
        "id": "IC0tL_Ce21zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "c83de59cf3d57ab80fb5a8d23f884bf9706cf961",
        "trusted": true,
        "id": "rQf5mfN121zQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "865f866c851c9a0917d86813eddbe9aafcbd23e5",
        "trusted": false,
        "id": "n_MFv-fq21zR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}